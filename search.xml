<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>FlagEval9月榜总结</title>
      <link href="/post/87cd825.html"/>
      <url>/post/87cd825.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="FlagEval-9月榜-总结"><a href="#FlagEval-9月榜-总结" class="headerlink" title="FlagEval 9月榜 总结"></a>FlagEval 9月榜 总结</h1><blockquote><p><strong>wechat link：</strong><a href="https://mp.weixin.qq.com/s/bPEPjuZVJ9gMwhXBW2kPcA">https://mp.weixin.qq.com/s/bPEPjuZVJ9gMwhXBW2kPcA</a></p><p><strong>official link：</strong><a href="https://flageval.baai.ac.cn/#/home">FlagEval - 首页 (baai.ac.cn)</a></p></blockquote><h2 id="Hightlight"><a href="#Hightlight" class="headerlink" title="Hightlight"></a>Hightlight</h2><ul><li>FlagEval大语言模型评测框架更新，细化40+子能力维度</li><li>基于智源自建CLCC主观评测集，分析7个知名模型的能力分布</li><li><u>FlagEval 9月榜单发布，新增 YuLan、Baichuan2 等最新开源基座模型和SFT模型</u></li></ul><h2 id="FlagEval"><a href="#FlagEval" class="headerlink" title="FlagEval"></a>FlagEval</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzI2MDcxMzQzOA==&mid=2247535981&idx=1&sn=1ff2e2573ce634569f37f7bd491e3445&scene=21#wechat_redirect">FlagEval 大语言模型评测体系</a>创新构建了<strong>“能力-任务-指标”</strong>三维评测框架，细粒度刻画基础模型的认知能力边界。FlagEval希望测出基础模型在<strong>微调后的“潜力”如何</strong>，同时许多传统评测方法在新的LLM上有失效现象（Ground Truth失效等）；此外，传统评测基准有明显的“任务为先”思维，导致模型评测主要从“任务”角度建立框架和基准。</p><p>大模型的训练成本（算力、人力）巨大，因此需要确定出合适的评测体系来降低试错次数，提高试错效率；且一个广泛对比评测的、权威中立榜单，对于大模型在产业落地层面的选型来说，至关重要。</p><p><img src="/post/undefined/image-20230921005501042.png" alt="image-20230921005501042"></p><p>包含 6 大评测任务，近30个评测数据集，超10万道评测题目。新的工作在【安全与价值观】大类和【基础语言能力-&gt;推理能力】小类中增加了评测标准。</p><p>当前，新升级的FlagEval 大语言模型评测的能力框架共计 43 个子能力维度。如下图所示</p><p><img src="/post/undefined/image-20230921005532747.png" alt="image-20230921005532747"></p><p><img src="/post/undefined/image-20230921005545893.png" alt="image-20230921005545893"></p><p>目前FlagEval平台实现了自然语言处理领域（NLP）的评测任务实现，以及部分多模态任务的实现，NLP的评测集成到了自主开发的系统中（已申请，正在审核），多模态的任务评测放到了github上，在github中目前开源的测评工具有基于视觉语言模型评测的mCLIPEval和基于文本到图像（T2I）模型评测的ImageEval-prompt。</p><p><img src="/post/undefined/image-20230921233324417.png" alt="image-20230921233324417"></p><h2 id="FlagEval的框架"><a href="#FlagEval的框架" class="headerlink" title="FlagEval的框架"></a>FlagEval的框架</h2><h3 id="能力框架"><a href="#能力框架" class="headerlink" title="能力框架"></a>能力框架</h3><p><img src="https://flageval.baai.ac.cn/static/media/zh/intro-2.png?v=4" alt="intro-2.png"></p><h3 id="任务框架"><a href="#任务框架" class="headerlink" title="任务框架"></a>任务框架</h3><p><img src="/post/undefined/intro-3-0712.png" alt="intro-3-0712"></p><h3 id="指标框架"><a href="#指标框架" class="headerlink" title="指标框架"></a>指标框架</h3><p>目前只支持准确性指标</p><p>后续将持续更新迭代，增加不确定性（Uncertainty）、鲁棒性（Robustness）、效率（Efficiency）等指标。</p><ul><li><strong>准确性（Accuracy）</strong>：准确性是模型的基础属性，输出的准确性决定了模型是否可用。在 FlagEval 中，准确性是每个评测场景和任务中准确性度量的总称，包括文本分类中的精确匹配（exact-match accuracy），问题回答中基于词重叠的 F1 分数，信息检索的 MRR 和 NDCG 分数，以及摘要的 ROUGE 分数等。</li><li><strong>不确定性（Uncertainty）</strong>：指模型对其预测结果的信心或确定性的度量，这对于在模型可能出错的情况下做出适当的预期和应对措施非常重要。例如，在高风险的环境中，如决策制定，模型的不确定性指标可以让我们对可能的错误结果有所预期，并进行适当调整和干预，避免潜在的风险。</li><li><strong>鲁棒性（Robustness）</strong>：鲁棒性指的是模型在面对输入的扰动时能够保持其性能的能力。例如，一个鲁棒的模型应该能够在问题被稍微改写或包含轻微的打字错误的情况下，仍然能够正确地回答问题。鲁棒性对于实际应用特别重要，因为输入往往是嘈杂的或具有敌意的。在语言模型的背景下，可以通过扰动输入文本并测量模型输出的变化来评估鲁棒性。</li><li><strong>效率（Efficiency）</strong>：效率通常指的是模型的计算效率，包括训练和推理的时间、算力资源。效率会影响模型在实际应用中的可行性。例如，一个非常准确的模型如果需要大量的计算资源或者时间来进行训练或推理，那么它可能就不适合在资源有限或者需要快速响应的环境中使用。</li></ul><h3 id="评测方法"><a href="#评测方法" class="headerlink" title="评测方法"></a>评测方法</h3><p>可以看到目前FlagEval通过【基础语言能力】、【高级语言能力】和【安全与价值观】三个类别涵括了所有测评任务，结合主客观的测评方法。</p><ul><li>基础模型（Basic Model）的评测以“适配评测+提示学习评测”的客观评测为主：<ul><li>适配评测（多选题评测）参考了<a href="https://github.com/EleutherAI/lm-evaluation-harness">EleutherAI&#x2F;lm-evaluation-harness: A framework for few-shot evaluation of autoregressive language models. (github.com)</a>的代码样式（评测指标的集成）并扩展到中文。lm-eval主要实现了基于众多benchmark和model的集成式调用，并针对不同的dataset将多选题的格式完成了适配</li><li>提示学习评测（文本生成）参考了[<a href="https://arxiv.org/abs/2211.09110">2211.09110] Holistic Evaluation of Language Models (arxiv.org)</a>并扩展到中文。helm同样实现了集成式的datasets和model等的调用。</li></ul></li><li>主观评测先复用基础模型的客观评测，考察微调过程是否对基础模型造成了某些能力的提升或下降。然后将人工与自动的主观评测接入：<ul><li>人工主观评测：采用<strong>“多人背靠背标注+第三人仲裁”</strong>（人工测评方式），多人背靠背标注也会采用GPT-4标注的方式增加多样性。</li><li>自动主观评测：在GPT-4根据能力框架创建的主观问题上，采用<strong>GPT-4自动化标注</strong>的方式进行标注。</li></ul></li></ul><h2 id="最新模型测评"><a href="#最新模型测评" class="headerlink" title="最新模型测评"></a>最新模型测评</h2><p>依据最新版的能力框架，FlagEval 团队同步更新了智源自建的 Chinese Linguistics &amp; Cognition Challenge (CLCC) 主观评测数据集题库 v2.0。</p><h2 id="简单总结"><a href="#简单总结" class="headerlink" title="简单总结"></a>简单总结</h2><p>个人理解，FlagEval的主要工作就是：</p><ol><li><p>整合了目前开源的评测datasets到一起，设立了一个任务+指标+能力的框架（针对Generation QA任务，通过Accuracy的指标，评测model的推理能力）</p><p><img src="/post/undefined/image-20230923152145516.png" alt="image-20230923152145516"></p></li><li><p>搞了一个大系统，设定推理参数，可以实现一体化的评测。</p><p><img src="/post/undefined/image-20230923152208409.png" alt="image-20230923152208409"></p></li><li><p>依据blog中的说法，引入了一些自创datasets评测一些特殊能力</p></li></ol><p>目前，FlagEval开源实现了的更多是客观评测的一些指标，主观评测的指标在推文中有给出，比如伦理道德中的个人品德是用哪个数据集、指标是什么，这些都没有给出；因为评测资格还没有通过所以暂时也还不能看到系统里的具体情况。因此这里我暂时还存疑。</p>]]></content>
      
      
      <categories>
          
          <category> 实验室工作 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> llm </tag>
            
            <tag> evaluation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FlagEval 9月榜 总结</title>
      <link href="/post/959a2f9b.html"/>
      <url>/post/959a2f9b.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="FlagEval-9月榜-总结"><a href="#FlagEval-9月榜-总结" class="headerlink" title="FlagEval 9月榜 总结"></a>FlagEval 9月榜 总结</h1><blockquote><p><strong>wechat link：</strong><a href="https://mp.weixin.qq.com/s/bPEPjuZVJ9gMwhXBW2kPcA">https://mp.weixin.qq.com/s/bPEPjuZVJ9gMwhXBW2kPcA</a></p><p><strong>official link：</strong><a href="https://flageval.baai.ac.cn/#/home">FlagEval - 首页 (baai.ac.cn)</a></p></blockquote><h2 id="Hightlight"><a href="#Hightlight" class="headerlink" title="Hightlight"></a>Hightlight</h2><ul><li>FlagEval大语言模型评测框架更新，细化40+子能力维度</li><li>基于智源自建CLCC主观评测集，分析7个知名模型的能力分布</li><li><u>FlagEval 9月榜单发布，新增 YuLan、Baichuan2 等最新开源基座模型和SFT模型</u></li></ul><h2 id="FlagEval"><a href="#FlagEval" class="headerlink" title="FlagEval"></a>FlagEval</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzI2MDcxMzQzOA==&mid=2247535981&idx=1&sn=1ff2e2573ce634569f37f7bd491e3445&scene=21#wechat_redirect">FlagEval 大语言模型评测体系</a>创新构建了<strong>“能力-任务-指标”</strong>三维评测框架，细粒度刻画基础模型的认知能力边界。FlagEval希望测出基础模型在<strong>微调后的“潜力”如何</strong>，同时许多传统评测方法在新的LLM上有失效现象（Ground Truth失效等）；此外，传统评测基准有明显的“任务为先”思维，导致模型评测主要从“任务”角度建立框架和基准。</p><p>大模型的训练成本（算力、人力）巨大，因此需要确定出合适的评测体系来降低试错次数，提高试错效率；且一个广泛对比评测的、权威中立榜单，对于大模型在产业落地层面的选型来说，至关重要。</p><p><img src="/post/959a2f9b/image-20230921005501042.png" alt="image-20230921005501042"></p><p>包含 6 大评测任务，近30个评测数据集，超10万道评测题目。新的工作在【安全与价值观】大类和【基础语言能力-&gt;推理能力】小类中增加了评测标准。</p><p>当前，新升级的FlagEval 大语言模型评测的能力框架共计 43 个子能力维度。如下图所示</p><p><img src="/post/959a2f9b/image-20230921005532747.png" alt="image-20230921005532747"></p><p><img src="/post/959a2f9b/image-20230921233324417.png" alt="image-20230921233324417"></p><p>目前FlagEval平台实现了自然语言处理领域（NLP）的评测任务实现，以及部分多模态任务的实现，NLP的评测集成到了自主开发的系统中（已申请，正在审核），多模态的任务评测放到了github上，在github中目前开源的测评工具有基于视觉语言模型评测的mCLIPEval和基于文本到图像（T2I）模型评测的ImageEval-prompt。</p><h2 id="FlagEval的框架"><a href="#FlagEval的框架" class="headerlink" title="FlagEval的框架"></a>FlagEval的框架</h2><h3 id="能力框架"><a href="#能力框架" class="headerlink" title="能力框架"></a>能力框架</h3><p><img src="https://flageval.baai.ac.cn/static/media/zh/intro-2.png?v=4" alt="intro-2.png"></p><h3 id="任务框架"><a href="#任务框架" class="headerlink" title="任务框架"></a>任务框架</h3><p><img src="/2023-09-20-FlagEval-9%E6%9C%88%E6%A6%9C-%E6%80%BB%E7%BB%93/intro-3-0712.png" alt="intro-3.png"></p><h3 id="指标框架"><a href="#指标框架" class="headerlink" title="指标框架"></a>指标框架</h3><p>目前只支持准确性指标</p><p>后续将持续更新迭代，增加不确定性（Uncertainty）、鲁棒性（Robustness）、效率（Efficiency）等指标。</p><ul><li><strong>准确性（Accuracy）</strong>：准确性是模型的基础属性，输出的准确性决定了模型是否可用。在 FlagEval 中，准确性是每个评测场景和任务中准确性度量的总称，包括文本分类中的精确匹配（exact-match accuracy），问题回答中基于词重叠的 F1 分数，信息检索的 MRR 和 NDCG 分数，以及摘要的 ROUGE 分数等。</li><li><strong>不确定性（Uncertainty）</strong>：指模型对其预测结果的信心或确定性的度量，这对于在模型可能出错的情况下做出适当的预期和应对措施非常重要。例如，在高风险的环境中，如决策制定，模型的不确定性指标可以让我们对可能的错误结果有所预期，并进行适当调整和干预，避免潜在的风险。</li><li><strong>鲁棒性（Robustness）</strong>：鲁棒性指的是模型在面对输入的扰动时能够保持其性能的能力。例如，一个鲁棒的模型应该能够在问题被稍微改写或包含轻微的打字错误的情况下，仍然能够正确地回答问题。鲁棒性对于实际应用特别重要，因为输入往往是嘈杂的或具有敌意的。在语言模型的背景下，可以通过扰动输入文本并测量模型输出的变化来评估鲁棒性。</li><li><strong>效率（Efficiency）</strong>：效率通常指的是模型的计算效率，包括训练和推理的时间、算力资源。效率会影响模型在实际应用中的可行性。例如，一个非常准确的模型如果需要大量的计算资源或者时间来进行训练或推理，那么它可能就不适合在资源有限或者需要快速响应的环境中使用。</li></ul><h3 id="评测方法"><a href="#评测方法" class="headerlink" title="评测方法"></a>评测方法</h3><p>可以看到目前FlagEval通过【基础语言能力】、【高级语言能力】和【安全与价值观】三个类别涵括了所有测评任务，结合主客观的测评方法。</p><ul><li>基础模型（Basic Model）的评测以“适配评测+提示学习评测”的客观评测为主：<ul><li>适配评测（多选题评测）参考了<a href="https://github.com/EleutherAI/lm-evaluation-harness">EleutherAI&#x2F;lm-evaluation-harness: A framework for few-shot evaluation of autoregressive language models. (github.com)</a>的代码样式（评测指标的集成）并扩展到中文。lm-eval主要实现了基于众多benchmark和model的集成式调用，并针对不同的dataset将多选题的格式完成了适配</li><li>提示学习评测（文本生成）参考了[<a href="https://arxiv.org/abs/2211.09110">2211.09110] Holistic Evaluation of Language Models (arxiv.org)</a>并扩展到中文。helm同样实现了集成式的datasets和model等的调用。</li></ul></li><li>主观评测先复用基础模型的客观评测，考察微调过程是否对基础模型造成了某些能力的提升或下降。然后将人工与自动的主观评测接入：<ul><li>人工主观评测：采用<strong>“多人背靠背标注+第三人仲裁”</strong>（人工测评方式），多人背靠背标注也会采用GPT-4标注的方式增加多样性。</li><li>自动主观评测：在GPT-4根据能力框架创建的主观问题上，采用<strong>GPT-4自动化标注</strong>的方式进行标注。</li></ul></li></ul><h2 id="最新模型测评"><a href="#最新模型测评" class="headerlink" title="最新模型测评"></a>最新模型测评</h2><p>依据最新版的能力框架，FlagEval 团队同步更新了智源自建的 Chinese Linguistics &amp; Cognition Challenge (CLCC) 主观评测数据集题库 v2.0。</p><p><img src="/post/959a2f9b/image-20230923151729981.png" alt="image-20230923151729981"></p><h2 id="简单总结"><a href="#简单总结" class="headerlink" title="简单总结"></a>简单总结</h2><p>个人理解，FlagEval的主要工作就是：</p><ol><li><p>整合了目前开源的评测datasets到一起，设立了一个任务+指标+能力的框架（针对Generation QA任务，通过Accuracy的指标，评测model的推理能力）</p><p><img src="/post/959a2f9b/image-20230923152145516.png" alt="image-20230923152145516"></p></li><li><p>搞了一个大系统，设定推理参数，可以实现一体化的评测。</p><p><img src="/post/959a2f9b/image-20230923152210411.png" alt="image-20230923152210411"></p></li><li><p>依据blog中的说法，引入了一些自创datasets评测一些特殊能力</p></li></ol><p>目前，FlagEval开源实现了的更多是客观评测的一些指标，主观评测的指标在推文中有给出，比如伦理道德中的个人品德是用哪个数据集、指标是什么，这些都没有给出；因为评测资格还没有通过所以暂时也还不能看到系统里的具体情况。因此这里我暂时还存疑。</p>]]></content>
      
      
      <categories>
          
          <category> 实验室工作 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> llm </tag>
            
            <tag> evaluation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux服务器后台执行指令——tmux</title>
      <link href="/post/aaeeb3aa.html"/>
      <url>/post/aaeeb3aa.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Linux服务器后台执行指令——tmux"><a href="#Linux服务器后台执行指令——tmux" class="headerlink" title="Linux服务器后台执行指令——tmux"></a>Linux服务器后台执行指令——tmux</h1><p>tmux (terminal multiplexer)是一种易用的终端复用器，简单来说，它允许我们在同一个终端窗口中创建多个不同的会话，支持同时执行不同的程序，通过tmux，我们可以将多个需要训练的任务放到后台完成，从而解放前台，将任务挂到后台后就可以随时离开。</p><h2 id="1-安装"><a href="#1-安装" class="headerlink" title="1 安装"></a>1 安装</h2><p>安装tmux可以将其当作系统库或者python库来实现，系统库需要有sudo或者brew的权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">linux</span><br>sudo apt-get install tmux <br><span class="hljs-meta prompt_"># </span><span class="language-bash">mac</span><br>brew install tmux<br><span class="hljs-meta prompt_"># </span><span class="language-bash">python (anaconda)</span><br>pip install tmux / conda install tmux<br></code></pre></td></tr></table></figure><p>这里我使用的是python库安装，因为我没有sudo权限</p><p><img src="/post/aaeeb3aa/image-20230918131056578.png" alt="image-20230918131056578"></p><p>完成安装之后就可以调用并使用啦，使用方法如下：</p><h2 id="2-会话操作"><a href="#2-会话操作" class="headerlink" title="2 会话操作"></a>2 会话操作</h2><h3 id="2-1-创建tmux会话窗口"><a href="#2-1-创建tmux会话窗口" class="headerlink" title="2.1 创建tmux会话窗口"></a>2.1 创建tmux会话窗口</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tmux new -s &lt;session-name&gt;<br></code></pre></td></tr></table></figure><h3 id="2-2-分离tmux会话窗口"><a href="#2-2-分离tmux会话窗口" class="headerlink" title="2.2 分离tmux会话窗口"></a>2.2 分离tmux会话窗口</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">terminal未执行指令</span><br>tmux detach<br><span class="hljs-meta prompt_"># </span><span class="language-bash">terminal正在执行指令</span><br>ctrl+B D<br></code></pre></td></tr></table></figure><p>执行后，就会退出当前tmux的窗口，但是会话和里面的进程仍然在后台运行。</p><p>下面的操作大部分均会给出命令行指令和键盘指令两种</p><h3 id="2-3-查看tmux会话列表"><a href="#2-3-查看tmux会话列表" class="headerlink" title="2.3 查看tmux会话列表"></a>2.3 查看tmux会话列表</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">tmux ls<br>ctrl+B S<br></code></pre></td></tr></table></figure><p><img src="/post/aaeeb3aa/image-20230918132101472.png" alt="image-20230918132101472"></p><h3 id="2-4-连接到tmux会话"><a href="#2-4-连接到tmux会话" class="headerlink" title="2.4 连接到tmux会话"></a>2.4 连接到tmux会话</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tmux a -t &lt;session-name&gt;<br></code></pre></td></tr></table></figure><h3 id="2-5-杀掉tmux会话"><a href="#2-5-杀掉tmux会话" class="headerlink" title="2.5 杀掉tmux会话"></a>2.5 杀掉tmux会话</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tmux kill-session -t &lt;session-name&gt;<br></code></pre></td></tr></table></figure><h3 id="2-6-切换tmux会话"><a href="#2-6-切换tmux会话" class="headerlink" title="2.6 切换tmux会话"></a>2.6 切换tmux会话</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tmux switch -t &lt;session-name&gt;<br></code></pre></td></tr></table></figure><h3 id="2-7-重命名tmux会话"><a href="#2-7-重命名tmux会话" class="headerlink" title="2.7 重命名tmux会话"></a>2.7 重命名tmux会话</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">tmux rename-session -t &lt;old-name&gt; &lt;new-name&gt;<br>ctrl+B $<br></code></pre></td></tr></table></figure><h2 id="3-窗口操作"><a href="#3-窗口操作" class="headerlink" title="3 窗口操作"></a>3 窗口操作</h2><h3 id="3-1-窗口浏览"><a href="#3-1-窗口浏览" class="headerlink" title="3.1 窗口浏览"></a>3.1 窗口浏览</h3><p>tmux默认不支持翻页查看操作，上下翻页需要将其转换到浏览模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">ctrl+B [<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">停止浏览</span><br>Q<br></code></pre></td></tr></table></figure><h3 id="3-2-拆分窗格（Pane）"><a href="#3-2-拆分窗格（Pane）" class="headerlink" title="3.2 拆分窗格（Pane）"></a>3.2 拆分窗格（Pane）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">划分左右两个窗格</span><br>ctrl+B %<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">划分上下两个窗格</span><br>ctrl+B &quot;<br></code></pre></td></tr></table></figure><h3 id="3-3-窗格切换"><a href="#3-3-窗格切换" class="headerlink" title="3.3 窗格切换"></a>3.3 窗格切换</h3><p>将光标切换到其他窗格中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">光标切换到其他窗格中</span><br>ctrl+B &lt;键盘方向键&gt;<br></code></pre></td></tr></table></figure><h3 id="3-4-关闭窗口"><a href="#3-4-关闭窗口" class="headerlink" title="3.4 关闭窗口"></a>3.4 关闭窗口</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">ctrl+B X<br>y<br></code></pre></td></tr></table></figure><p>这样就可以实现后台执行指令啦！解放前台人人有责！</p>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> tmux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从0复现transformers经典架构</title>
      <link href="/post/92486878.html"/>
      <url>/post/92486878.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="从0复现transformers经典架构"><a href="#从0复现transformers经典架构" class="headerlink" title="从0复现transformers经典架构"></a>从0复现transformers经典架构</h1><p>因为个人觉得自己的<code>pytorch</code>水平实在是菜，故想着自己复现一个经典模型，复现过程中提一提自己的代码水平。</p><p>本文参考自教程<a href="https://www.youtube.com/watch?v=U0s0f995w14&t=2791s">Pytorch Transformers from Scratch (Attention is all you need) - YouTube</a>，这个教程中详尽介绍了Transformer的实现过程，我跟了一遍之后能够自己再次复现，十分有效。在行文过程中，本文也会就比较陌生的torch函数补充讲解一丢丢。</p><p>注意：此教程中未实现论文中所说的一些trick，如sin函数的pos_embedding等，这些会在之后的过程中予以完善。</p><h2 id="自注意力机制"><a href="#自注意力机制" class="headerlink" title="自注意力机制"></a>自注意力机制</h2><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>Transformer中最为核心的部分就是将mask的自注意力机制融入到了框架中，这个部分也是最难实现的部分。点积自注意力机制的框架在论文中如图所示：</p><p><img src="/post/92486878/image-20230825222703098.png" alt="image-20230825222703098"></p><p>其中，注意力部分的公式如下：<br>$$<br>{\rm Attention}(Q,K,V) &#x3D; {\rm softmax}(\frac{QK^\top}{\sqrt{d_k}})V<br>$$<br>图片的左边表示注意力实现时的过程，在两个MatMul之间需要执行一系列操作。</p><p>首先说明：创建一个block（class），最重要的就是<code>__init__</code>和<code>forward</code>两个部分，其中<code>__init__</code>定义基本参数和需要跟着模型训练的参数；<code>forward</code>函数用于定义基本逻辑。以自注意力模块的部分为例，<code>__init__</code>函数定义模型的embed_size，以及逻辑实现过程中需要的层(<code>nn.XXX</code>)。<code>SelfAttention</code>的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SelfAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,embed_size,heads</span>):<br>        <span class="hljs-built_in">super</span>(SelfAttention,self).__init__()<br>        self.embed_size = embed_size<br>        self.heads = heads<br>        self.heads_dim = embed_size // heads<br><span class="hljs-comment"># assert</span><br>        <span class="hljs-keyword">assert</span> (self.heads_dim * heads == embed_size), <span class="hljs-string">&quot;Embed size must be divided by num of heads&quot;</span><br><span class="hljs-comment"># q k v</span><br>        self.v = nn.Linear(self.heads_dim,self.heads_dim)<br>        self.k = nn.Linear(self.heads_dim,self.heads_dim)<br>        self.q = nn.Linear(self.heads_dim,self.heads_dim)<br><br>        self.fnn = nn.Linear(self.embed_size,self.embed_size)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,value,key,query,mask</span>):<br>        N = value.shape[<span class="hljs-number">0</span>]<br>        v_len,k_len,q_len = value.shape[<span class="hljs-number">1</span>],key.shape[<span class="hljs-number">1</span>],query.shape[<span class="hljs-number">1</span>]<br><br>        <span class="hljs-comment"># divide into heads_dim</span><br>        value = value.reshape(N,v_len,self.heads,self.heads_dim)<br>        key = key.reshape(N,k_len,self.heads,self.heads_dim)<br>        query = query.reshape(N,q_len,self.heads,self.heads_dim)<br><br>        <span class="hljs-comment"># linear </span><br>        value = self.v(value)<br>        key = self.k(key)<br>        query = self.q(query)<br><br>        <span class="hljs-comment"># matmul</span><br>        <span class="hljs-comment"># query shape: N, q_len, heads, heads_dim</span><br>        <span class="hljs-comment"># key shape: N, k_len, heads, heads_dim</span><br>        <span class="hljs-comment"># energy shape: N, heads, q_len, k_len</span><br>        energy = torch.einsum(<span class="hljs-string">&quot;nqhd,nkhd-&gt;nhqk&quot;</span>,[query,key])<br><br>        <span class="hljs-comment"># scale</span><br>        energy = energy / self.embed_size ** (<span class="hljs-number">1</span>/<span class="hljs-number">2</span>)<br><br>        <span class="hljs-comment"># mask</span><br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            energy = energy.masked_fill(mask == <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-1e20&#x27;</span>))<br>        <br>        <span class="hljs-comment"># softmax</span><br>        energy = torch.softmax(energy,dim=-<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># matmul &amp; concat </span><br>        <span class="hljs-comment"># energy shape: N, heads, q_len, k_len</span><br>        <span class="hljs-comment"># value shape: N, v_len, heads, heads_dim</span><br>        <span class="hljs-comment"># output shape: N, q_len, heads, heads_dim</span><br>        out = torch.einsum(<span class="hljs-string">&#x27;nhqk,nkhd-&gt;nqhd&#x27;</span>,[energy,value]).reshape(N,q_len,self.heads*self.heads_dim)<br><br>        <span class="hljs-comment"># concat &amp; linear</span><br>        out = self.fnn(out)<br><br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><p>从<code>__init__</code>函数中看到，多头自注意力机制的“多头”是在这里进行划分；且为达到Multi-head Attention中对<code>Q、K、V</code>分开操作，以及最后的线性变换，需要设置对应的<code>Linear</code>层。</p><p>从<code>__forward</code>函数中看到，这段代码实现了基本的Scale Dot-Prodution Attention，按照</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs tex">Divide into heads -&gt; Linear Func -&gt; Matmul(energy) -&gt; Scale -&gt; Mask(masked<span class="hljs-built_in">_</span>fill) -&gt; Softmax(at dim -1) -&gt; Matmul<br></code></pre></td></tr></table></figure><p>的顺序逐步进行矩阵变换与点乘。</p><p>第 20-23 行对应的是<code>Divide into heads</code>步骤。<code>forward</code>接收的<code>Q、K、V</code>参数的shape是<code>N x seq_len x embed_size</code>，其中<code>N</code>表示一个batch里的总数，<code>seq_len</code>代表sequence的长度，<code>embed_size</code>在这里也对应于<code>self.embed_size</code>的参数。为了完成多头注意力机制，对于<code>Q、K、V</code>要做<code>reshape</code>操作，将原shape重构成<code>[N, key_len, heads, heads_dim]</code>，其中<code>H</code>代表头的数量(heads)，<code>D</code>代表每个头的维度dim，其中和定义的一样，<code>embed_size = heads * heads_dim</code>。</p><p>第 25-28 行对应<code>Q、K、V</code>的线性变换操作，不再赘述</p><p>第 30-37 行对应第一个Matmul操作和Scale放缩，是对<code>Q</code>和<code>K</code>完成点乘。这里有矩阵shape的变换如注释所示，从两个单独的矩阵变成一个乘积矩阵，实现方法有以下两种：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># 1、逐步操作</span><br><span class="hljs-comment"># 修改Q和K的shape </span><br><span class="hljs-attribute">query</span> = query.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>) # shape:<span class="hljs-meta"> [N, n_heads, query_len, head_dim]</span><br><span class="hljs-attribute">key</span> = key.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>) # shape:<span class="hljs-meta"> [N, n_heads, key_len, head_dim]</span><br><span class="hljs-attribute">energy</span> = (Q @ K.permute(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)) / self.embed_size ** (<span class="hljs-number">1</span>/<span class="hljs-number">2</span>) # energy shape:<span class="hljs-meta"> [N, n_heads, query_len, key_len]</span><br><br><span class="hljs-comment"># 2、爱因斯坦求和</span><br><span class="hljs-attribute">energy</span> = torch.einsum(&#x27;nqhl,nkhl-&gt;nhqk&#x27;,[query,key])<br><span class="hljs-attribute">energy</span> = energy / self.embed_size ** (<span class="hljs-number">1</span>/<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p>其中，第一种方法逐步操作比较好理解，我们的目标shape为<code>[N, n_heads, query_len, key_len]</code>，方便之后和<code>V</code>点乘时的操作，需要保留<code>N</code>和<code>heads</code>这两个维度，因此首先对<code>Q</code>和<code>K</code>进行维度操作，这里用到了矩阵维度变换中常用的两种操作（见下注解1），将这两个维度前置。完成reshape操作之后，Q @ K.permute获得了结果为<code>[N, n_heads, query_len, key_len]</code>的乘积。</p><p>第二种是一种特化表示法（见下注解2）的使用，逻辑和上面其实大差不差，但是操作上会简化很多。这样基本就实现了<code>Q @ K</code>的维度操作，这里难度比较大。</p><p>第 39-41 行实现Mask部分的操作，如果设置了mask，自注意力就将mask填充到注意力的结果中。这里使用到了masked_fill方法。</p><p>第 43-44 行完成softmax操作，对最后一维（数值维）进行softmax</p><p>第 45-49 行完成第二个Matmul操作，这里的方式和前面讲解的基本一致，需要注意的是我们最后得到结果理论上应该是query的shape，即在注意力计算之后shape不变，因此需要首先reshape成<code>[N, q_len, heads, heads_dim]</code>，然后再将最后两维合并成<code>[N, q_len, embed_size]</code></p><p>第 51-52 行完成注意力计算后的线性变换。至此完成点积注意力的实现。</p><h3 id="注解"><a href="#注解" class="headerlink" title="注解"></a>注解</h3><blockquote><p><strong>注解1：维度修改(torch.view)和维度转置(torch.permute)</strong></p></blockquote><p>参见<a href="https://blog.csdn.net/weixin_41735859/article/details/106049688">pytorch中reshape()、view()、permute()、transpose()总结_景唯acr的博客-CSDN博客</a></p><p><strong>torch.view和torch.reshape</strong>两个方法为维度修改操作，是针对维度进行修改，可以删除或者增加维度，<a href="https://so.csdn.net/so/search?q=pytorch&spm=1001.2101.3001.7020">pytorch</a> 中的 torch.reshape() 大致相当于 tensor.contiguous().view()。使用torch.view()时需要保证tensor的连续性contiguous</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">a.contiguous().view()<br></code></pre></td></tr></table></figure><p>torch.permute和torch.transpose两个方法为维度转置操作，transpose 只能一次转换两个维度，permute 可以一次转换多个维度。具体来说，permute通过对原shape从左到右标号，然后重新设置先后顺序，来达到多个维度的转置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">a.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>) <span class="hljs-comment"># 将原来的0,1,2,3的维度顺序修改为0,2,1,3</span><br></code></pre></td></tr></table></figure><p>注意，permute和transpose都不能实现维度的增加或减少。</p><blockquote><p><strong>注解2：爱因斯坦求和(torch.einsum)</strong></p></blockquote><p>参见：<a href="https://blog.csdn.net/a2806005024/article/details/96462827">Pytorch中, torch.einsum详解-CSDN博客</a></p><p>爱因斯坦求和是一种对求和公式简洁高效的记法，以一种统一的方式表示各种各样的张量运算（内积、外积、转置、点乘、矩阵的迹、其他自定义运算），简化基本运算的操作。当然这个维度变换需要一定的学习成本。以上面的代码为例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># query shape: n q h l(下标表示)</span><br><span class="hljs-comment"># key shape: n k h l(下标表示)</span><br><span class="hljs-comment"># einsum: nqhl,nkql(代表两个矩阵乘)-&gt;nhqk(代表最后想要的shape)</span><br>energy = torch.einsum(<span class="hljs-string">&#x27;nqhl,nkhl-&gt;nhqk&#x27;</span>,[query,key])<br></code></pre></td></tr></table></figure><blockquote><p><strong>注解3：掩码填充(torch.masked_fill)</strong></p></blockquote><p>函数签名：masked_fill_(mask, value)</p><p>用value填充tensor中满足mask表达式的部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 判定mask中为0的部分，将其填充为-1e20，使其在注意力计算中的比重降为极低</span><br>energy = energy.masked_fill(mask == <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-1e20&#x27;</span>))<br></code></pre></td></tr></table></figure><h2 id="Transformer-Block"><a href="#Transformer-Block" class="headerlink" title="Transformer Block"></a>Transformer Block</h2><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><p><img src="/post/92486878/image-20230830224045849.png" alt="image-20230830224045849"></p><p>在Transform的Encoder和Decoder中，有一段Block可以复用，如红框所示，这里我们首先实现这个Block。</p><p>在图中可以看出，Block按照</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs txt">Multi-head Attention(SelfAttention) -&gt; Add&amp;Norm -&gt; Feed Forward -&gt; Add&amp;Norm<br></code></pre></td></tr></table></figure><p>的顺序构筑，更多的，在<code>Add &amp; Norm</code>之后一般需要接上一个<code>Dropout</code>，在ffn层中一般也要接进去<code>Dropout</code>来提高随机性，因此在<code>__init__</code>部分需要率先对这些层完成预定义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerBlock</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                 embed_size,</span><br><span class="hljs-params">                 heads,</span><br><span class="hljs-params">                 forward_expansion,</span><br><span class="hljs-params">                 dropout</span>):<br>        <span class="hljs-built_in">super</span>(TransformerBlock,self).__init__()<br>        self.embed_size = embed_size<br>        self.heads = heads<br>        self.attention = SelfAttention(<br>            embed_size=embed_size,<br>            heads=heads<br>        )<br>        self.norm1 = nn.LayerNorm(embed_size)<br>        self.ffn = nn.Sequential(OrderedDict([<br>            (<span class="hljs-string">&#x27;hidden_layer&#x27;</span>,nn.Linear(embed_size,forward_expansion*embed_size)),<br>            (<span class="hljs-string">&#x27;activation&#x27;</span>,nn.ReLU()),<br>            (<span class="hljs-string">&#x27;dropout&#x27;</span>,nn.Dropout(dropout)),<br>            (<span class="hljs-string">&#x27;output_layer&#x27;</span>,nn.Linear(forward_expansion*embed_size,embed_size))<br>        ]))<br>        self.norm2 = nn.LayerNorm(embed_size)<br>        self.dropout = nn.Dropout(dropout)<br></code></pre></td></tr></table></figure><p>前面的基本参数和attention的定义比较简单，两个norm层在Transformer中使用的是LayerNorm（相对于BatchNorm，见讲解）；ffn（feed forward）层先将数据映射到高维空间，然后非线性激活并加入dropout，最后还原，这样先高维后低维的操作可以更好的提取抽象特征；最后的dropout就是两部分之间的随机后处理。</p><p>在<code>__init__</code>的参数列表中，<code>embed_size</code>和<code>heads</code>是Attention所需参数；<code>forward_expansion</code>用于ffn层的高维映射，从<code>embed_size</code>映射到<code>forward_expansion * embed_size</code>，即成比例扩大；<code>dropout</code>是<code>Dropout</code>层的drop单位比例。</p><p>在<code>forward</code>方法中，输入可以看图片中Block的指入的指针，即下面的<code>Q、K、V</code>三个部分，在计算attention时还需加入<code>mask</code>的处理。<code>forward</code>就实现了Block的基本逻辑</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,value,key,query,mask</span>):<br>    <span class="hljs-comment"># multi-head attention</span><br>    att = self.attention(value,key,query,mask)<br>    <span class="hljs-comment"># add &amp; norm 1</span><br>    att = self.dropout(self.norm1(att + query))<br>    <span class="hljs-comment"># feed forward</span><br>    out = self.ffn(att)<br>    <span class="hljs-comment"># add &amp; norm 2</span><br>    out = self.dropout(self.norm2(out + att))<br>    <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><p>首先，调用attention，传入<code>Q、K、V</code>和<code>mask</code>，执行多头注意力操作；后先将attention的值和输入的其中一部分（这里为query，因为Decoder中只有query是从input中输入的）加和做残差，输入到norm1之后执行dropout；后面执行ffn层，并将ffn的输出out和att加和做残差，输入到norm2之后执行dropout。这样就完成了此TransformerBlock的基本逻辑。</p><h3 id="注解-1"><a href="#注解-1" class="headerlink" title="注解"></a>注解</h3><blockquote><p><strong>注解：LayerNorm和BatchNorm的区别</strong></p></blockquote><p>参见：<a href="https://zhuanlan.zhihu.com/p/608655896">BatchNorm与LayerNorm的理解 - 知乎 (zhihu.com)</a>，<a href="https://zhuanlan.zhihu.com/p/74516930">NLP中 batch normalization与 layer normalization - 知乎 (zhihu.com)</a></p><p>Batch 顾名思义是对一个batch进行操作。假设我们有 10行 3列 的数据，即我们的batchsize &#x3D; 10，每一行数据有三个特征，假设这三个特征是【身高、体重、年龄】。那么BN是针对每一列（特征）进行缩放，例如算出【身高】的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种“列缩放”。</p><p>而layer方向相反，它针对的是每一行进行缩放。即只看一笔数据，算出这笔所有特征的均值与方差再缩放。这是一种“行缩放”。</p><p>在NLP中，由于BatchNorm操作时是对每句话的第一个词、第二个词、…、第m个词操作，缺乏合理性，而LayerNorm是以每句话为单位操作，因此LayerNorm在NLP中使用的更为广泛。</p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p><img src="/post/92486878/image-20230831195638650.png" alt="image-20230831195638650"></p><p>Encoder部分用于对语料的编码，使其能够在提取信息后用于自注意力机制。前文提到的Transformer Block已经实现了Block的构筑，在具体到Encoder部分时我们就只需要完成以下部分：</p><ul><li>输入编码：完成Word_Embedding和Position_Embedding</li><li>模块堆叠：实现Block x N</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                 src_vocab_size,</span><br><span class="hljs-params">                 embed_size,</span><br><span class="hljs-params">                 heads,</span><br><span class="hljs-params">                 forward_expansion,</span><br><span class="hljs-params">                 dropout,</span><br><span class="hljs-params">                 n_layers,</span><br><span class="hljs-params">                 max_len,</span><br><span class="hljs-params">                 device</span>):<br>        <span class="hljs-built_in">super</span>(Encoder,self).__init__()<br>        self.embed_size = embed_size<br>        self.device = device<br>        <span class="hljs-comment"># embedding module</span><br>        self.word_embedding = nn.Embedding(src_vocab_size,embed_size)<br>        self.position_embedding = nn.Embedding(max_len,embed_size)<br>        <span class="hljs-comment"># N x Transformer block</span><br>        self.layers = nn.ModuleList([<br>            TransformerBlock(<br>                embed_size=embed_size,<br>                heads=heads,<br>                forward_expansion=forward_expansion,<br>                dropout=dropout<br>            ) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_layers)<br>        ])<br>        self.dropout = nn.Dropout(dropout)<br><br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><p>因此，<code>__init__</code>部分就只需完成两个Embedding的定义，多个TransformerBlock的序列定义（nn.ModuleList），还有其中随机Drop用的Dropout层；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Embedding层</span><br>self.word_embedding = nn.Embedding(src_vocab_size,embed_size)<br>self.position_embedding = nn.Embedding(max_len,embed_size)<br><span class="hljs-comment"># 多个Block堆叠</span><br>self.layers = nn.ModuleList([<br>    TransformerBlock(<br>        embed_size=embed_size,<br>        heads=heads,<br>        forward_expansion=forward_expansion,<br>        dropout=dropout<br>    ) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_layers)<br>])<br></code></pre></td></tr></table></figure><p><code>forward</code>部分实现基本逻辑，首先定义位置的概念。这里简化位置的定义，直接用0~seq_len-1的位置硬编号来表示word的绝对位置，当然这种基于顺序的位置编码一定是有效的。</p><p>对于位置，首先设置一个从0到seq_len-1的<code>(1,seq_len)</code>维tensor，然后使用expand将其扩为<code>（N,seq_len）</code>维（见注解），最后送到device中供运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">position = torch.arange(<span class="hljs-number">0</span>,seq_len).expand(N,seq_len).to(self.device)<br></code></pre></td></tr></table></figure><p>之后，使用定义好的两个embedding模块对原tensor和位置进行编码，相加之后过一个dropout，就实现了对输入的预处理，相加结果存为<code>out</code>，并在下面每次调用一层Transformer Block时作为<code>Q、K、V</code>输入到网络中，获得中间结果并再次存回<code>out</code>供下层输入。在图中我们也可以看到，Encoder的<code>Q、K、V</code>在定义上是一致的，都是输入编码。</p><h3 id="注解-2"><a href="#注解-2" class="headerlink" title="注解"></a>注解</h3><blockquote><p><strong>注解：tensor扩张（torch.repeat和torch.expand）</strong></p></blockquote><p>参见：<a href="https://zhuanlan.zhihu.com/p/555322123">「PyTorch」repeat() 和 expand() - 知乎 (zhihu.com)</a></p><p><code>torch.repeat</code>会复制数据本身，将其从维度上扩展</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>x.repeat(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>tensor([[<span class="hljs-number">0.7953</span>, <span class="hljs-number">0.4801</span>, <span class="hljs-number">0.7178</span>, <span class="hljs-number">0.7953</span>, <span class="hljs-number">0.4801</span>, <span class="hljs-number">0.7178</span>],<br>        [<span class="hljs-number">0.7953</span>, <span class="hljs-number">0.4801</span>, <span class="hljs-number">0.7178</span>, <span class="hljs-number">0.7953</span>, <span class="hljs-number">0.4801</span>, <span class="hljs-number">0.7178</span>],<br>        [<span class="hljs-number">0.7953</span>, <span class="hljs-number">0.4801</span>, <span class="hljs-number">0.7178</span>, <span class="hljs-number">0.7953</span>, <span class="hljs-number">0.4801</span>, <span class="hljs-number">0.7178</span>]])<br></code></pre></td></tr></table></figure><p>上例中，x从第一维（可以理解成行维）扩展三倍（即从一行-&gt;三行），从第二维（理解成列维）扩展两倍（即从三列-&gt;六列）。</p><p><code>torch.expand</code>在平铺的时候只能扩张大小为 1 的维度，但其不会复制数据，因此效率高于<code>torch.repeat</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">y = torch.rand(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>y.expand(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>tensor([[[<span class="hljs-number">0.1799</span>, <span class="hljs-number">0.5697</span>, <span class="hljs-number">0.9464</span>, <span class="hljs-number">0.7371</span>],<br>         [<span class="hljs-number">0.1799</span>, <span class="hljs-number">0.5697</span>, <span class="hljs-number">0.9464</span>, <span class="hljs-number">0.7371</span>],<br>         [<span class="hljs-number">0.1799</span>, <span class="hljs-number">0.5697</span>, <span class="hljs-number">0.9464</span>, <span class="hljs-number">0.7371</span>]],<br><br>        [[<span class="hljs-number">0.6830</span>, <span class="hljs-number">0.6944</span>, <span class="hljs-number">0.0609</span>, <span class="hljs-number">0.6319</span>],<br>         [<span class="hljs-number">0.6830</span>, <span class="hljs-number">0.6944</span>, <span class="hljs-number">0.0609</span>, <span class="hljs-number">0.6319</span>],<br>         [<span class="hljs-number">0.6830</span>, <span class="hljs-number">0.6944</span>, <span class="hljs-number">0.0609</span>, <span class="hljs-number">0.6319</span>]],<br><br>        [[<span class="hljs-number">0.3938</span>, <span class="hljs-number">0.3009</span>, <span class="hljs-number">0.9311</span>, <span class="hljs-number">0.6702</span>],<br>         [<span class="hljs-number">0.3938</span>, <span class="hljs-number">0.3009</span>, <span class="hljs-number">0.9311</span>, <span class="hljs-number">0.6702</span>],<br>         [<span class="hljs-number">0.3938</span>, <span class="hljs-number">0.3009</span>, <span class="hljs-number">0.9311</span>, <span class="hljs-number">0.6702</span>]]])<br></code></pre></td></tr></table></figure><p>上例中，y从第二维（单维度）扩展三倍。</p><h2 id="Decoder-Block"><a href="#Decoder-Block" class="headerlink" title="Decoder Block"></a>Decoder Block</h2><p><img src="/post/92486878/image-20230831203843543.png" alt="image-20230831203843543"></p><p>在实现Transformer Block时我们就提到，Decoder中也有复用的部分，我们完成了复用部分的实现，那其余的部分则需另外实现，这里就是对Decoder部分的另外实现。</p><p>在图中可以看出，除了TransformerBlock之外，只有下面的<code>mask-attention</code>模块和携带的<code>add&amp;norm</code>需要实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># DecoderBlock = TransformerBlock + Additional Block</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderBlock</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                 embed_size,</span><br><span class="hljs-params">                 heads,</span><br><span class="hljs-params">                 forward_expansion,</span><br><span class="hljs-params">                 dropout</span>):<br>        <span class="hljs-built_in">super</span>(DecoderBlock,self).__init__()<br>        self.embed_size = embed_size<br>        self.heads = heads<br>        <span class="hljs-comment"># Masked Attention</span><br>        self.attention = SelfAttention(<br>            embed_size=embed_size,<br>            heads=heads<br>        )<br>        <span class="hljs-comment"># Add &amp; Norm</span><br>        self.norm = nn.LayerNorm(embed_size)<br>        <span class="hljs-comment"># Transformer Block</span><br>        self.transformer_block = TransformerBlock(<br>            embed_size=embed_size,<br>            heads=heads,<br>            forward_expansion=forward_expansion,<br>            dropout=dropout<br>        )<br>        self.dropout = nn.Dropout(dropout)`<br></code></pre></td></tr></table></figure><p>和前面实现的部分基本一致，<code>__init__</code>部分实现逻辑需要的模块，这里即为一个Attention、一个LayerNorm，还有一个TransformerBlock；Dropout常规随机处理。</p><p><code>forward</code>的实现也没有什么花样，将其逻辑串起来即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,value,key,x,src_mask,trg_mask</span>):<br>    <span class="hljs-comment"># masked multi-head attention</span><br>    att = self.attention(x,x,x,trg_mask)<br>    <span class="hljs-comment"># add &amp; norm</span><br>    att = self.dropout(self.norm(att + x))<br>    <span class="hljs-comment"># transform block</span><br>    out = self.transformer_block(value,key,att,src_mask)<br>    <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><p>这里我们不区分Decoder的<code>Q、K、V</code>分别是什么，这个区分放到Decoder中来实现。<code>Masked Multi-head Attention</code>的<code>mask</code>使用的是目标语言即<code>trg_mask</code>，而TransformerBlock的部分使用的是从Encoder传过来的语言的<code>src_mask</code>。</p><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>和Encoder的设置基本一致，Decoder主要将DecoderBlock中未涉及到的输入部分和堆叠部分予以实现，因此<code>__init__</code>部分大部分基本和Encoder一致，不再赘述；除Encoder中涉及到的部分外，Decoder在输出部分会有一个<code>Linear</code>和<code>Softmax</code>，将Decoder的结果映射回到既有维度，<code>Softmax</code>直接使用函数调用，而<code>Linear</code>则需要预定义在<code>__init__</code>中训练。</p><p><img src="/post/92486878/image-20230831205902518.png" alt="image-20230831205902518"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Decoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                 trg_vocab_size,</span><br><span class="hljs-params">                 embed_size,</span><br><span class="hljs-params">                 heads,</span><br><span class="hljs-params">                 forward_expansion,</span><br><span class="hljs-params">                 dropout,</span><br><span class="hljs-params">                 n_layers,</span><br><span class="hljs-params">                 max_len,</span><br><span class="hljs-params">                 device</span>) :<br>        <span class="hljs-built_in">super</span>(Decoder,self).__init__()<br>        <span class="hljs-comment"># 和Encoder基本一致</span><br>        <span class="hljs-comment"># -------------------</span><br>        self.embed_size = embed_size<br>        self.device = device<br>        self.word_embedding = nn.Embedding(trg_vocab_size,embed_size)<br>        self.position_embedding = nn.Embedding(max_len,embed_size)<br>        self.layers = nn.ModuleList([<br>            DecoderBlock(<br>                embed_size=embed_size,<br>                heads=heads,<br>                forward_expansion=forward_expansion,<br>                dropout=dropout<br>            ) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_layers)<br>        ])<br>        self.dropout = nn.Dropout(dropout)<br>        <span class="hljs-comment"># --------------------</span><br>        <span class="hljs-comment"># 输出线性映射</span><br>        self.fnn = nn.Linear(embed_size,trg_vocab_size)<br></code></pre></td></tr></table></figure><p>而<code>forward</code>执行Decoder的步骤，像Encoder一样对输入和位置编码并送到Attention中，执行Add &amp; Norm，后将Decoder的中间结果作为<code>query</code>，Encoder的输出作为<code>key</code>和<code>value</code>输入到TransformerBlock中，以达到用<code>key</code>检索<code>query</code>的能力。最后将Decoder的输出套上线性层和Softmax。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x,enc_out,src_mask,trg_mask</span>):<br>        N,seq_len = x.shape<br>        position = torch.arange(<span class="hljs-number">0</span>,seq_len).expand(N,seq_len).to(self.device)<br>    <br>        <span class="hljs-comment">#decoder embedding</span><br>        word_embedding = self.word_embedding(x)<br>        position_embedding = self.position_embedding(position)<br>        out = self.dropout(word_embedding+position_embedding) <br><br>        <span class="hljs-comment">#stacking</span><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:<br>            out = layer(enc_out,enc_out,out,src_mask,trg_mask)<br>        <br>        out = self.fnn(out)<br>        <span class="hljs-keyword">return</span> out <br></code></pre></td></tr></table></figure><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>将<code>Encoder</code>和<code>Decoder</code>组装起来，即可完成Transformer的搭建，这个模块的功能十分简单，获取Encoder和Decoder的输入，分别送到Encoder和Decoder完成整体训练即可。这里需要注意的是<code>src_mask</code>和<code>trg_mask</code>需要在组装时提前实现。</p><p>首先是<code>__init__</code>函数，接收<code>src</code>和<code>trg</code>的<code>vocab_size</code>和<code>pad_idx（一般一致）</code>，并将两个部分需要的参数传进来。参数中，大部分是<code>Encoder</code>和<code>Decoder</code>需要的参数（后面八个），前面的四个为source和target序列的基本参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Transformer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, </span><br><span class="hljs-params">                 <span class="hljs-comment"># src and trg basic param</span></span><br><span class="hljs-params">                 src_vocab_size,</span><br><span class="hljs-params">                 trg_vocab_size,</span><br><span class="hljs-params">                 src_pad_idx,</span><br><span class="hljs-params">                 trg_pad_idx,</span><br><span class="hljs-params">                 <span class="hljs-comment"># model param</span></span><br><span class="hljs-params">                 embed_size=<span class="hljs-number">256</span>,</span><br><span class="hljs-params">                 heads=<span class="hljs-number">8</span>,</span><br><span class="hljs-params">                 forward_expansion=<span class="hljs-number">4</span>,</span><br><span class="hljs-params">                 dropout=<span class="hljs-number">0</span>,</span><br><span class="hljs-params">                 max_len=<span class="hljs-number">100</span>,</span><br><span class="hljs-params">                 device=<span class="hljs-string">&quot;cuda&quot;</span>,</span><br><span class="hljs-params">                 n_encoder_layers=<span class="hljs-number">6</span>,</span><br><span class="hljs-params">                 n_decoder_layers=<span class="hljs-number">6</span></span>):<br>        <br>        <span class="hljs-built_in">super</span>(Transformer,self).__init__()<br>        self.encoder = Encoder(<br>            src_vocab_size=src_vocab_size,<br>            embed_size=embed_size,<br>            heads=heads,<br>            forward_expansion=forward_expansion,<br>            dropout=dropout,<br>            max_len=max_len,<br>            device=device,<br>            n_layers=n_encoder_layers<br>        )<br><br>        self.decoder = Decoder(<br>            trg_vocab_size=trg_vocab_size,<br>            embed_size=embed_size,<br>            heads=heads,<br>            forward_expansion=forward_expansion,<br>            dropout=dropout,<br>            max_len=max_len,<br>            device=device,<br>            n_layers=n_decoder_layers<br>        )<br>        self.src_pad_idx = src_pad_idx<br>        self.trg_pad_idx = trg_pad_idx<br>        self.device = device<br></code></pre></td></tr></table></figure><p>然后在Transformer中定义mask的获取方法</p><p>对于source部分，实现一个padding mask，我们按照是否为<code>src_pad_idx</code>来判断，如果是则代表token是补足的padding token，没有作用，应该设为无限小的mask。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">src_mask</span>(<span class="hljs-params">self, src</span>):<br>    <span class="hljs-comment"># [N, 1, 1, seq_len], 便于之后做广播运算</span><br>    src_mask = (src != self.src_pad_idx).unsqueeze(<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> src_mask.to(self.device)<br></code></pre></td></tr></table></figure><p>对于decoder部分，实现一个sequence mask，旨在mask掉未来的token，使得模型专注于前面的sequence。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">trg_mask</span>(<span class="hljs-params">self, trg</span>):<br>    <span class="hljs-comment"># shape</span><br>    N, trg_len = trg.shape<br>    <span class="hljs-comment"># trg_pad_mask, padding mask</span><br>    trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">2</span>)<br>    <span class="hljs-comment"># 首先创建一个下三角的mask</span><br>    <span class="hljs-comment"># 再使用按位与，将pad_mask也加进去（广播）</span><br>    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).<span class="hljs-built_in">bool</span>().to(self.device) &amp; trg_pad_mask<br>    <span class="hljs-keyword">return</span> trg_mask.to(self.device)<br></code></pre></td></tr></table></figure><p>最后将<code>forward</code>实现即可，forward接收<code>source seq</code>和<code>target seq</code>，生成对应mask，将其先放到<code>Encoder</code>生成<code>encoder_output</code>后，其作为<code>k、v</code>加入到<code>Decoder</code>中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,src,trg</span>):<br>    <span class="hljs-comment"># generate masks</span><br>    src_masks = self.src_mask(src)<br>    trg_masks = self.trg_mask(trg)<br><br>    <span class="hljs-comment"># encoder output</span><br>    enc_out = self.encoder(src,src_masks)<br>    <span class="hljs-comment"># decoder output</span><br>    out = self.decoder(trg,enc_out,src_masks,trg_masks)<br>    <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><p>综上便完成了整体Transformer的搭建</p><h2 id="Test-code"><a href="#Test-code" class="headerlink" title="Test code"></a>Test code</h2><p>下附一个简单测试代码，没有什么实际含义，主要是用于测试矩阵shape是否对齐。下一篇具体就德语到英语的翻译模型来继续应用咱的Transformer:))))</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment"># device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available else &quot;cpu&quot;)</span><br>    device = <span class="hljs-string">&quot;cuda&quot;</span><br>    <br>    x = torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">9</span>,<span class="hljs-number">5</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">8</span>,<span class="hljs-number">7</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">2</span>]]).to(device)<br>    trg = torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">7</span>,<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">7</span>,<span class="hljs-number">6</span>,<span class="hljs-number">2</span>]]).to(device)<br>    src_pad_idx = <span class="hljs-number">0</span><br>    trg_pad_idx = <span class="hljs-number">0</span><br>    src_vocab_size = <span class="hljs-number">10</span><br>    trg_vocab_size = <span class="hljs-number">10</span><br><br>    model = Transformer(<br>        src_vocab_size=src_vocab_size,<br>        trg_vocab_size=trg_vocab_size,<br>        src_pad_idx=src_pad_idx,<br>        trg_pad_idx=trg_pad_idx<br>    ).to(device)<br><br>    out = model(x, trg[:, :-<span class="hljs-number">1</span>])<br>    <span class="hljs-comment"># out = torch.softmax(out,dim=2)</span><br>    output = model(x, trg[:, :-<span class="hljs-number">1</span>])<br>    output_dim = output.shape[-<span class="hljs-number">1</span>]<br>    output = output.contiguous().view(-<span class="hljs-number">1</span>, output_dim) <span class="hljs-comment">#batch_size * sequence_length, output_dim</span><br>    trg = trg[:, <span class="hljs-number">1</span>:].contiguous().view(-<span class="hljs-number">1</span>) <span class="hljs-comment">#batch_size * sequence_length</span><br>    <span class="hljs-built_in">print</span>(output.shape,trg.shape)     <br>    <span class="hljs-built_in">print</span>(output)<br>    <span class="hljs-built_in">print</span>(trg)       <br>    <span class="hljs-comment"># 获取概率最高的索引</span><br>    predicted_indices = torch.argmax(out, dim=<span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment">#去掉序列末尾</span><br>    predicted_indices = predicted_indices[:, :-<span class="hljs-number">1</span>]<br><br>    <span class="hljs-comment"># 输出预测的下一个数</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted Next Token Indices:&quot;</span>)<br>    <span class="hljs-built_in">print</span>(predicted_indices)<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> transformer </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux服务器命令行配置conda虚拟环境</title>
      <link href="/post/dd88fd87.html"/>
      <url>/post/dd88fd87.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Linux服务器命令行配置conda虚拟环境"><a href="#Linux服务器命令行配置conda虚拟环境" class="headerlink" title="Linux服务器命令行配置conda虚拟环境"></a>Linux服务器命令行配置conda虚拟环境</h1><blockquote><p>先驱条件：Linux服务器已有GPU驱动</p></blockquote><h2 id="1-Anaconda安装"><a href="#1-Anaconda安装" class="headerlink" title="1 Anaconda安装"></a>1 Anaconda安装</h2><h3 id="1-1-下载安装包"><a href="#1-1-下载安装包" class="headerlink" title="1.1 下载安装包"></a>1.1 下载安装包</h3><p>这里我们使用清华镜像文件下载会更快</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2020.07-Linux-x86_64.sh<br></code></pre></td></tr></table></figure><h3 id="1-2-安装"><a href="#1-2-安装" class="headerlink" title="1.2 安装"></a>1.2 安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash Anaconda3-2020.07-Linux-x86_64.sh<br></code></pre></td></tr></table></figure><h3 id="1-3-更新环境变量"><a href="#1-3-更新环境变量" class="headerlink" title="1.3 更新环境变量"></a>1.3 更新环境变量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><p>更新完成之后，本地环境就会被默认为<code>base</code>环境。</p><h2 id="2-创建conda虚拟环境"><a href="#2-创建conda虚拟环境" class="headerlink" title="2 创建conda虚拟环境"></a>2 创建conda虚拟环境</h2><p>anaconda安装成功后即可以使用conda指令来创建虚拟环境。</p><h3 id="2-1-创建环境"><a href="#2-1-创建环境" class="headerlink" title="2.1 创建环境"></a>2.1 创建环境</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda create -n name python=3.x<br></code></pre></td></tr></table></figure><p>其中<code>name</code>对应于虚拟环境的名称，<code>python=3.x</code>对应于python默认版本</p><p>安装过程中直接无脑y即可</p><h3 id="2-2-激活环境"><a href="#2-2-激活环境" class="headerlink" title="2.2 激活环境"></a>2.2 激活环境</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda activate name<br></code></pre></td></tr></table></figure><p>此时可以看到conda环境的切换</p><p><img src="/post/dd88fd87/image-20230817214341994.png" alt="image-20230817214341994"></p><p>需要关闭虚拟环境可以使用<code>deactivate</code>指令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda deactivate name<br></code></pre></td></tr></table></figure><h3 id="2-3-安装其他包"><a href="#2-3-安装其他包" class="headerlink" title="2.3 安装其他包"></a>2.3 安装其他包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install xxx<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> conda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hugging face Tutorial笔记</title>
      <link href="/post/9586f022.html"/>
      <url>/post/9586f022.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="使用Transformers的框架"><a href="#使用Transformers的框架" class="headerlink" title="使用Transformers的框架"></a>使用Transformers的框架</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入需要用到的库</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoTokenizer, AutoModelForSequenceClassification<br><br><span class="hljs-comment"># 加载预训练好的 BERT 模型和分词器</span><br>checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span><br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)<br>model = AutoModelForSequenceClassification.from_pretrained(checkpoint)<br><br><span class="hljs-comment"># 定义两个待处理的样本序列</span><br>sequences = [<br>    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,<br>    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,<br>]<br><br><span class="hljs-comment"># 对输入序列进行分词和处理，生成一个张量的批次</span><br>batch = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br><br><span class="hljs-comment"># 在批次中添加标签张量</span><br>batch[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># 定义优化器并计算损失函数</span><br>optimizer = AdamW(model.parameters())<br>loss = model(**batch).loss<br><br><span class="hljs-comment"># 计算梯度并执行优化器的一次反向传播操作</span><br>loss.backward()<br>optimizer.step()<br></code></pre></td></tr></table></figure><h1 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h1><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入需要使用的库和模块</span><br><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, DataCollatorWithPadding<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br><span class="hljs-comment"># 加载SST-2数据集</span><br>raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;sst2&quot;</span>)<br><br><span class="hljs-comment"># 设置预训练模型</span><br>checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span><br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)<br><br><span class="hljs-comment"># 定义对数据集的处理函数，使用tokenizer对输入数据进行分词，并设置截断和填充。</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_func</span>(<span class="hljs-params">example</span>):<br>    <span class="hljs-keyword">return</span> tokenizer(<br>        example[<span class="hljs-string">&#x27;sentence&#x27;</span>],<br>        truncation=<span class="hljs-literal">True</span>,<br>        padding=<span class="hljs-literal">True</span><br>    )<br><br><span class="hljs-comment"># 对数据集进行分词处理，并移除不需要的列</span><br><span class="hljs-comment"># 根据代码中的操作，我们可以发现，我们可以移除idx和sentence这两列，因为它们在后面的处理中不会被使用</span><br>tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_func, batched=<span class="hljs-literal">True</span>)<br>tokenized_datasets = tokenized_datasets.remove_columns([<span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>])<br>tokenized_datasets = tokenized_datasets.rename_column(<span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>)<br>tokenized_datasets = tokenized_datasets.with_format(<span class="hljs-string">&#x27;torch&#x27;</span>)<br><br><span class="hljs-comment"># 定义数据收集器，用于按需填充输入数据</span><br>data_collator = DataCollatorWithPadding(tokenizer)<br><br><span class="hljs-comment"># 创建一个批量数据加载器，并使用数据收集器进行填充</span><br>train_dataloader = DataLoader(<br>    tokenized_datasets[<span class="hljs-string">&#x27;train&#x27;</span>],<br>    batch_size=<span class="hljs-number">16</span>,<br>    shuffle=<span class="hljs-literal">True</span>,<br>    collate_fn=data_collator<br>)<br><br><span class="hljs-comment"># 遍历数据加载器并输出每个批次的输入ID形状</span><br><span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataloader):<br>    <span class="hljs-built_in">print</span>(batch[<span class="hljs-string">&#x27;input_ids&#x27;</span>].shape)<br>    <span class="hljs-keyword">if</span> step &gt; <span class="hljs-number">5</span>:<br>        <span class="hljs-keyword">break</span><br><br></code></pre></td></tr></table></figure><h2 id="加入Trainer训练"><a href="#加入Trainer训练" class="headerlink" title="加入Trainer训练"></a>加入Trainer训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, DataCollatorWithPadding, TrainingArguments,Trainer, AutoModelForSequenceClassification<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">import</span> evaluate<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;sst2&quot;</span>)<br><br>checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span><br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)  <span class="hljs-comment"># 从checkpoint中加载预训练好的tokenizer</span><br>model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=<span class="hljs-number">2</span>) <span class="hljs-comment"># 从checkpoint中加载预训练好的分类模型，num_labels为分类数量</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_func</span>(<span class="hljs-params">example</span>):<br>    <span class="hljs-keyword">return</span> tokenizer(<br>        example[<span class="hljs-string">&#x27;sentence&#x27;</span>],<br>        truncation=<span class="hljs-literal">True</span>,<br>        padding=<span class="hljs-literal">True</span><br>    )<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">predictions</span>):<br>    metrics = evaluate.load(<span class="hljs-string">&#x27;glue&#x27;</span>,<span class="hljs-string">&#x27;sst2&#x27;</span>)<br>    logits,labels = predictions<br>    pred = np.argmax(logits,axis=-<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> metrics.compute(predictions=pred, references=labels)<br><br>tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_func, batched=<span class="hljs-literal">True</span>) <span class="hljs-comment"># 使用tokenizer对数据集进行tokenize操作</span><br><br>data_collator = DataCollatorWithPadding(tokenizer) <span class="hljs-comment"># 使用tokenizer对数据进行padding和collate操作</span><br><br>train_dataloader = DataLoader(<br>    tokenized_datasets[<span class="hljs-string">&#x27;train&#x27;</span>], <span class="hljs-comment"># 加载tokenize后的训练数据</span><br>    batch_size=<span class="hljs-number">16</span>, <span class="hljs-comment"># 指定batch_size大小</span><br>    shuffle=<span class="hljs-literal">True</span>, <span class="hljs-comment"># 是否打乱数据</span><br>    collate_fn=data_collator <span class="hljs-comment"># 指定collate函数</span><br>)<br><br>training_args = TrainingArguments(<span class="hljs-string">&quot;test_training&quot;</span>,evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>) <span class="hljs-comment"># 指定训练参数</span><br><br>trainer = Trainer(<br>    model,<br>    training_args,<br>    train_dataset = tokenized_datasets[<span class="hljs-string">&#x27;train&#x27;</span>], <span class="hljs-comment"># 指定训练集</span><br>    eval_dataset = tokenized_datasets[<span class="hljs-string">&#x27;validation&#x27;</span>], <span class="hljs-comment"># 指定验证集</span><br>    data_collator = data_collator, <span class="hljs-comment"># 指定collator</span><br>    tokenizer = tokenizer, <span class="hljs-comment"># 指定tokenizer</span><br>    compute_metrics = compute_metrics <span class="hljs-comment"># 指定评价函数</span><br>)<br><br>trainer.train() <span class="hljs-comment"># 进行模型训练</span><br></code></pre></td></tr></table></figure><h2 id="含加速器的完整代码"><a href="#含加速器的完整代码" class="headerlink" title="含加速器的完整代码"></a>含加速器的完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> evaluate<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, get_scheduler<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator<br><br><span class="hljs-comment"># 获取tokenizer</span><br>checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span><br>raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;sst2&quot;</span>)<br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)<br><span class="hljs-comment"># 对数据集进行处理</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_func</span>(<span class="hljs-params">example</span>):<br>    <span class="hljs-keyword">return</span> tokenizer(<br>        example[<span class="hljs-string">&#x27;sentence&#x27;</span>],<br>        truncation = <span class="hljs-literal">True</span><br>    )<br>tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_func,batched=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 对不需要的列和错误的列做处理</span><br>tokenized_datasets = tokenized_datasets.remove_columns([<span class="hljs-string">&quot;sentence&quot;</span>, <span class="hljs-string">&quot;idx&quot;</span>])<br>tokenized_datasets = tokenized_datasets.rename_column(<span class="hljs-string">&quot;label&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>)<br>tokenized_datasets.set_format(<span class="hljs-string">&quot;torch&quot;</span>)<br><span class="hljs-comment"># 建立数据加载器，方便数据存取</span><br><span class="hljs-comment"># 动态填充</span><br>data_collator = DataCollatorWithPadding(tokenizer)<br>train_dataloader = DataLoader(<br>    tokenized_datasets[<span class="hljs-string">&#x27;train&#x27;</span>],<br>    shuffle = <span class="hljs-literal">True</span>,<br>    batch_size = <span class="hljs-number">8</span>,<br>    collate_fn = data_collator<br>)<br>eval_dataloader = DataLoader(<br>    tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],<br>    batch_size=<span class="hljs-number">8</span>,<br>    collate_fn=data_collator<br>)<br><span class="hljs-comment"># 定义模型</span><br>model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)<br><span class="hljs-comment"># 设置优化器和学习率调整器</span><br>optimizer = AdamW(model.parameters(),lr=<span class="hljs-number">5e-5</span>)<br>num_epochs = <span class="hljs-number">3</span><br>num_train_steps = num_epochs * <span class="hljs-built_in">len</span>(train_dataloader) <span class="hljs-comment"># 注意这里</span><br>lr_scheduler = get_scheduler(<br>    <span class="hljs-string">&quot;linear&quot;</span>,<br>    optimizer=optimizer,<br>    num_warmup_steps=<span class="hljs-number">0</span>,<br>    num_training_steps=num_train_steps,<br>)<br><span class="hljs-comment"># 设置训练的加速器</span><br>accelerator = Accelerator()<br><span class="hljs-comment"># 将模型，数据loader和优化器传进去</span><br>model,train_dataloader,eval_dataloader,optimizer = accelerator.prepare(<br>    model,train_dataloader,eval_dataloader,optimizer<br>)<br><span class="hljs-comment"># 设置跑模型的设备为Cuda</span><br>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>)<br><span class="hljs-comment"># 调整模型的运行到GPU</span><br>model.to(device)<br><span class="hljs-comment"># 训练过程</span><br>progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_train_steps)) <span class="hljs-comment"># tqdm的进度条</span><br>model.train() <span class="hljs-comment"># 设置为train模式</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:<br>        <span class="hljs-comment"># 将batch中的数据也放到device</span><br>        <span class="hljs-comment"># batch = &#123;k: v.to(device) for k, v in batch.items()&#125;</span><br>        output = model(**batch)<br>        <span class="hljs-comment"># 在output中获取孙树</span><br>        loss = output.loss<br>        <span class="hljs-comment"># 反向传播</span><br>        loss.backward()<br>        <span class="hljs-comment"># 用加速器反向传播</span><br>        <span class="hljs-comment"># 梯度更新</span><br>        optimizer.step()<br>        <span class="hljs-comment"># 更新学习率</span><br>        lr_scheduler.step()<br>        <span class="hljs-comment"># 设置grad为0，防止影响下次训练</span><br>        optimizer.zero_grad()<br>        progress_bar.update(<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 加载评估指标</span><br>metrics = evaluate.load(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;sst2&quot;</span>)<br>model.<span class="hljs-built_in">eval</span>() <span class="hljs-comment"># 设置为评估模式</span><br><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> eval_dataloader:<br>    <span class="hljs-comment"># 将batch中的tensor数据移动到设备上（GPU或CPU）</span><br>    batch = &#123;k :v.to(device) <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> batch.items()&#125;<br>    <span class="hljs-comment"># 关闭梯度计算，加快推理速度</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        output = model(**batch) <span class="hljs-comment"># 通过模型进行预测</span><br>    <span class="hljs-comment"># 获取预测结果</span><br>    logits = output.logits<br>    pred  = torch.argmax(logits,axis=-<span class="hljs-number">1</span>)<br>    metrics.add_batch(predictions=accelerator.gather(pred),references=accelerator.gather(batch[<span class="hljs-string">&#x27;labels&#x27;</span>]))<br>metrics.compute() <span class="hljs-comment"># 计算评估指标的平均值</span><br></code></pre></td></tr></table></figure><h2 id="train-py"><a href="#train-py" class="headerlink" title="train.py"></a>train.py</h2><p><img src="/2023-04-18-Hugging-face-Tutorial%E7%AC%94%E8%AE%B0/image-20230427235756835.png" alt="image-20230427235756835"></p><p><img src="/2023-04-18-Hugging-face-Tutorial%E7%AC%94%E8%AE%B0/image-20230428000011925.png" alt="image-20230428000011925"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Hugging face </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N作业A5：Attention和Transformers</title>
      <link href="/post/686c4b32.html"/>
      <url>/post/686c4b32.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="row">    <embed src="../download/a5.pdf" width="100%" height="550" type="application/pdf"></div><h1 id="1、Attention-exploration-20-points"><a href="#1、Attention-exploration-20-points" class="headerlink" title="1、Attention exploration (20 points)"></a>1、Attention exploration (20 points)</h1><h3 id="（a）"><a href="#（a）" class="headerlink" title="（a）"></a>（a）</h3><h4 id="（i）"><a href="#（i）" class="headerlink" title="（i）"></a>（i）</h4><p>key向量可以对应于n个离散的类别，而$\alpha_i&#x3D;\frac{\exp(k_i^\top q)}{\sum_{j&#x3D;1}^n\exp(k_j^\top q)},0\le \alpha_i \le 1$，且$\sum_{i&#x3D;1}^n \alpha_i &#x3D; 1$，所以注意力分布$a_i$可以视作一个对应于n个类别的分类概率分布。</p><h4 id="（ii）"><a href="#（ii）" class="headerlink" title="（ii）"></a>（ii）</h4><p>当$k_j^\top q \ll k_i^\top q, i\in \lbrace 1,…,n \rbrace \and i\ne j$时，注意力权重会集中到$\alpha_j$。</p><h4 id="（iii）"><a href="#（iii）" class="headerlink" title="（iii）"></a>（iii）</h4><p>在（ii）条件下，$c \approx v_j$</p><h4 id="（iv）"><a href="#（iv）" class="headerlink" title="（iv）"></a>（iv）</h4><p>如果查询q与单个key向量非常相似，并且几乎与其余的key向量正交，那么注意力输出c可能几乎与该键向量的相应值向量相同，就像直接“复制”了一样。</p><h3 id="（b）"><a href="#（b）" class="headerlink" title="（b）"></a>（b）</h3><h4 id="（i）-1"><a href="#（i）-1" class="headerlink" title="（i）"></a>（i）</h4><p>不妨设$v_a&#x3D;c_1a_1+…+c_ma_m,v_b&#x3D;d_1b_1+…+d_pb_p$，$\forall a_j^\top$，有：</p><ul><li>$a_j^\top v_a &#x3D; c_1a_j^\top a_1 + … +c_j a_j^\top a_j + …+ c_m a_j^\top a_m&#x3D;c_j$</li><li>$a_j^\top v_b &#x3D; d_1a_j^\top b_1+…+d_ja_j^\top b_j+ … + d_pa_j^\top b_p &#x3D; 0$</li></ul><p>$∴$令$M &#x3D; \begin{bmatrix}<br> a_1^\top \\<br>…\\<br> a_m^\top<br>\end{bmatrix}$，则$M(v_a+v_b) &#x3D; Mv_a &#x3D; v_a$</p><h4 id="（ii）-1"><a href="#（ii）-1" class="headerlink" title="（ii）"></a>（ii）</h4><p>若令$c\approx \frac{1}{2}(v_a+v_b)$，则需要a和b有大致同等的权重，而其他部分没有权重，$∴k_a^\top q \approx k_b^\top q$，又$∵$要加大a和b与其他部分的差距，$∴q &#x3D; \beta(k_a+k_b)$，其中$\beta \gg 0$，此时$k_a^\top q &#x3D; k^\top_b q &#x3D; \beta$</p><h3 id="（c）"><a href="#（c）" class="headerlink" title="（c）"></a>（c）</h3><h4 id="（i）-2"><a href="#（i）-2" class="headerlink" title="（i）"></a>（i）</h4><p>$∵\Sigma_i &#x3D; \alpha I$且$\alpha$可忽略，$∴$在正态分布中，$k\approx \mu$，$∴$同1(b).ii，$q &#x3D; \beta(\mu_a+\mu_b),\beta\gg 0$。</p><h4 id="（ii）-2"><a href="#（ii）-2" class="headerlink" title="（ii）"></a>（ii）</h4><p>$∵\Sigma_a &#x3D; \alpha I+\frac{1}{2}(\mu_a\mu_a^\top)$且$\alpha$可忽略，所以$k_a&#x3D;\gamma\mu_a,\gamma \sim\mathcal N(1,\frac{1}{2})$，$∴$有<br>$$<br>c &#x3D; v_a\alpha_a+v_b\alpha_b &#x3D; \frac{\exp(\gamma\beta)}{\exp(\gamma\beta)+\exp(\beta)}v_a+\frac{\exp(\beta)}{\exp(\gamma\beta)+\exp(\beta)}v_b &#x3D; \frac{1}{exp((1-\gamma)\beta)+1}v_a + \frac{1}{\exp((\gamma-1)\beta)+1}v_b<br>$$<br>即随着$k$的不同取样，$c$在$v_a$和$v_b$之间震荡。</p><h3 id="（d）"><a href="#（d）" class="headerlink" title="（d）"></a>（d）</h3><h4 id="（i）-3"><a href="#（i）-3" class="headerlink" title="（i）"></a>（i）</h4><p>$c &#x3D; \frac{1}{2}(c_1+c_2) &#x3D; \frac{1}{2}(v_a+v_b)$，因此分别使$q_1$和$q_2$对应于$\frac{1}{2}v_a$和${1\over 2}v_b$，$∴q_1 &#x3D; \beta \mu_a,q_2 &#x3D; \beta\mu_b,\beta\gg 0$</p><h4 id="（ii）-3"><a href="#（ii）-3" class="headerlink" title="（ii）"></a>（ii）</h4><p>$c\approx \frac{1}{2}(v_a+v_b)$，$∵k$变化会导致$\alpha_a$和$\alpha_b$独立发生变化，但当$\beta$足够大时，其查询$q$的结果依然会分别偏向$v_a$和$v_b$</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N作业A4：机器翻译</title>
      <link href="/post/1ce6e410.html"/>
      <url>/post/1ce6e410.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="row">    <embed src="../download/a4.pdf" width="100%" height="550" type="application/pdf"></div><h1 id="1、Neural-Machine-Translation-with-RNNs-45-points"><a href="#1、Neural-Machine-Translation-with-RNNs-45-points" class="headerlink" title="1、Neural Machine Translation with RNNs (45 points)"></a>1、<strong>Neural Machine Translation with RNNs (45 points)</strong></h1><h3 id="（a）"><a href="#（a）" class="headerlink" title="（a）"></a>（a）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pad_sents</span>(<span class="hljs-params">sents, pad_token</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Pad list of sentences according to the longest sentence in the batch.</span><br><span class="hljs-string">        The paddings should be at the end of each sentence.</span><br><span class="hljs-string">    @param sents (list[list[str]]): list of sentences, where each sentence</span><br><span class="hljs-string">                                    is represented as a list of words</span><br><span class="hljs-string">    @param pad_token (str): padding token</span><br><span class="hljs-string">    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter</span><br><span class="hljs-string">        than the max length sentence are padded out with the pad_token, such that</span><br><span class="hljs-string">        each sentences in the batch now has equal length.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    sents_padded = []<br><br>    <span class="hljs-comment">### YOUR CODE HERE (~6 Lines)</span><br>    len_list = [<span class="hljs-built_in">len</span>(l) <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> sents]<br>    max_len = <span class="hljs-built_in">sorted</span>(len_list,reverse=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> sents:<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_len-<span class="hljs-built_in">len</span>(l)):<br>            l.append(pad_token)<br>        sents_padded.append(l)<br>    <span class="hljs-comment">### END YOUR CODE</span><br>    <span class="hljs-keyword">return</span> sents_padded<br></code></pre></td></tr></table></figure><h3 id="（b）"><a href="#（b）" class="headerlink" title="（b）"></a>（b）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ModelEmbeddings</span>(nn.Module): <br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Class that converts input words to their embeddings.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, embed_size, vocab</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Init the Embedding layers.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        @param embed_size (int): Embedding size (dimensionality)</span><br><span class="hljs-string">        @param vocab (Vocab): Vocabulary object containing src and tgt languages</span><br><span class="hljs-string">                              See vocab.py for documentation.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(ModelEmbeddings, self).__init__()<br>        self.embed_size = embed_size<br><br>        <span class="hljs-comment"># default values</span><br>        self.source = <span class="hljs-literal">None</span><br>        self.target = <span class="hljs-literal">None</span><br><br>        src_pad_token_idx = vocab.src[<span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>]<br>        tgt_pad_token_idx = vocab.tgt[<span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>]<br><br>        <span class="hljs-comment">### YOUR CODE HERE (~2 Lines)</span><br>        <span class="hljs-comment">### TODO - Initialize the following variables:</span><br>        <span class="hljs-comment">###     self.source (Embedding Layer for source language)</span><br>        <span class="hljs-comment">###     self.target (Embedding Layer for target langauge)</span><br>        <span class="hljs-comment">###</span><br>        <span class="hljs-comment">### Note:</span><br>        <span class="hljs-comment">###     1. `vocab` object contains two vocabularies:</span><br>        <span class="hljs-comment">###            `vocab.src` for source</span><br>        <span class="hljs-comment">###            `vocab.tgt` for target</span><br>        <span class="hljs-comment">###     2. You can get the length of a specific vocabulary by running:</span><br>        <span class="hljs-comment">###             `len(vocab.&lt;specific_vocabulary&gt;)`</span><br>        <span class="hljs-comment">###     3. Remember to include the padding token for the specific vocabulary</span><br>        <span class="hljs-comment">###        when creating your Embedding.</span><br>        <span class="hljs-comment">###</span><br>        <span class="hljs-comment">### Use the following docs to properly initialize these variables:</span><br>        <span class="hljs-comment">###     Embedding Layer:</span><br>        <span class="hljs-comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding</span><br>        self.source = nn.Embedding(<span class="hljs-built_in">len</span>(vocab.src),embed_size,padding_idx=src_pad_token_idx)<br>        self.target = nn.Embedding(<span class="hljs-built_in">len</span>(vocab.tgt),embed_size,padding_idx=tgt_pad_token_idx)<br><br>        <span class="hljs-comment">### END YOUR CODE</span><br></code></pre></td></tr></table></figure><h3 id="（c）"><a href="#（c）" class="headerlink" title="（c）"></a>（c）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, embed_size, hidden_size, vocab, dropout_rate=<span class="hljs-number">0.2</span></span>):<br>       <span class="hljs-string">&quot;&quot;&quot; Init NMT Model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">       @param embed_size (int): Embedding size (dimensionality)</span><br><span class="hljs-string">       @param hidden_size (int): Hidden Size, the size of hidden states (dimensionality)</span><br><span class="hljs-string">       @param vocab (Vocab): Vocabulary object containing src and tgt languages</span><br><span class="hljs-string">                             See vocab.py for documentation.</span><br><span class="hljs-string">       @param dropout_rate (float): Dropout probability, for attention</span><br><span class="hljs-string">       &quot;&quot;&quot;</span><br>       <span class="hljs-built_in">super</span>(NMT, self).__init__()<br>       self.model_embeddings = ModelEmbeddings(embed_size, vocab)<br>       self.hidden_size = hidden_size<br>       self.dropout_rate = dropout_rate<br>       self.vocab = vocab<br><br>       <span class="hljs-comment"># default values</span><br>       self.encoder = <span class="hljs-literal">None</span><br>       self.decoder = <span class="hljs-literal">None</span><br>       self.h_projection = <span class="hljs-literal">None</span><br>       self.c_projection = <span class="hljs-literal">None</span><br>       self.att_projection = <span class="hljs-literal">None</span><br>       self.combined_output_projection = <span class="hljs-literal">None</span><br>       self.target_vocab_projection = <span class="hljs-literal">None</span><br>       self.dropout = <span class="hljs-literal">None</span><br>       <span class="hljs-comment"># For sanity check only, not relevant to implementation</span><br>       self.gen_sanity_check = <span class="hljs-literal">False</span><br>       self.counter = <span class="hljs-number">0</span><br><br>       <span class="hljs-comment">### YOUR CODE HERE (~9 Lines)</span><br>       <span class="hljs-comment">### TODO - Initialize the following variables IN THIS ORDER:</span><br>       <span class="hljs-comment">###     self.post_embed_cnn (Conv1d layer with kernel size 2, input and output channels = embed_size,</span><br>       <span class="hljs-comment">###         padding = same to preserve output shape)</span><br>       <span class="hljs-comment">###     self.encoder (Bidirectional LSTM with bias)</span><br>       <span class="hljs-comment">###     self.decoder (LSTM Cell with bias)</span><br>       <span class="hljs-comment">###     self.h_projection (Linear Layer with no bias), called W_&#123;h&#125; in the PDF.</span><br>       <span class="hljs-comment">###     self.c_projection (Linear Layer with no bias), called W_&#123;c&#125; in the PDF.</span><br>       <span class="hljs-comment">###     self.att_projection (Linear Layer with no bias), called W_&#123;attProj&#125; in the PDF.</span><br>       <span class="hljs-comment">###     self.combined_output_projection (Linear Layer with no bias), called W_&#123;u&#125; in the PDF.</span><br>       <span class="hljs-comment">###     self.target_vocab_projection (Linear Layer with no bias), called W_&#123;vocab&#125; in the PDF.</span><br>       <span class="hljs-comment">###     self.dropout (Dropout Layer)</span><br>       <span class="hljs-comment">###</span><br>       <span class="hljs-comment">### Use the following docs to properly initialize these variables:</span><br>       <span class="hljs-comment">###     LSTM:</span><br>       <span class="hljs-comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM</span><br>       <span class="hljs-comment">###     LSTM Cell:</span><br>       <span class="hljs-comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell</span><br>       <span class="hljs-comment">###     Linear Layer:</span><br>       <span class="hljs-comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear</span><br>       <span class="hljs-comment">###     Dropout Layer:</span><br>       <span class="hljs-comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout</span><br>       <span class="hljs-comment">###     Conv1D Layer:</span><br>       <span class="hljs-comment">###         https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html</span><br>       <span class="hljs-comment">#padding的策略是samee      </span><br>       self.post_embed_cnn = nn.Conv1d(in_channels=embed_size,out_channels=embed_size,kernel_size=<span class="hljs-number">2</span>,padding=<span class="hljs-string">&quot;same&quot;</span>)<br>       self.encoder = nn.LSTM(input_size=embed_size,hidden_size=hidden_size,bidirectional=<span class="hljs-literal">True</span>,bias=<span class="hljs-literal">True</span>)<br>       self.decoder = nn.LSTMCell(input_size=embed_size+hidden_size,hidden_size=hidden_size,bias=<span class="hljs-literal">True</span>)<br>       self.h_projection = nn.Linear(in_features=<span class="hljs-number">2</span>*hidden_size,out_features=hidden_size,bias=<span class="hljs-literal">False</span>)<br>       self.c_projection = nn.Linear(in_features=<span class="hljs-number">2</span>*hidden_size,out_features=hidden_size,bias=<span class="hljs-literal">False</span>)<br>       self.att_projection = nn.Linear(in_features=<span class="hljs-number">2</span>*hidden_size,out_features=hidden_size,bias=<span class="hljs-literal">False</span>)<br>       self.combined_output_projection = nn.Linear(in_features=<span class="hljs-number">3</span>*hidden_size,out_features=hidden_size,bias=<span class="hljs-literal">False</span>)<br>       <span class="hljs-comment">#这里需要注意输出的是vocab.target的size</span><br>       self.target_vocab_projection = nn.Linear(in_features=hidden_size,out_features=<span class="hljs-built_in">len</span>(vocab.tgt),bias=<span class="hljs-literal">False</span>)<br>       self.dropout = nn.Dropout(p=dropout_rate)<br><br>       <span class="hljs-comment">### END YOUR CODE</span><br></code></pre></td></tr></table></figure><h3 id="（d）"><a href="#（d）" class="headerlink" title="（d）"></a>（d）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, source_padded: torch.Tensor, source_lengths: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">Tuple</span>[<br>    torch.Tensor, <span class="hljs-type">Tuple</span>[torch.Tensor, torch.Tensor]]:<br>    <span class="hljs-string">&quot;&quot;&quot; Apply the encoder to source sentences to obtain encoder hidden states.</span><br><span class="hljs-string">        Additionally, take the final states of the encoder and project them to obtain initial states for decoder.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where</span><br><span class="hljs-string">                                    b = batch_size, src_len = maximum source sentence length. Note that</span><br><span class="hljs-string">                                   these have already been sorted in order of longest to shortest sentence.</span><br><span class="hljs-string">    @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch</span><br><span class="hljs-string">    @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where</span><br><span class="hljs-string">                                    b = batch size, src_len = maximum source sentence length, h = hidden size.</span><br><span class="hljs-string">    @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder&#x27;s initial</span><br><span class="hljs-string">                                            hidden state and cell. Both tensors should have shape (2, b, h).</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    enc_hiddens, dec_init_state = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment">### YOUR CODE HERE (~ 11 Lines)</span><br>    <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span></span><br>    <span class="hljs-comment">###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.</span><br>    <span class="hljs-comment">###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note</span><br>    <span class="hljs-comment">###         that there is no initial hidden state or cell for the encoder.</span><br>    <span class="hljs-comment">###     2. Apply the post_embed_cnn layer. Before feeding X into the CNN, first use torch.permute to change the</span><br>    <span class="hljs-comment">###         shape of X to (b, e, src_len). After getting the output from the CNN, still stored in the X variable,</span><br>    <span class="hljs-comment">###         remember to use torch.permute again to revert X back to its original shape.</span><br>    <span class="hljs-comment">###     3. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.</span><br>    <span class="hljs-comment">###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.</span><br>    <span class="hljs-comment">###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.</span><br>    <span class="hljs-comment">###         - Note that the shape of the tensor output returned by the encoder RNN is (src_len, b, h*2) and we want to</span><br>    <span class="hljs-comment">###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`, so you may need to do more permuting.</span><br>    <span class="hljs-comment">###         - Note on using pad_packed_sequence -&gt; For batched inputs, you need to make sure that each of the</span><br>    <span class="hljs-comment">###           individual input examples has the same shape.</span><br>    <span class="hljs-comment">###     4. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):</span><br>    <span class="hljs-comment">###         - `init_decoder_hidden`:</span><br>    <span class="hljs-comment">###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.</span><br>    <span class="hljs-comment">###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).</span><br>    <span class="hljs-comment">###             Apply the h_projection layer to this in order to compute init_decoder_hidden.</span><br>    <span class="hljs-comment">###             This is h_0^&#123;dec&#125; in the PDF. Here b = batch size, h = hidden size</span><br>    <span class="hljs-comment">###         - `init_decoder_cell`:</span><br>    <span class="hljs-comment">###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.</span><br>    <span class="hljs-comment">###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).</span><br>    <span class="hljs-comment">###             Apply the c_projection layer to this in order to compute init_decoder_cell.</span><br>    <span class="hljs-comment">###             This is c_0^&#123;dec&#125; in the PDF. Here b = batch size, h = hidden size</span><br>    <span class="hljs-comment">###</span><br>    <span class="hljs-comment">### See the following docs, as you may need to use some of the following functions in your implementation:</span><br>    <span class="hljs-comment">###     Pack the padded sequence X before passing to the encoder:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html</span><br>    <span class="hljs-comment">###     Pad the packed sequence, enc_hiddens, returned by the encoder:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html</span><br>    <span class="hljs-comment">###     Tensor Concatenation:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/generated/torch.cat.html</span><br>    <span class="hljs-comment">###     Tensor Permute:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/generated/torch.permute.html</span><br>    <span class="hljs-comment">###     Tensor Reshape (a possible alternative to permute):</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html</span><br>    <span class="hljs-comment"># 注意这里需要理解一下模型的结构，前面定义了Model_embed层，其中source是一个Embedding，可以直接嵌入source语句</span><br>    X = self.model_embeddings.source(source_padded)<br>    X = self.post_embed_cnn(X.permute(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br>    X = X.permute(<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)<br>    enc_hiddens,(last_hidden,last_cell) = self.encoder(pack_padded_sequence(X,source_lengths))<br>    enc_hiddens = pad_packed_sequence(enc_hiddens)[<span class="hljs-number">0</span>].permute(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>)<br>    <span class="hljs-comment">#0，1的顺序（我个人感觉和pdf不一致）</span><br>    last_hidden = torch.cat((last_hidden[<span class="hljs-number">0</span>],last_hidden[<span class="hljs-number">1</span>]),dim=<span class="hljs-number">1</span>)<br>    last_cell = torch.cat((last_cell[<span class="hljs-number">0</span>],last_cell[<span class="hljs-number">1</span>]),dim=<span class="hljs-number">1</span>)<br>    init_decoder_hidden = self.h_projection(last_hidden)<br>    init_decoder_cell = self.c_projection(last_cell)<br>    dec_init_state = (init_decoder_hidden,init_decoder_cell)<br>    <span class="hljs-comment">### END YOUR CODE</span><br>    <span class="hljs-keyword">return</span> enc_hiddens, dec_init_state<br></code></pre></td></tr></table></figure><p><img src="/post/1ce6e410/image-20230401000237712.png" alt="image-20230401000237712"></p><h3 id="（e）"><a href="#（e）" class="headerlink" title="（e）"></a>（e）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,</span><br><span class="hljs-params">           dec_init_state: <span class="hljs-type">Tuple</span>[torch.Tensor, torch.Tensor], target_padded: torch.Tensor</span>) -&gt; torch.Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;Compute combined output vectors for a batch.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where</span><br><span class="hljs-string">                                 b = batch size, src_len = maximum source sentence length, h = hidden size.</span><br><span class="hljs-string">    @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where</span><br><span class="hljs-string">                                 b = batch size, src_len = maximum source sentence length.</span><br><span class="hljs-string">    @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder</span><br><span class="hljs-string">    @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where</span><br><span class="hljs-string">                                   tgt_len = maximum target sentence length, b = batch size.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where</span><br><span class="hljs-string">                                    tgt_len = maximum target sentence length, b = batch_size,  h = hidden size</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Chop off the &lt;END&gt; token for max length sentences.</span><br>    target_padded = target_padded[:-<span class="hljs-number">1</span>]<br><br>    <span class="hljs-comment"># Initialize the decoder state (hidden and cell)</span><br>    dec_state = dec_init_state<br><br>    <span class="hljs-comment"># Initialize previous combined output vector o_&#123;t-1&#125; as zero</span><br>    batch_size = enc_hiddens.size(<span class="hljs-number">0</span>)<br>    o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)<br><br>    <span class="hljs-comment"># Initialize a list we will use to collect the combined output o_t on each step</span><br>    combined_outputs = []<br><br>    <span class="hljs-comment">### YOUR CODE HERE (~9 Lines)</span><br>    <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span></span><br>    <span class="hljs-comment">###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,</span><br>    <span class="hljs-comment">###         which should be shape (b, src_len, h),</span><br>    <span class="hljs-comment">###         where b = batch size, src_len = maximum source length, h = hidden size.</span><br>    <span class="hljs-comment">###         This is applying W_&#123;attProj&#125; to h^enc, as described in the PDF.</span><br>    <span class="hljs-comment">###     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.</span><br>    <span class="hljs-comment">###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.</span><br>    <span class="hljs-comment">###     3. Use the torch.split function to iterate over the time dimension of Y.</span><br>    <span class="hljs-comment">###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.</span><br>    <span class="hljs-comment">###             - Squeeze Y_t into a tensor of dimension (b, e).</span><br>    <span class="hljs-comment">###             - Construct Ybar_t by concatenating Y_t with o_prev on their last dimension</span><br>    <span class="hljs-comment">###             - Use the step function to compute the the Decoder&#x27;s next (cell, state) values</span><br>    <span class="hljs-comment">###               as well as the new combined output o_t.</span><br>    <span class="hljs-comment">###             - Append o_t to combined_outputs</span><br>    <span class="hljs-comment">###             - Update o_prev to the new o_t.</span><br>    <span class="hljs-comment">###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of</span><br>    <span class="hljs-comment">###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)</span><br>    <span class="hljs-comment">###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.</span><br>    <span class="hljs-comment">###</span><br>    <span class="hljs-comment">### Note:</span><br>    <span class="hljs-comment">###    - When using the squeeze() function make sure to specify the dimension you want to squeeze</span><br>    <span class="hljs-comment">###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.</span><br>    <span class="hljs-comment">###</span><br>    <span class="hljs-comment">### You may find some of these functions useful:</span><br>    <span class="hljs-comment">###     Zeros Tensor:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/torch.html#torch.zeros</span><br>    <span class="hljs-comment">###     Tensor Splitting (iteration):</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/torch.html#torch.split</span><br>    <span class="hljs-comment">###     Tensor Dimension Squeezing:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/torch.html#torch.squeeze</span><br>    <span class="hljs-comment">###     Tensor Concatenation:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/torch.html#torch.cat</span><br>    <span class="hljs-comment">###     Tensor Stacking:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/torch.html#torch.stack</span><br>    <span class="hljs-comment">#enc_hidden:(b, src_len, h*2)</span><br>    enc_hiddens_proj = self.att_projection(enc_hiddens)<br>    <span class="hljs-comment"># self.model_embeddings.target</span><br>    Y = self.model_embeddings.target(target_padded)<br>    Y = torch.split(Y,<span class="hljs-number">1</span>,dim=<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">for</span> y_t <span class="hljs-keyword">in</span> Y:<br>        y_t = torch.squeeze(y_t)<br>        ybar_t = torch.cat((y_t,o_prev),dim=<span class="hljs-number">1</span>)<br>        dec_state,o_t,e_t = self.step(ybar_t,dec_state,enc_hiddens,enc_hiddens_proj,enc_masks)<br>        combined_outputs.append(o_t)<br>        o_prev = o_t<br>    combined_outputs=torch.stack(combined_outputs,dim=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">return</span> combined_outputs<br></code></pre></td></tr></table></figure><p><img src="/post/1ce6e410/image-20230401000612593.png" alt="image-20230401000612593"></p><h3 id="（f）"><a href="#（f）" class="headerlink" title="（f）"></a>（f）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self, Ybar_t: torch.Tensor,</span><br><span class="hljs-params">         dec_state: <span class="hljs-type">Tuple</span>[torch.Tensor, torch.Tensor],</span><br><span class="hljs-params">         enc_hiddens: torch.Tensor,</span><br><span class="hljs-params">         enc_hiddens_proj: torch.Tensor,</span><br><span class="hljs-params">         enc_masks: torch.Tensor</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>, torch.Tensor, torch.Tensor]:<br>    <span class="hljs-string">&quot;&quot;&quot; Compute one forward step of the LSTM decoder, including the attention computation.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,</span><br><span class="hljs-string">                            where b = batch size, e = embedding size, h = hidden size.</span><br><span class="hljs-string">    @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.</span><br><span class="hljs-string">            First tensor is decoder&#x27;s prev hidden state, second tensor is decoder&#x27;s prev cell.</span><br><span class="hljs-string">    @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,</span><br><span class="hljs-string">                                src_len = maximum source length, h = hidden size.</span><br><span class="hljs-string">    @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),</span><br><span class="hljs-string">                                where b = batch size, src_len = maximum source length, h = hidden size.</span><br><span class="hljs-string">    @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),</span><br><span class="hljs-string">                                where b = batch size, src_len is maximum source length.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.</span><br><span class="hljs-string">            First tensor is decoder&#x27;s new hidden state, second tensor is decoder&#x27;s new cell.</span><br><span class="hljs-string">    @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.</span><br><span class="hljs-string">    @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.</span><br><span class="hljs-string">                            Note: You will not use this outside of this function.</span><br><span class="hljs-string">                                  We are simply returning this value so that we can sanity check</span><br><span class="hljs-string">                                  your implementation.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    combined_output = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment">### YOUR CODE HERE (~3 Lines)</span><br>    <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span></span><br>    <span class="hljs-comment">###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.</span><br>    <span class="hljs-comment">###     2. Split dec_state into its two parts (dec_hidden, dec_cell)</span><br>    <span class="hljs-comment">###     3. Compute the attention scores e_t, a Tensor shape (b, src_len).</span><br>    <span class="hljs-comment">###        Note: b = batch_size, src_len = maximum source length, h = hidden size.</span><br>    <span class="hljs-comment">###</span><br>    <span class="hljs-comment">###       Hints:</span><br>    <span class="hljs-comment">###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)</span><br>    <span class="hljs-comment">###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_&#123;attProj&#125; h^enc (batched).</span><br>    <span class="hljs-comment">###         - Use batched matrix multiplication (torch.bmm) to compute e_t (be careful about the input/ output shapes!)</span><br>    <span class="hljs-comment">###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.</span><br>    <span class="hljs-comment">###         - When using the squeeze() function make sure to specify the dimension you want to squeeze</span><br>    <span class="hljs-comment">###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.</span><br>    <span class="hljs-comment">###</span><br>    <span class="hljs-comment">### Use the following docs to implement this functionality:</span><br>    <span class="hljs-comment">###     Batch Multiplication:</span><br>    <span class="hljs-comment">###        https://pytorch.org/docs/stable/torch.html#torch.bmm</span><br>    <span class="hljs-comment">###     Tensor Unsqueeze:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze</span><br>    <span class="hljs-comment">###     Tensor Squeeze:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/torch.html#torch.squeeze</span><br>    dec_state = self.decoder(Ybar_t,dec_state)<br>    dec_hidden = dec_state[<span class="hljs-number">0</span>]<br>    dec_cell = dec_state[<span class="hljs-number">1</span>]<br>    <span class="hljs-comment"># enc_hidden_proj:(b,src_len,h), dec_hidden:(b,h),e_t:(b,src_len)</span><br>    <span class="hljs-comment"># torch.squeeze去除size==1的维度，unsqueeze在指定维度补1</span><br>    e_t = torch.bmm(enc_hiddens_proj,dec_hidden.unsqueeze(-<span class="hljs-number">1</span>)).squeeze(-<span class="hljs-number">1</span>)<br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-comment"># Set e_t to -inf where enc_masks has 1</span><br>    <span class="hljs-keyword">if</span> enc_masks <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        e_t.data.masked_fill_(enc_masks.<span class="hljs-built_in">bool</span>(), -<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>))<br><br>    <span class="hljs-comment">### YOUR CODE HERE (~6 Lines)</span><br>    <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span></span><br>    <span class="hljs-comment">###     1. Apply softmax to e_t to yield alpha_t</span><br>    <span class="hljs-comment">###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the</span><br>    <span class="hljs-comment">###         attention output vector, a_t.</span><br>    <span class="hljs-comment"># $$     Hints:</span><br>    <span class="hljs-comment">###           - alpha_t is shape (b, src_len)</span><br>    <span class="hljs-comment">###           - enc_hiddens is shape (b, src_len, 2h)</span><br>    <span class="hljs-comment">###           - a_t should be shape (b, 2h)</span><br>    <span class="hljs-comment">###           - You will need to do some squeezing and unsqueezing.</span><br>    <span class="hljs-comment">###     Note: b = batch size, src_len = maximum source length, h = hidden size.</span><br>    <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span></span><br>    <span class="hljs-comment">###     3. Concatenate dec_hidden with a_t to compute tensor U_t</span><br>    <span class="hljs-comment">###     4. Apply the combined output projection layer to U_t to compute tensor V_t</span><br>    <span class="hljs-comment">###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.</span><br>    <span class="hljs-comment">###</span><br>    <span class="hljs-comment">### Use the following docs to implement this functionality:</span><br>    <span class="hljs-comment">###     Softmax:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.softmax</span><br>    <span class="hljs-comment">###     Batch Multiplication:</span><br>    <span class="hljs-comment">###        https://pytorch.org/docs/stable/torch.html#torch.bmm</span><br>    <span class="hljs-comment">###     Tensor View:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view</span><br>    <span class="hljs-comment">###     Tensor Concatenation:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/torch.html#torch.cat</span><br>    <span class="hljs-comment">###     Tanh:</span><br>    <span class="hljs-comment">###         https://pytorch.org/docs/stable/torch.html#torch.tanh</span><br>    alpha_t = F.softmax(e_t)<br>    a_t = torch.bmm(alpha_t.unsqueeze(<span class="hljs-number">1</span>),enc_hiddens).squeeze(dim=<span class="hljs-number">1</span>)<br>    U_t = torch.cat((a_t,dec_hidden),dim=<span class="hljs-number">1</span>)<br>    V_t = self.combined_output_projection(U_t)<br>    O_t = self.dropout(F.tanh(V_t))<br><br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    combined_output = O_t<br>    <span class="hljs-keyword">return</span> dec_state, combined_output, e_t<br></code></pre></td></tr></table></figure><h3 id="（g）"><a href="#（g）" class="headerlink" title="（g）"></a>（g）</h3><p>对encoder的padding数据进行mask，能够标记出数据中padding过程补足的位置，从而在训练过程中用一些方法将其忽略，减少其在注意力计算中产生的影响。使用masks时，对补充的位置masks值设置为1，即bool&#x3D;True，这样在Encoder建模过程中就可以将padding的位置补充为-inf，而注意力计算过程中$exp(-inf) \rightarrow 0$，从而使模型对padding的部分忽略。</p><h3 id="（h）"><a href="#（h）" class="headerlink" title="（h）"></a>（h）</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs txt">epoch 3, iter 18610, avg. loss 58.13, avg. ppl 8.95 cum. examples 320, speed 1491.16 words/sec, time elapsed 3291.41 sec<br>epoch 3, iter 18620, avg. loss 54.11, avg. ppl 8.00 cum. examples 640, speed 5900.33 words/sec, time elapsed 3292.82 sec<br>epoch 3, iter 18630, avg. loss 56.25, avg. ppl 8.79 cum. examples 960, speed 5968.53 words/sec, time elapsed 3294.21 sec<br>epoch 3, iter 18640, avg. loss 58.12, avg. ppl 9.20 cum. examples 1280, speed 5329.38 words/sec, time elapsed 3295.78 sec<br>epoch 3, iter 18650, avg. loss 51.07, avg. ppl 7.36 cum. examples 1600, speed 4701.92 words/sec, time elapsed 3297.52 sec<br>epoch 3, iter 18660, avg. loss 55.50, avg. ppl 8.40 cum. examples 1920, speed 5755.93 words/sec, time elapsed 3298.97 sec<br>epoch 3, iter 18670, avg. loss 53.45, avg. ppl 8.33 cum. examples 2240, speed 5521.04 words/sec, time elapsed 3300.44 sec<br>epoch 3, iter 18680, avg. loss 55.00, avg. ppl 8.06 cum. examples 2560, speed 5861.04 words/sec, time elapsed 3301.88 sec<br>epoch 3, iter 18690, avg. loss 54.89, avg. ppl 8.64 cum. examples 2880, speed 5247.28 words/sec, time elapsed 3303.43 sec<br>epoch 3, iter 18700, avg. loss 57.09, avg. ppl 8.81 cum. examples 3200, speed 5595.19 words/sec, time elapsed 3304.93 sec<br>epoch 3, iter 18710, avg. loss 59.10, avg. ppl 9.15 cum. examples 3520, speed 5764.15 words/sec, time elapsed 3306.41 sec<br>epoch 3, iter 18720, avg. loss 51.66, avg. ppl 7.38 cum. examples 3840, speed 4835.91 words/sec, time elapsed 3308.12 sec<br>epoch 3, iter 18730, avg. loss 58.08, avg. ppl 9.23 cum. examples 4160, speed 5069.38 words/sec, time elapsed 3309.77 sec<br>epoch 3, iter 18740, avg. loss 58.46, avg. ppl 8.77 cum. examples 4480, speed 5673.07 words/sec, time elapsed 3311.29 sec<br>epoch 3, iter 18750, avg. loss 58.70, avg. ppl 9.46 cum. examples 4800, speed 5863.14 words/sec, time elapsed 3312.72 sec<br>epoch 4, iter 18760, avg. loss 51.31, avg. ppl 6.98 cum. examples 5120, speed 5585.10 words/sec, time elapsed 3314.23 sec<br>epoch 4, iter 18770, avg. loss 49.36, avg. ppl 6.88 cum. examples 5440, speed 6081.40 words/sec, time elapsed 3315.58 sec<br>epoch 4, iter 18780, avg. loss 50.36, avg. ppl 7.06 cum. examples 5760, speed 5890.12 words/sec, time elapsed 3316.98 sec<br>epoch 4, iter 18790, avg. loss 48.79, avg. ppl 6.37 cum. examples 6080, speed 5914.69 words/sec, time elapsed 3318.40 sec<br>epoch 4, iter 18800, avg. loss 51.09, avg. ppl 7.16 cum. examples 6400, speed 5709.77 words/sec, time elapsed 3319.86 sec<br>epoch 4, iter 18800, cum. loss 54.53, cum. ppl 8.10 cum. examples 6400<br>begin validation ...<br>validation: iter 18800, dev. ppl 12.352464<br>hit patience 1<br>hit #4 trial<br>load previously best model and decay learning rate to 0.000031<br>restore parameters of the optimizers<br>epoch 4, iter 18810, avg. loss 52.73, avg. ppl 7.19 cum. examples 320, speed 1987.35 words/sec, time elapsed 3324.16 sec<br>epoch 4, iter 18820, avg. loss 50.84, avg. ppl 7.38 cum. examples 640, speed 5747.25 words/sec, time elapsed 3325.58 sec<br>epoch 4, iter 18830, avg. loss 51.07, avg. ppl 7.29 cum. examples 960, speed 5666.00 words/sec, time elapsed 3327.03 sec<br>epoch 4, iter 18840, avg. loss 50.78, avg. ppl 6.95 cum. examples 1280, speed 5808.57 words/sec, time elapsed 3328.47 sec<br>epoch 4, iter 18850, avg. loss 50.70, avg. ppl 6.89 cum. examples 1600, speed 5786.21 words/sec, time elapsed 3329.93 sec<br>epoch 4, iter 18860, avg. loss 52.06, avg. ppl 7.29 cum. examples 1920, speed 5404.75 words/sec, time elapsed 3331.48 sec<br>epoch 4, iter 18870, avg. loss 51.83, avg. ppl 7.13 cum. examples 2240, speed 5924.19 words/sec, time elapsed 3332.90 sec<br>epoch 4, iter 18880, avg. loss 51.02, avg. ppl 7.25 cum. examples 2560, speed 5307.42 words/sec, time elapsed 3334.46 sec<br>epoch 4, iter 18890, avg. loss 49.71, avg. ppl 6.98 cum. examples 2880, speed 5497.55 words/sec, time elapsed 3335.95 sec<br>epoch 4, iter 18900, avg. loss 51.77, avg. ppl 7.19 cum. examples 3200, speed 5847.27 words/sec, time elapsed 3337.38 sec<br>epoch 4, iter 18910, avg. loss 50.58, avg. ppl 6.99 cum. examples 3520, speed 5952.13 words/sec, time elapsed 3338.78 sec<br>epoch 4, iter 18920, avg. loss 53.59, avg. ppl 7.55 cum. examples 3840, speed 5720.31 words/sec, time elapsed 3340.26 sec<br>epoch 4, iter 18930, avg. loss 49.79, avg. ppl 6.85 cum. examples 4160, speed 5844.42 words/sec, time elapsed 3341.68 sec<br>epoch 4, iter 18940, avg. loss 53.56, avg. ppl 7.98 cum. examples 4480, speed 5481.18 words/sec, time elapsed 3343.19 sec<br>epoch 4, iter 18950, avg. loss 48.81, avg. ppl 6.52 cum. examples 4800, speed 5548.51 words/sec, time elapsed 3344.69 sec<br>epoch 4, iter 18960, avg. loss 51.65, avg. ppl 7.16 cum. examples 5120, speed 5802.95 words/sec, time elapsed 3346.14 sec<br>epoch 4, iter 18970, avg. loss 48.99, avg. ppl 6.55 cum. examples 5440, speed 5652.33 words/sec, time elapsed 3347.61 sec<br>epoch 4, iter 18980, avg. loss 52.00, avg. ppl 7.15 cum. examples 5760, speed 5564.60 words/sec, time elapsed 3349.13 sec<br>epoch 4, iter 18990, avg. loss 49.65, avg. ppl 6.67 cum. examples 6080, speed 5997.98 words/sec, time elapsed 3350.53 sec<br>epoch 4, iter 19000, avg. loss 49.06, avg. ppl 6.51 cum. examples 6400, speed 5984.15 words/sec, time elapsed 3351.93 sec<br>epoch 4, iter 19000, cum. loss 51.01, cum. ppl 7.06 cum. examples 6400<br>begin validation ...<br>validation: iter 19000, dev. ppl 12.401339<br>hit patience 1<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">[nltk_data] Downloading package punkt to /root/nltk_data...<br>[nltk_data]   Package punkt is already up-to-date!<br>load test source sentences from [./zh_en_data/test.zh]<br>load test target sentences from [./zh_en_data/test.en]<br>load model from model.bin<br>Decoding:   0% 0/1001 [00:00&lt;?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py:309: UserWarning: Using padding=&#x27;same&#x27; with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)<br>  return F.conv1d(input, weight, bias, self.stride,<br>/content/drive/MyDrive/a4/nmt_model.py:376: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.<br>  alpha_t = F.softmax(e_t)<br>/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.<br>  warnings.warn(&quot;nn.functional.tanh is deprecated. Use torch.tanh instead.&quot;)<br>Decoding: 100% 1001/1001 [00:43&lt;00:00, 28.36it/s]<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">## Corpus BLEU: 19.97235090067466</span></span><br></code></pre></td></tr></table></figure><h3 id="（i）"><a href="#（i）" class="headerlink" title="（i）"></a>（i）</h3><h4 id="（i）-1"><a href="#（i）-1" class="headerlink" title="（i）"></a>（i）</h4><p>优点：相比于multiplicative attention，其参数量更少，计算量更低</p><p>缺点：限定了查询向量$s$和值向量$h$的维度必须相等</p><h4 id="（ii）"><a href="#（ii）" class="headerlink" title="（ii）"></a>（ii）</h4><p>优点：将注意力变成了神经网络可训练的形式，可以将注意力和网络一起训练，提高效率</p><p>缺点：大规模加法的计算时间要远高于矩阵乘法</p><h1 id="2-Analyzing-NMT-Systems-25-points"><a href="#2-Analyzing-NMT-Systems-25-points" class="headerlink" title="2. Analyzing NMT Systems (25 points)"></a>2. Analyzing NMT Systems (25 points)</h1><h3 id="（a）-1"><a href="#（a）-1" class="headerlink" title="（a）"></a>（a）</h3><p>多个词或语素的组合可能会产生完全不同的意义，因此卷积层通过将相邻的几个词或语素结合起来，从而提高对语料数据集信息的提取程度，进而帮助到NMT系统之后的模型理解过程。</p><h3 id="（b）-1"><a href="#（b）-1" class="headerlink" title="（b）"></a>（b）</h3><h4 id="（i）-2"><a href="#（i）-2" class="headerlink" title="（i）"></a>（i）</h4><p><em>the culprits were</em>和<em>the culprit was</em>的不同之处在单复数的问题，模型没有完全学习出来句子中应该的单复数形式，或许可以通过增大数据量、扩大隐藏层大小等来解决；</p><h4 id="（ii）-1"><a href="#（ii）-1" class="headerlink" title="（ii）"></a>（ii）</h4><p><em>resources have been exhausted</em>重复了两次，或许可以通过引入自注意力机制，在句子生成时考虑到自身句子的合理性；</p><h4 id="（iii）"><a href="#（iii）" class="headerlink" title="（iii）"></a>（iii）</h4><p> <em>a national mourning today</em>在英语训练数据集中只出现了一次，而且“今天是XX日”的语言结构也比较难学习，可以通过针对性提高数据规模等方法提高对这种语言结构的学习能力</p><h4 id="（iv）"><a href="#（iv）" class="headerlink" title="（iv）"></a>（iv）</h4><p>“唔做唔错”是比较方言的语言，而模型的数据为普通话数据，因此训练出来的模型对这种方言比较难测试，可以通过加入方言数据或训练一个方言特化的模型来解决。</p><h3 id="（c）-1"><a href="#（c）-1" class="headerlink" title="（c）"></a>（c）</h3><h4 id="（i）-3"><a href="#（i）-3" class="headerlink" title="（i）"></a>（i）</h4><p>对于$c_1$：$p_1&#x3D;\frac{4}{9}$，$p_2&#x3D;\frac{3}{8}$，$BP\approx 0.8$，$BLEU\approx 0.32$</p><p>对于$c_2$：$p_1&#x3D;1$，$p_2&#x3D;\frac{3}{5}$，$BP&#x3D;1$，$BLEU\approx0.77$</p><p>$∵c_1&lt;c_2$，$∴c_2$的效果更好，我不认可这个结果。</p><h4 id="（ii）-2"><a href="#（ii）-2" class="headerlink" title="（ii）"></a>（ii）</h4><p>对于$c_1$：$p_1&#x3D;\frac{4}{9}$，$p_2&#x3D;\frac{3}{8}$，$BP\approx 1$，$BLEU\approx 0.4$</p><p>对于$c_2$：$p_1&#x3D;\frac{1}{2}$，$p_2&#x3D;\frac{1}{5}$，$BP&#x3D;1$，$BLEU\approx0.32$</p><p>$∵c_1&gt;c_2$，$∴c_1$的效果更好，我认可这个结果。</p><h4 id="（iii）-1"><a href="#（iii）-1" class="headerlink" title="（iii）"></a>（iii）</h4><p>只有单个参考翻译的$BLEU$会有比较高的噪声，即可能会过高或过低，从而对$BLEU$的评分产生一些影响。NMT系统在解释和解码源句子时更灵活，更有可能获得公平的BLEU评分。</p><h4 id="（iv）-1"><a href="#（iv）-1" class="headerlink" title="（iv）"></a>（iv）</h4><p>优点：</p><p>1、$BLEU$评分能够使得模型评价更简单，有一个固定的评价指标</p><p>2、语言独立，利于理解</p><p>缺点：</p><p>1、和人类直观评价相比仍有距离</p><p>2、没有考虑语法和句子结构</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N作业A3：依存分析</title>
      <link href="/post/30fdab5f.html"/>
      <url>/post/30fdab5f.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="row">    <embed src="../download/a3_handout.pdf" width="100%" height="550" type="application/pdf"></div><h1 id="1、Machine-Learning-amp-Neural-Networks-8-points"><a href="#1、Machine-Learning-amp-Neural-Networks-8-points" class="headerlink" title="1、Machine Learning &amp; Neural Networks (8 points)"></a>1、<strong>Machine Learning &amp; Neural Networks (8 points)</strong></h1><h3 id="（a）Adam-Optimizer"><a href="#（a）Adam-Optimizer" class="headerlink" title="（a）Adam Optimizer"></a>（a）Adam Optimizer</h3><h4 id="（i）"><a href="#（i）" class="headerlink" title="（i）"></a>（i）</h4><p>$m$通过在每次更新时将历史方向和梯度方向进行比较，同向则矢量相加加快收敛速度，异向则减缓更新速度，这种含动量的更新方向可以减少梯度更新的动荡，这种低方差有助于保持梯度下降的效率。导致更快的收敛。</p><h4 id="（ii）"><a href="#（ii）" class="headerlink" title="（ii）"></a>（ii）</h4><p>具有更少更新历史的模型参数将获得更大的更新。这规范化了更新步骤，避免超调或单调递减的学习速率，适度调整了学习率的大小</p><h3 id="（b）Dropout"><a href="#（b）Dropout" class="headerlink" title="（b）Dropout"></a>（b）Dropout</h3><h4 id="（i）-1"><a href="#（i）-1" class="headerlink" title="（i）"></a>（i）</h4><p>$d\odot h$在$1-p_{drop}$的比例下降低了隐向量的规模，为了将其恢复到原规模，$\gamma &#x3D; \frac{1}{1-p_{drop} }$</p><h4 id="（ii）-1"><a href="#（ii）-1" class="headerlink" title="（ii）"></a>（ii）</h4><p>训练时使用dropout可以提高训练模型的鲁棒性，防止过拟合现象发生，而评估时没有这个需要。</p><h1 id="2-Neural-Transition-Based-Dependency-Parsing-46-points"><a href="#2-Neural-Transition-Based-Dependency-Parsing-46-points" class="headerlink" title="2. Neural Transition-Based Dependency Parsing (46 points)"></a><strong>2. Neural Transition-Based Dependency Parsing (46 points)</strong></h1><h3 id="（a）"><a href="#（a）" class="headerlink" title="（a）"></a>（a）</h3><table><thead><tr><th>Stack</th><th>Buffer</th><th>New dependency</th><th>Transition</th></tr></thead><tbody><tr><td>[ROOT]</td><td>[I,attend,lectures,in,the,NLP,class]</td><td></td><td>Initial Configuration</td></tr><tr><td>[ROOT,I]</td><td>[attend,lectures,in,the,NLP,class]</td><td></td><td>SHIFT</td></tr><tr><td>[ROOT,I,attend]</td><td>[lectures,in,the,NLP,class]</td><td></td><td>SHIFT</td></tr><tr><td>[ROOT,attend]</td><td>[lectures,in,the,NLP,class]</td><td>attend-&gt;I</td><td>LEFT-ARC</td></tr><tr><td>[ROOT,attend,lectures]</td><td>[in,the,NLP,class]</td><td></td><td>SHIFT</td></tr><tr><td>[ROOT,attend]</td><td>[in,the,NLP,class]</td><td>attend-&gt;lectures</td><td>RIGHT-ARC</td></tr><tr><td>[ROOT,attend,in]</td><td>[the,NLP,class]</td><td></td><td>SHIFT</td></tr><tr><td>[ROOT,attend,in,the]</td><td>[NLP,class]</td><td></td><td>SHIFT</td></tr><tr><td>[ROOT,attend,in,the,NLP]</td><td>[class]</td><td></td><td>SHIFT</td></tr><tr><td>[ROOT,attend,in,the,NLP,class]</td><td>[]</td><td></td><td>SHIFT</td></tr><tr><td>[ROOT,attend,in,the,class]</td><td>[]</td><td>class-&gt;NLP</td><td>LEFT-ARC</td></tr><tr><td>[ROOT,attend,in,class]</td><td>[]</td><td>class-&gt;the</td><td>LEFT-ARC</td></tr><tr><td>[ROOT,attend,class]</td><td>[]</td><td>class-&gt;in</td><td>LEFT-ARC</td></tr><tr><td>[ROOT,attend]</td><td>[]</td><td>attend-&gt;class</td><td>RIGHT-ARC</td></tr><tr><td>[ROOT]</td><td>[]</td><td>ROOT-&gt;attend</td><td>RIGHT-ARC</td></tr><tr><td>[ROOT]</td><td>[]</td><td></td><td>Decline</td></tr></tbody></table><h3 id="（b）"><a href="#（b）" class="headerlink" title="（b）"></a>（b）</h3><p>$2n$，每个词都执行一个SHIFT，并作为关系尾执行一次LEFT-ARC或RIGHT-ARC</p><h3 id="（c）"><a href="#（c）" class="headerlink" title="（c）"></a>（c）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PartialParse</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, sentence</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initializes this partial parse.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        @param sentence (list of str): The sentence to be parsed as a list of words.</span><br><span class="hljs-string">                                        Your code should not modify the sentence.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># The sentence being parsed is kept for bookkeeping purposes. Do NOT alter it in your code.</span><br>        self.sentence = sentence<br><br>        <span class="hljs-comment">### YOUR CODE HERE (3 Lines)</span><br>        <span class="hljs-comment">### Your code should initialize the following fields:</span><br>        <span class="hljs-comment">###     self.stack: The current stack represented as a list with the top of the stack as the</span><br>        <span class="hljs-comment">###                 last element of the list.</span><br>        <span class="hljs-comment">###     self.buffer: The current buffer represented as a list with the first item on the</span><br>        <span class="hljs-comment">###                  buffer as the first item of the list</span><br>        <span class="hljs-comment">###     self.dependencies: The list of dependencies produced so far. Represented as a list of</span><br>        <span class="hljs-comment">###             tuples where each tuple is of the form (head, dependent).</span><br>        <span class="hljs-comment">###             Order for this list doesn&#x27;t matter.</span><br>        <span class="hljs-comment">###</span><br>        <span class="hljs-comment">### Note: The root token should be represented with the string &quot;ROOT&quot;</span><br>        <span class="hljs-comment">### Note: If you need to use the sentence object to initialize anything, make sure to not directly </span><br>        <span class="hljs-comment">###       reference the sentence object.  That is, remember to NOT modify the sentence object. </span><br><br>        self.stack = [<span class="hljs-string">&#x27;ROOT&#x27;</span>]<br>        self.buffer = sentence<br>        self.dependencies = []<br>        <span class="hljs-comment">### END YOUR CODE</span><br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_step</span>(<span class="hljs-params">self, transition</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Performs a single parse step by applying the given transition to this partial parse</span><br><span class="hljs-string"></span><br><span class="hljs-string">        @param transition (str): A string that equals &quot;S&quot;, &quot;LA&quot;, or &quot;RA&quot; representing the shift,</span><br><span class="hljs-string">                                left-arc, and right-arc transitions. You can assume the provided</span><br><span class="hljs-string">                                transition is a legal transition.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment">### YOUR CODE HERE (~7-12 Lines)</span><br>        <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span></span><br>        <span class="hljs-comment">###     Implement a single parsing step, i.e. the logic for the following as</span><br>        <span class="hljs-comment">###     described in the pdf handout:</span><br>        <span class="hljs-comment">###         1. Shift</span><br>        <span class="hljs-comment">###         2. Left Arc</span><br>        <span class="hljs-comment">###         3. Right Arc</span><br>        <span class="hljs-keyword">if</span> transition == <span class="hljs-string">&quot;S&quot;</span>:<br>            self.stack.append(self.buffer[<span class="hljs-number">0</span>])<br>            self.buffer = self.buffer[<span class="hljs-number">1</span>:]<br>        <span class="hljs-keyword">else</span>:<br>            r = self.stack[-<span class="hljs-number">1</span>]<br>            l = self.stack[-<span class="hljs-number">2</span>]<br>            <span class="hljs-keyword">if</span> transition == <span class="hljs-string">&quot;LA&quot;</span>:<br>                self.dependencies.append((r, l))<br>                self.stack = self.stack[:-<span class="hljs-number">2</span>]<br>                self.stack.append(r)<br>            <span class="hljs-keyword">else</span> :<br>                self.dependencies.append((l, r))<br>                self.stack = self.stack[:-<span class="hljs-number">1</span>]<br>        <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, transitions</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Applies the provided transitions to this PartialParse</span><br><span class="hljs-string"></span><br><span class="hljs-string">        @param transitions (list of str): The list of transitions in the order they should be applied</span><br><span class="hljs-string"></span><br><span class="hljs-string">        @return dependencies (list of string tuples): The list of dependencies produced when</span><br><span class="hljs-string">                                                        parsing the sentence. Represented as a list of</span><br><span class="hljs-string">                                                        tuples where each tuple is of the form (head, dependent).</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> transition <span class="hljs-keyword">in</span> transitions:<br>            self.parse_step(transition)<br>        <span class="hljs-keyword">return</span> self.dependencies<br></code></pre></td></tr></table></figure><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs txt">parser_transitions.py:203: SyntaxWarning: &quot;is&quot; with a literal. Did you mean &quot;==&quot;?<br>  return [(&quot;RA&quot; if pp.stack[1] is &quot;right&quot; else &quot;LA&quot;) if len(pp.buffer) == 0 else &quot;S&quot;<br>SHIFT test passed!<br>LEFT-ARC test passed!<br>RIGHT-ARC test passed!<br>parse test passed!<br></code></pre></td></tr></table></figure><h3 id="（d）"><a href="#（d）" class="headerlink" title="（d）"></a>（d）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">minibatch_parse</span>(<span class="hljs-params">sentences, model, batch_size</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Parses a list of sentences in minibatches using a model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    @param sentences (list of list of str): A list of sentences to be parsed</span><br><span class="hljs-string">                                            (each sentence is a list of words and each word is of type string)</span><br><span class="hljs-string">    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function</span><br><span class="hljs-string">                                model.predict(partial_parses) that takes in a list of PartialParses as input and</span><br><span class="hljs-string">                                returns a list of transitions predicted for each parse. That is, after calling</span><br><span class="hljs-string">                                    transitions = model.predict(partial_parses)</span><br><span class="hljs-string">                                transitions[i] will be the next transition to apply to partial_parses[i].</span><br><span class="hljs-string">    @param batch_size (int): The number of PartialParses to include in each minibatch</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">    @return dependencies (list of dependency lists): A list where each element is the dependencies</span><br><span class="hljs-string">                                                    list for a parsed sentence. Ordering should be the</span><br><span class="hljs-string">                                                    same as in sentences (i.e., dependencies[i] should</span><br><span class="hljs-string">                                                    contain the parse for sentences[i]).</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    dependencies = []<br><br>    <span class="hljs-comment">### YOUR CODE HERE (~8-10 Lines)</span><br>    <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span></span><br>    <span class="hljs-comment">###     Implement the minibatch parse algorithm.  Note that the pseudocode for this algorithm is given in the pdf handout.</span><br>    <span class="hljs-comment">###</span><br>    <span class="hljs-comment">###     Note: A shallow copy (as denoted in the PDF) can be made with the &quot;=&quot; sign in python, e.g.</span><br>    <span class="hljs-comment">###                 unfinished_parses = partial_parses[:].</span><br>    <span class="hljs-comment">###             Here `unfinished_parses` is a shallow copy of `partial_parses`.</span><br>    <span class="hljs-comment">###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances</span><br>    <span class="hljs-comment">###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.</span><br>    <span class="hljs-comment">###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`</span><br>    <span class="hljs-comment">###             contains references to the same objects. Thus, you should NOT use the `del` operator</span><br>    <span class="hljs-comment">###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that</span><br>    <span class="hljs-comment">###             is being accessed by `partial_parses` and may cause your code to crash.</span><br>    partial_parses = []<br>    <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> sentences:<br>        partial_parses.append(PartialParse(each))<br>    unfinished_parses = partial_parses[:]<br>    <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(unfinished_parses)&gt;<span class="hljs-number">0</span>:<br>        minibatch = unfinished_parses[:batch_size]<br>        <span class="hljs-comment"># print(&quot;minibatch&quot;,len(minibatch))</span><br>        transitions = model.predict(minibatch)<br>        <span class="hljs-comment"># print(&quot;transitions:&quot;,transitions)</span><br>        <span class="hljs-comment"># for idx in range(len(minibatch)):</span><br>        <span class="hljs-comment">#     p = minibatch[idx]</span><br>        <span class="hljs-comment">#     p.parse_step(transitions[idx])</span><br>        <span class="hljs-keyword">for</span> transition,unfinished_parse <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(transitions,minibatch):<br>            unfinished_parse.parse_step(transition)<br>        unfinished_parses = [<br>            m <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> unfinished_parses <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span>(<span class="hljs-built_in">len</span>(m.buffer)==<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(m.stack)==<span class="hljs-number">1</span>)<br>        ]<br>    <span class="hljs-comment">### END YOUR CODE</span><br>    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> partial_parses:<br>        dependencies.append(p.dependencies)<br>    <span class="hljs-keyword">return</span> dependencies<br><br></code></pre></td></tr></table></figure><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs txt">parser_transitions.py:203: SyntaxWarning: &quot;is&quot; with a literal. Did you mean &quot;==&quot;?<br>  return [(&quot;RA&quot; if pp.stack[1] is &quot;right&quot; else &quot;LA&quot;) if len(pp.buffer) == 0 else &quot;S&quot;<br>minibatch_parse test passed!<br></code></pre></td></tr></table></figure><h3 id="（e）"><a href="#（e）" class="headerlink" title="（e）"></a>（e）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ParserModel</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot; Feedforward neural network with an embedding layer and two hidden layers.</span><br><span class="hljs-string">    The ParserModel will predict which transition should be applied to a</span><br><span class="hljs-string">    given partial parse configuration.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    PyTorch Notes:</span><br><span class="hljs-string">        - Note that &quot;ParserModel&quot; is a subclass of the &quot;nn.Module&quot; class. In PyTorch all neural networks</span><br><span class="hljs-string">            are a subclass of this &quot;nn.Module&quot;.</span><br><span class="hljs-string">        - The &quot;__init__&quot; method is where you define all the layers and parameters</span><br><span class="hljs-string">            (embedding layers, linear layers, dropout layers, etc.).</span><br><span class="hljs-string">        - &quot;__init__&quot; gets automatically called when you create a new instance of your class, e.g.</span><br><span class="hljs-string">            when you write &quot;m = ParserModel()&quot;.</span><br><span class="hljs-string">        - Other methods of ParserModel can access variables that have &quot;self.&quot; prefix. Thus,</span><br><span class="hljs-string">            you should add the &quot;self.&quot; prefix layers, values, etc. that you want to utilize</span><br><span class="hljs-string">            in other ParserModel methods.</span><br><span class="hljs-string">        - For further documentation on &quot;nn.Module&quot; please see https://pytorch.org/docs/stable/nn.html.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, embeddings, n_features=<span class="hljs-number">36</span>,</span><br><span class="hljs-params">        hidden_size=<span class="hljs-number">200</span>, n_classes=<span class="hljs-number">3</span>, dropout_prob=<span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot; Initialize the parser model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        @param embeddings (ndarray): word embeddings (num_words, embedding_size)</span><br><span class="hljs-string">        @param n_features (int): number of input features</span><br><span class="hljs-string">        @param hidden_size (int): number of hidden units</span><br><span class="hljs-string">        @param n_classes (int): number of output classes</span><br><span class="hljs-string">        @param dropout_prob (float): dropout probability</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(ParserModel, self).__init__()<br>        self.n_features = n_features<br>        self.n_classes = n_classes<br>        self.dropout_prob = dropout_prob<br>        self.embed_size = embeddings.shape[<span class="hljs-number">1</span>]<br>        self.hidden_size = hidden_size<br>        self.embeddings = nn.Parameter(torch.tensor(embeddings))<br><br>        <span class="hljs-comment">### YOUR CODE HERE (~9-10 Lines)</span><br>        <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span></span><br>        <span class="hljs-comment">###     1) Declare `self.embed_to_hidden_weight` and `self.embed_to_hidden_bias` as `nn.Parameter`.</span><br>        <span class="hljs-comment">###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`</span><br>        <span class="hljs-comment">###        with default parameters.</span><br>        <span class="hljs-comment">###     2) Construct `self.dropout` layer.</span><br>        <span class="hljs-comment">###     3) Declare `self.hidden_to_logits_weight` and `self.hidden_to_logits_bias` as `nn.Parameter`.</span><br>        <span class="hljs-comment">###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`</span><br>        <span class="hljs-comment">###        with default parameters.</span><br>        <span class="hljs-comment">###</span><br>        <span class="hljs-comment">### Note: Trainable variables are declared as `nn.Parameter` which is a commonly used API</span><br>        <span class="hljs-comment">###       to include a tensor into a computational graph to support updating w.r.t its gradient.</span><br>        <span class="hljs-comment">###       Here, we use Xavier Uniform Initialization for our Weight initialization.</span><br>        <span class="hljs-comment">###       It has been shown empirically, that this provides better initial weights</span><br>        <span class="hljs-comment">###       for training networks than random uniform initialization.</span><br>        <span class="hljs-comment">###       For more details checkout this great blogpost:</span><br>        <span class="hljs-comment">###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization</span><br>        <span class="hljs-comment">###</span><br>        <span class="hljs-comment">### Please see the following docs for support:</span><br>        <span class="hljs-comment">###     nn.Parameter: https://pytorch.org/docs/stable/nn.html#parameters</span><br>        <span class="hljs-comment">###     Initialization: https://pytorch.org/docs/stable/nn.init.html</span><br>        <span class="hljs-comment">###     Dropout: https://pytorch.org/docs/stable/nn.html#dropout-layers</span><br>        <span class="hljs-comment">### </span><br>        <span class="hljs-comment">### See the PDF for hints.</span><br>        <span class="hljs-comment">#输入维度为特征数*嵌入size</span><br>        <span class="hljs-comment">#weight:从输入维度到权重维度</span><br>        self.embed_to_hidden_weight = nn.Parameter(torch.empty(self.embed_size*self.n_features,self.hidden_size))<br>        <span class="hljs-comment">#也可以</span><br>        <span class="hljs-comment">#self.embed_to_hidden_weight == nn.Parameter(nn.init.xavier_uniform_(....))</span><br>        nn.init.xavier_uniform_(self.embed_to_hidden_weight)<br>        <span class="hljs-comment">#和权重维度保持一致</span><br>        self.embed_to_hidden_bias = nn.Parameter(torch.empty(self.hidden_size))<br>        nn.init.uniform_(self.embed_to_hidden_bias)<br>        <span class="hljs-comment">#构建一个Dropout层</span><br>        self.dropout = nn.Dropout(p=dropout_prob)<br>        self.hidden_to_logits_weight = nn.Parameter(torch.empty(self.hidden_size,self.n_classes))<br>        nn.init.xavier_uniform_(self.hidden_to_logits_weight)<br>        self.hidden_to_logits_bias = nn.Parameter(torch.empty(self.n_classes))<br>        nn.init.uniform_(self.hidden_to_logits_bias)<br>        <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">embedding_lookup</span>(<span class="hljs-params">self, w</span>):<br>        <span class="hljs-string">&quot;&quot;&quot; Utilize `w` to select embeddings from embedding matrix `self.embeddings`</span><br><span class="hljs-string">            @param w (Tensor): input tensor of word indices (batch_size, n_features)</span><br><span class="hljs-string"></span><br><span class="hljs-string">            @return x (Tensor): tensor of embeddings for words represented in w</span><br><span class="hljs-string">                                (batch_size, n_features * embed_size)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment">### YOUR CODE HERE (~1-4 Lines)</span><br>        <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span></span><br>        <span class="hljs-comment">###     1) For each index `i` in `w`, select `i`th vector from self.embeddings</span><br>        <span class="hljs-comment">###     2) Reshape the tensor using `view` function if necessary</span><br>        <span class="hljs-comment">###</span><br>        <span class="hljs-comment">### Note: All embedding vectors are stacked and stored as a matrix. The model receives</span><br>        <span class="hljs-comment">###       a list of indices representing a sequence of words, then it calls this lookup</span><br>        <span class="hljs-comment">###       function to map indices to sequence of embeddings.</span><br>        <span class="hljs-comment">###</span><br>        <span class="hljs-comment">###       This problem aims to test your understanding of embedding lookup,</span><br>        <span class="hljs-comment">###       so DO NOT use any high level API like nn.Embedding</span><br>        <span class="hljs-comment">###       (we are asking you to implement that!). Pay attention to tensor shapes</span><br>        <span class="hljs-comment">###       and reshape if necessary. Make sure you know each tensor&#x27;s shape before you run the code!</span><br>        <span class="hljs-comment">###</span><br>        <span class="hljs-comment">### Pytorch has some useful APIs for you, and you can use either one</span><br>        <span class="hljs-comment">### in this problem (except nn.Embedding). These docs might be helpful:</span><br>        <span class="hljs-comment">###     Index select: https://pytorch.org/docs/stable/torch.html#torch.index_select</span><br>        <span class="hljs-comment">###     Gather: https://pytorch.org/docs/stable/torch.html#torch.gather</span><br>        <span class="hljs-comment">###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view</span><br>        <span class="hljs-comment">###     Flatten: https://pytorch.org/docs/stable/generated/torch.flatten.html</span><br>        <span class="hljs-comment"># index_select函数:选择其中的部分</span><br>        x = []<br>        <span class="hljs-comment"># print(w.shape)</span><br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> w:<br>            <span class="hljs-comment"># dim=0选择行，batch用于lookup</span><br>            em = torch.index_select(self.embeddings,dim=<span class="hljs-number">0</span>,index=batch)<br>            <span class="hljs-comment"># 选择完之后的size就是n_features*embedding_size</span><br>            x.append(em)<br>        <span class="hljs-comment"># 要转换的list里面的元素包含多维的tensor</span><br>        x = torch.tensor([item.detach().numpy() <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> x]).view(w.shape[<span class="hljs-number">0</span>],-<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 一行写完</span><br>        <span class="hljs-comment"># x = torch.tensor([torch.index_select(self.embeddings,dim=0,index=batch).detach().numpy() for batch in w])</span><br>        <span class="hljs-comment">### END YOUR CODE</span><br>        <span class="hljs-keyword">return</span> x<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, w</span>):<br>        <span class="hljs-string">&quot;&quot;&quot; Run the model forward.</span><br><span class="hljs-string"></span><br><span class="hljs-string">            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss</span><br><span class="hljs-string"></span><br><span class="hljs-string">            PyTorch Notes:</span><br><span class="hljs-string">                - Every nn.Module object (PyTorch model) has a `forward` function.</span><br><span class="hljs-string">                - When you apply your nn.Module to an input tensor `w` this function is applied to the tensor.</span><br><span class="hljs-string">                    For example, if you created an instance of your ParserModel and applied it to some `w` as follows,</span><br><span class="hljs-string">                    the `forward` function would called on `w` and the result would be stored in the `output` variable:</span><br><span class="hljs-string">                        model = ParserModel()</span><br><span class="hljs-string">                        output = model(w) # this calls the forward function</span><br><span class="hljs-string">                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward</span><br><span class="hljs-string"></span><br><span class="hljs-string">        @param w (Tensor): input tensor of tokens (batch_size, n_features)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)</span><br><span class="hljs-string">                                 without applying softmax (batch_size, n_classes)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment">### YOUR CODE HERE (~3-5 lines)</span><br>        <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span></span><br>        <span class="hljs-comment">###     Complete the forward computation as described in write-up. In addition, include a dropout layer</span><br>        <span class="hljs-comment">###     as decleared in `__init__` after ReLU function.</span><br>        <span class="hljs-comment">###</span><br>        <span class="hljs-comment">### Note: We do not apply the softmax to the logits here, because</span><br>        <span class="hljs-comment">### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.</span><br>        <span class="hljs-comment">###</span><br>        <span class="hljs-comment">### Please see the following docs for support:</span><br>        <span class="hljs-comment">###     Matrix product: https://pytorch.org/docs/stable/torch.html#torch.matmul</span><br>        <span class="hljs-comment">###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu</span><br>        <span class="hljs-comment"># forward：前向传播</span><br>        <span class="hljs-comment"># 执行embedding_lookup作为输入x</span><br>        emb = self.embedding_lookup(w)<br>        emb = self.dropout(emb)<br>        <span class="hljs-comment"># @:矩阵乘法，这里可以简化操作，省去detach().numpy()</span><br>        hidden_state = torch.nn.functional.relu(emb @ self.embed_to_hidden_weight+self.embed_to_hidden_bias)<br>        logits = hidden_state @ self.hidden_to_logits_weight+self.hidden_to_logits_bias<br>        <span class="hljs-comment">### END YOUR CODE</span><br>        <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">parser, train_data, dev_data, output_path, batch_size=<span class="hljs-number">1024</span>, n_epochs=<span class="hljs-number">10</span>, lr=<span class="hljs-number">0.0005</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Train the neural dependency parser.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    @param parser (Parser): Neural Dependency Parser</span><br><span class="hljs-string">    @param train_data ():</span><br><span class="hljs-string">    @param dev_data ():</span><br><span class="hljs-string">    @param output_path (str): Path to which model weights and results are written.</span><br><span class="hljs-string">    @param batch_size (int): Number of examples in a single batch</span><br><span class="hljs-string">    @param n_epochs (int): Number of training epochs</span><br><span class="hljs-string">    @param lr (float): Learning rate</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    best_dev_UAS = <span class="hljs-number">0</span><br><br><br>    <span class="hljs-comment">### YOUR CODE HERE (~2-7 lines)</span><br>    <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span></span><br>    <span class="hljs-comment">###      1) Construct Adam Optimizer in variable `optimizer`</span><br>    <span class="hljs-comment">###      2) Construct the Cross Entropy Loss Function in variable `loss_func` with `mean`</span><br>    <span class="hljs-comment">###         reduction (default)</span><br>    <span class="hljs-comment">###</span><br>    <span class="hljs-comment">### Hint: Use `parser.model.parameters()` to pass optimizer</span><br>    <span class="hljs-comment">###       necessary parameters to tune.</span><br>    <span class="hljs-comment">### Please see the following docs for support:</span><br>    <span class="hljs-comment">###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html</span><br>    <span class="hljs-comment">###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss</span><br>    optimizer = torch.optim.Adam(parser.model.parameters(),lr=lr)<br>    loss_func = nn.CrossEntropyLoss()<br><br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Epoch &#123;:&#125; out of &#123;:&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch + <span class="hljs-number">1</span>, n_epochs))<br>        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)<br>        <span class="hljs-keyword">if</span> dev_UAS &gt; best_dev_UAS:<br>            best_dev_UAS = dev_UAS<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;New best dev UAS! Saving model.&quot;</span>)<br>            torch.save(parser.model.state_dict(), output_path)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&quot;</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_for_epoch</span>(<span class="hljs-params">parser, train_data, dev_data, optimizer, loss_func, batch_size</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Train the neural dependency parser for single epoch.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Note: In PyTorch we can signify train versus test and automatically have</span><br><span class="hljs-string">    the Dropout Layer applied and removed, accordingly, by specifying</span><br><span class="hljs-string">    whether we are training, `model.train()`, or evaluating, `model.eval()`</span><br><span class="hljs-string"></span><br><span class="hljs-string">    @param parser (Parser): Neural Dependency Parser</span><br><span class="hljs-string">    @param train_data ():</span><br><span class="hljs-string">    @param dev_data ():</span><br><span class="hljs-string">    @param optimizer (nn.Optimizer): Adam Optimizer</span><br><span class="hljs-string">    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function</span><br><span class="hljs-string">    @param batch_size (int): batch size</span><br><span class="hljs-string"></span><br><span class="hljs-string">    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    parser.model.train() <span class="hljs-comment"># Places model in &quot;train&quot; mode, i.e. apply dropout layer</span><br>    n_minibatches = math.ceil(<span class="hljs-built_in">len</span>(train_data) / batch_size)<br>    loss_meter = AverageMeter()<br><br>    <span class="hljs-keyword">with</span> tqdm(total=(n_minibatches)) <span class="hljs-keyword">as</span> prog:<br>        <span class="hljs-keyword">for</span> i, (train_x, train_y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(minibatches(train_data, batch_size)):<br>            optimizer.zero_grad()   <span class="hljs-comment"># remove any baggage in the optimizer</span><br>            loss = <span class="hljs-number">0.</span> <span class="hljs-comment"># store loss for this batch here</span><br>            train_x = torch.from_numpy(train_x).long()<br>            train_y = torch.from_numpy(train_y.nonzero()[<span class="hljs-number">1</span>]).long()<br><br>            <span class="hljs-comment">### YOUR CODE HERE (~4-10 lines)</span><br>            <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span></span><br>            <span class="hljs-comment">###      1) Run train_x forward through model to produce `logits`</span><br>            <span class="hljs-comment">###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.</span><br>            <span class="hljs-comment">###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss</span><br>            <span class="hljs-comment">###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)</span><br>            <span class="hljs-comment">###         are the predictions (y^ from the PDF).</span><br>            <span class="hljs-comment">###      3) Backprop losses</span><br>            <span class="hljs-comment">###      4) Take step with the optimizer</span><br>            <span class="hljs-comment">### Please see the following docs for support:</span><br>            <span class="hljs-comment">###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step</span><br>            logits = parser.model.forward(train_x)<br>            <span class="hljs-comment"># nn.CrossEntropyLoss的使用</span><br>            loss = loss_func(logits,train_y)<br>            loss.backward()<br>            optimizer.step()<br><br>            <span class="hljs-comment">### END YOUR CODE</span><br>            prog.update(<span class="hljs-number">1</span>)<br>            loss_meter.update(loss.item())<br><br>    <span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;Average Train Loss: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(loss_meter.avg))<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Evaluating on dev set&quot;</span>,)<br>    parser.model.<span class="hljs-built_in">eval</span>() <span class="hljs-comment"># Places model in &quot;eval&quot; mode, i.e. don&#x27;t apply dropout layer</span><br>    dev_UAS, _ = parser.parse(dev_data)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;- dev UAS: &#123;:.2f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(dev_UAS * <span class="hljs-number">100.0</span>))<br>    <span class="hljs-keyword">return</span> dev_UAS<br><br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><code class="hljs shell">================================================================================<br>INITIALIZING<br>================================================================================<br>Loading data...<br>took 2.34 seconds<br>Building parser...<br>took 1.25 seconds<br>Loading pretrained embeddings...<br>took 3.19 seconds<br>Vectorizing data...<br>took 1.98 seconds<br>Preprocessing training data...<br>took 51.43 seconds<br>took 0.03 seconds<br><br>================================================================================<br>TRAINING<br>================================================================================<br>Epoch 1 out of 10<br><span class="hljs-meta prompt_">  0%</span><span class="language-bash">|                                                                                                                                                                                                                                                          | 0/1848 [00:00&lt;?, ?it/s]D</span><br>:\软件\安装目录\pycharm\projects\a3\parser_model.py:128: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\acti<br>ons-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\utils\tensor_new.cpp:233.)<br>  x = torch.tensor([item.detach().numpy() for item in x]).view(w.shape[0],-1)<br><span class="hljs-meta prompt_">100%</span><span class="language-bash">|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1848/1848 [08:49&lt;00:00,  3.49it/s]</span><br>Average Train Loss: 0.26055007827069077<br>Evaluating on dev set<br>1445850it [00:00, 35055374.72it/s]<br>- dev UAS: 79.67<br>New best dev UAS! Saving model.<br><br>Epoch 2 out of 10<br><span class="hljs-meta prompt_">100%</span><span class="language-bash">|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1848/1848 [09:00&lt;00:00,  3.42it/s]</span><br>Average Train Loss: 0.16393936111378077<br>Evaluating on dev set<br>1445850it [00:00, 46227698.79it/s]<br>- dev UAS: 82.79<br>New best dev UAS! Saving model.<br><br>Epoch 3 out of 10<br><span class="hljs-meta prompt_">100%</span><span class="language-bash">|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1848/1848 [09:05&lt;00:00,  3.38it/s]</span><br>Average Train Loss: 0.14417635859897385<br>Evaluating on dev set<br>1445850it [00:00, 30844015.60it/s]<br>- dev UAS: 83.73<br>New best dev UAS! Saving model.<br><br>Epoch 4 out of 10<br><span class="hljs-meta prompt_">100%</span><span class="language-bash">|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1848/1848 [08:45&lt;00:00,  3.51it/s]</span><br>Average Train Loss: 0.13362910604967185<br>Evaluating on dev set<br>1445850it [00:00, 28795236.69it/s]<br>- dev UAS: 84.39<br>New best dev UAS! Saving model.<br><br>Epoch 5 out of 10<br><span class="hljs-meta prompt_">100%</span><span class="language-bash">|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1848/1848 [09:00&lt;00:00,  3.42it/s]</span><br>Average Train Loss: 0.12646352259434146<br>Evaluating on dev set<br>1445850it [00:00, 36683288.00it/s]<br>- dev UAS: 85.27<br>New best dev UAS! Saving model.<br><br>Epoch 6 out of 10<br><span class="hljs-meta prompt_">100%</span><span class="language-bash">|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1848/1848 [08:46&lt;00:00,  3.51it/s]</span><br>Average Train Loss: 0.12200444055825987<br>Evaluating on dev set<br>1445850it [00:00, 32126541.28it/s]<br>- dev UAS: 85.61<br>New best dev UAS! Saving model.<br><br>Epoch 7 out of 10<br><span class="hljs-meta prompt_">100%</span><span class="language-bash">|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1848/1848 [08:46&lt;00:00,  3.51it/s]</span><br>Average Train Loss: 0.11807776755729923<br>Evaluating on dev set<br>1445850it [00:00, 35243415.11it/s]<br>- dev UAS: 86.00<br>New best dev UAS! Saving model.<br><br>Epoch 8 out of 10<br><span class="hljs-meta prompt_">100%</span><span class="language-bash">|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1848/1848 [08:50&lt;00:00,  3.48it/s]</span><br>Average Train Loss: 0.11562635477841933<br>Evaluating on dev set<br>1445850it [00:00, 32848902.51it/s]<br>- dev UAS: 85.93<br><br>Epoch 9 out of 10<br><span class="hljs-meta prompt_">100%</span><span class="language-bash">|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1848/1848 [08:41&lt;00:00,  3.54it/s]</span><br>Average Train Loss: 0.1125802615318786<br>Evaluating on dev set<br>1445850it [00:00, 38039985.19it/s]<br>- dev UAS: 86.69<br>New best dev UAS! Saving model.<br><br>Epoch 10 out of 10<br><span class="hljs-meta prompt_">100%</span><span class="language-bash">|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1848/1848 [08:17&lt;00:00,  3.72it/s]</span><br>Average Train Loss: 0.11088591745099077<br>Evaluating on dev set<br>1445850it [00:00, 38262233.51it/s]<br>- dev UAS: 86.71<br>New best dev UAS! Saving model.<br><br>================================================================================<br>TESTING<br>================================================================================<br>Restoring the best model weights found on the dev set<br>Final evaluation on test set<br>2919736it [00:00, 68380481.23it/s]<br>- test UAS: 86.79<br>Done!<br><br><br>dev UAS:86.71<br>test UAS:86.79<br></code></pre></td></tr></table></figure><h3 id="（f）"><a href="#（f）" class="headerlink" title="（f）"></a>（f）</h3><h4 id="（i）-2"><a href="#（i）-2" class="headerlink" title="（i）"></a>（i）</h4><ul><li><strong>Error type：</strong>Prepositional Phrase Attachment Error</li><li><strong>Incorrect dependency：</strong>concerns-&gt;risks</li><li><strong>Correct dependency：</strong>citing-&gt;risks</li></ul><h4 id="（ii）-2"><a href="#（ii）-2" class="headerlink" title="（ii）"></a>（ii）</h4><ul><li><strong>Error type：</strong>Modifier Attachment Error</li><li><strong>Incorrect dependency：</strong>left-&gt;early</li><li><strong>Correct dependency：</strong>afternoon-&gt;early</li></ul><h4 id="（iii）"><a href="#（iii）" class="headerlink" title="（iii）"></a>（iii）</h4><ul><li><strong>Error type：</strong>Verb Phrase Attachment Error</li><li><strong>Incorrect dependency：</strong>declined-&gt;decision</li><li><strong>Correct dependency：</strong>comment-&gt;decision</li></ul><h4 id="（iv）"><a href="#（iv）" class="headerlink" title="（iv）"></a>（iv）</h4><ul><li><strong>Error type：</strong>Coordination Attachment Error</li><li><strong>Incorrect dependency：</strong>affects-&gt;one</li><li><strong>Correct dependency：</strong>plants-&gt;one</li></ul><h3 id="（g）"><a href="#（g）" class="headerlink" title="（g）"></a>（g）</h3><p>提高模型对词性的辨析能力，对基本的短语搭配有一定的学习能力。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N作业A2：词向量训练</title>
      <link href="/post/38d31aac.html"/>
      <url>/post/38d31aac.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="row">    <embed src="../download/a2.pdf" width="100%" height="550" type="application/pdf"></div><h1 id="手写题目-答案"><a href="#手写题目-答案" class="headerlink" title="手写题目 答案"></a>手写题目 答案</h1><p>注：本答案只包括公式推导的答案，部分主观题答案不包含（因为没做（笑</p><h3 id="a"><a href="#a" class="headerlink" title="(a)"></a>(a)</h3><p>对任意一个单词$w$，有<br>$$<br>\begin{cases}<br>1\ \ \ \ \ \ w&#x3D;o \\<br>0\ \ \ \ \ \ w\ne o<br>\end{cases}<br>$$</p><p>$$<br>\therefore -\sum_{w\in V}y_w\log(\hat{y_w}) &#x3D; - 0 \cdot \log(\hat{y_w}) - 1\cdot \log(\hat{y_o}) &#x3D; -\log(\hat{y_o})$​<br>$$</p><h3 id="b"><a href="#b" class="headerlink" title="(b)"></a>(b)</h3><h4 id="i"><a href="#i" class="headerlink" title="(i)"></a>(i)</h4><p>$$<br>\begin{align}<br>\frac{\partial J}{\partial v_c} &amp; &#x3D; \frac{\partial }{\partial v_c} \left ( -\log(\exp(u_o^\top v_c))+\log(\sum_{w\in V}\exp(u_w^\top v_c)) \right ) \\<br>&amp; &#x3D; -u_o+\sum_{w\in V}{ {\exp (u_w^\top v_c)} \over {\sum_{j\in V}\exp(u_j^\top v_c) }  }\cdot u_w &#x3D; -u_o + \sum_{w\in V}P(w|c)\cdot u_w \\<br>&amp; &#x3D; U^\top ( {y-\hat{y}} )<br>\end{align}<br>$$</p><h3 id="c"><a href="#c" class="headerlink" title="(c)"></a>(c)</h3><p>$$<br>\begin{align}<br>\frac{\partial J}{\partial u_w} &amp; &#x3D; \frac{\partial }{\partial u_w} \left( -\log \left ( {  {\exp(u_o^\top v_c) } \over {\sum_{w\in V}\exp(u_w^\top v_c) } } \right ) \right ) \\<br>&amp; &#x3D; - { {\partial u_o^\top v_c} \over {\partial u_w} } + \sum_{w\in V} { {\exp (u_w^\top v_c)} \over {\sum_{j\in V}\exp(u_j^\top v_c) } } \cdot v_c \\<br>&amp; &#x3D; - { {\partial u_o^\top v_c} \over {\partial u_w} } + \sum_{w\in V}P(w|c)\cdot v_c \\<br>①\ w&#x3D;o\ 时 \ \ {\partial J \over \partial u_o } &amp;&#x3D; -v_c+\hat{y_w}v_c &#x3D; (y-\hat{y_w})v_c\\<br>②\ w\ne o\ 时\ \ {\partial J \over \partial u_o } &amp;&#x3D;\hat{y_w}v_c<br>\end{align}<br>$$</p><h3 id="d"><a href="#d" class="headerlink" title="(d)"></a>(d)</h3><p>$$<br>{\partial J\over \partial U} &#x3D; \left [  { \partial J\over\partial U_1} , { \partial J\over\partial U_2} , …,{\partial J\over\partial U_{|V|} } \right]<br>$$</p><h3 id="e"><a href="#e" class="headerlink" title="(e)"></a>(e)</h3><p>$$<br>\frac{\partial f(x) }{\partial x} &#x3D;<br>\begin{cases}<br>\begin{matrix}<br> &amp; a &amp; x&lt;0 \\<br> &amp; 1 &amp; otherwise<br>\end{matrix}<br>\end{cases}<br>$$</p><h3 id="f"><a href="#f" class="headerlink" title="(f)"></a>(f)</h3><p>$$<br>\frac{\partial \sigma(x)}{\partial x} &#x3D; \frac{e^x (e^x+1)- e^x\cdot e^x}{(e^x+1)^2} &#x3D; \sigma(x)(1-\sigma(x))<br>$$</p><h3 id="g"><a href="#g" class="headerlink" title="(g)"></a>(g)</h3><h4 id="i-1"><a href="#i-1" class="headerlink" title="(i)"></a>(i)</h4><p>$$<br>\begin{align}<br>\frac{\partial J}{\partial v_c} &amp; &#x3D; \frac{\partial }{\partial v_c} \left( -\log\left(\sigma(u_o^\top v_c) - \sum_{s&#x3D;1}^K \log(\sigma(-u_{w_s}^\top v_c)) \right) \right) \\<br>&amp; &#x3D; -(1 - \sigma(u_o^\top v_c)) ·u_o - \sum_{s&#x3D;1}^K (1-\sigma(-u_{w_s}^\top v_c))(-u_{w_s})\\<br>\frac{\partial J}{\partial u_o} &amp;&#x3D; -(1-\sigma(u_o^\top v_c)) ·v_c \\<br>\frac{\partial J}{\partial u_k} &amp;&#x3D; -(1-\sigma(-u_k^\top v_c)) ·(-v_c)<br>\end{align}<br>$$</p><h4 id="ii"><a href="#ii" class="headerlink" title="(ii)"></a>(ii)</h4><p>$$<br>U_{o,[w_1,…,w_k]}^\top v_c &#x3D; \begin{bmatrix}<br> u_o^\top v_c\\<br> -u_{w_1}^\top v_c\\<br>… \\<br> -u_{w_k}^\top v_c\<br>\end{bmatrix},\ \ \ \<br>\therefore<br>\sigma(U^\top v_c)-1 &#x3D; \begin{bmatrix}<br> \sigma(u_o^\top v_c)-1\\<br> \sigma(-u_{w_1}^\top v_c)-1\\<br>… \\<br> \sigma(-u_{w_k}^\top v_c)-1\<br>\end{bmatrix}<br>$$</p><h4 id="iii"><a href="#iii" class="headerlink" title="(iii)"></a>(iii)</h4><p>因为使用负对数损失，其没有分母，便于计算，且不用迭代所有词汇</p><h3 id="h"><a href="#h" class="headerlink" title="(h)"></a>(h)</h3><p>$$<br>\begin{align}<br>{ \partial J \over \partial {u_{w_s} } } &amp;&#x3D;  { \partial \over \partial {u_{w_s} } }\left( -\sum_{j&#x3D;1}^K \log \sigma(-u_{w_j}^\top v_c) \right) \\<br>&amp; &#x3D; - \sum_{j&#x3D;1}^K \left(1-\sigma(-u_{w_j}^\top v_c) (-v_c) \right) \\<br>&amp; &#x3D; \sum_{j&#x3D;1}^K \sigma(u_{w_j}^\top v_c) ·v_c &#x3D; \sum_{w_j&#x3D;w_s} \sigma(u_{w_s}^\top v_c)·v_c<br>\end{align}<br>$$</p><h3 id="i-2"><a href="#i-2" class="headerlink" title="(i)"></a>(i)</h3><h4 id="i-iii"><a href="#i-iii" class="headerlink" title="(i-iii)"></a>(i-iii)</h4><p>$$<br>\begin{align}<br>{ \partial J_{skip-gram (v_c,w_{t-m},…,w_{t+m},U )} \over \partial U } &amp; &#x3D; \sum_{j\in \lbrace -m,m \rbrace \wedge j\notin \lbrace 0 \rbrace} { \partial J(v_c,w_{t+j},U ) \over \partial U } \\<br>{ \partial J_{skip-gram (v_c,w_{t-m},…,w_{t+m},U )} \over \partial v_c } &amp;&#x3D; \sum_{j\in \lbrace -m,m \rbrace \wedge j\notin \lbrace 0 \rbrace} { \partial J(v_c,w_{t+j},U ) \over \partial v_c } \\<br>{ \partial J_{skip-gram (v_c,w_{t-m},…,w_{t+m},U )} \over \partial v_w } &amp; &#x3D; 0<br>\end{align}<br>$$</p><h1 id="编程答案"><a href="#编程答案" class="headerlink" title="编程答案"></a>编程答案</h1><h3 id="a-1"><a href="#a-1" class="headerlink" title="(a)"></a>(a)</h3><h4 id="i-3"><a href="#i-3" class="headerlink" title="(i)"></a>(i)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Compute the sigmoid function for the input here.</span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">    x -- A scalar or numpy array.</span><br><span class="hljs-string">    Return:</span><br><span class="hljs-string">    s -- sigmoid(x)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment">### YOUR CODE HERE (~1 Line)</span><br>    s = <span class="hljs-number">1.</span> / (<span class="hljs-number">1.</span> + np.exp(-x))<br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">return</span> s<br></code></pre></td></tr></table></figure><p><img src="/post/38d31aac/image-20230327162935340.png" alt="image-20230327162935340"></p><h4 id="ii-1"><a href="#ii-1" class="headerlink" title="(ii)"></a>(ii)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">naiveSoftmaxLossAndGradient</span>(<span class="hljs-params"></span><br><span class="hljs-params">    centerWordVec,</span><br><span class="hljs-params">    outsideWordIdx,</span><br><span class="hljs-params">    outsideVectors,</span><br><span class="hljs-params">    dataset</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Naive Softmax loss &amp; gradient function for word2vec models</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Implement the naive softmax loss and gradients between a center word&#x27;s </span><br><span class="hljs-string">    embedding and an outside word&#x27;s embedding. This will be the building block</span><br><span class="hljs-string">    for our word2vec models. For those unfamiliar with numpy notation, note </span><br><span class="hljs-string">    that a numpy ndarray with a shape of (x, ) is a one-dimensional array, which</span><br><span class="hljs-string">    you can effectively treat as a vector with length x.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">    centerWordVec -- numpy ndarray, center word&#x27;s embedding</span><br><span class="hljs-string">                    in shape (word vector length, )</span><br><span class="hljs-string">                    (v_c in the pdf handout)</span><br><span class="hljs-string">    outsideWordIdx -- integer, the index of the outside word</span><br><span class="hljs-string">                    (o of u_o in the pdf handout)</span><br><span class="hljs-string">    outsideVectors -- outside vectors is</span><br><span class="hljs-string">                    in shape (num words in vocab, word vector length) </span><br><span class="hljs-string">                    for all words in vocab (tranpose of U in the pdf handout)</span><br><span class="hljs-string">    dataset -- needed for negative sampling, unused here.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Return:</span><br><span class="hljs-string">    loss -- naive softmax loss</span><br><span class="hljs-string">    gradCenterVec -- the gradient with respect to the center word vector</span><br><span class="hljs-string">                     in shape (word vector length, )</span><br><span class="hljs-string">                     (dJ / dv_c in the pdf handout)</span><br><span class="hljs-string">    gradOutsideVecs -- the gradient with respect to all the outside word vectors</span><br><span class="hljs-string">                    in shape (num words in vocab, word vector length) </span><br><span class="hljs-string">                    (dJ / dU)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment">### YOUR CODE HERE (~6-8 Lines)</span><br>    U,u_o,v_c,o = outsideVectors,outsideVectors[outsideWordIdx],centerWordVec,outsideWordIdx<br>    <span class="hljs-comment">#整体的softmax</span><br>    y_hat = softmax(np.dot(U,v_c))<br>    y = np.zeros(np.shape(y_hat))<br>    y[o]=<span class="hljs-number">1</span><br>    <span class="hljs-comment">#单独的o的损失</span><br>    loss = -np.log(y_hat[o])<br>    gradCenterVec=np.dot(U.T,(y_hat-y))<br>    gradOutsideVecs = np.dot((y_hat-y)[:,np.newaxis],v_c[:,np.newaxis].T)<br>    <span class="hljs-comment"># print(np.shape(gradCenterVec),np.shape(gradOutsideVecs))</span><br>    <span class="hljs-comment">### Please use the provided softmax function (imported earlier in this file)</span><br>    <span class="hljs-comment">### This numerically stable implementation helps you avoid issues pertaining</span><br>    <span class="hljs-comment">### to integer overflow. </span><br><br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">return</span> loss, gradCenterVec, gradOutsideVecs<br></code></pre></td></tr></table></figure><p><img src="/post/38d31aac/image-20230327163002002.png" alt="image-20230327163002002"></p><h4 id="iii-1"><a href="#iii-1" class="headerlink" title="(iii)"></a>(iii)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">negSamplingLossAndGradient</span>(<span class="hljs-params"></span><br><span class="hljs-params">    centerWordVec,</span><br><span class="hljs-params">    outsideWordIdx,</span><br><span class="hljs-params">    outsideVectors,</span><br><span class="hljs-params">    dataset,</span><br><span class="hljs-params">    K=<span class="hljs-number">10</span></span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Negative sampling loss function for word2vec models</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Implement the negative sampling loss and gradients for a centerWordVec</span><br><span class="hljs-string">    and a outsideWordIdx word vector as a building block for word2vec</span><br><span class="hljs-string">    models. K is the number of negative samples to take.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Note: The same word may be negatively sampled multiple times. For</span><br><span class="hljs-string">    example if an outside word is sampled twice, you shall have to</span><br><span class="hljs-string">    double count the gradient with respect to this word. Thrice if</span><br><span class="hljs-string">    it was sampled three times, and so forth.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># Negative sampling of words is done for you. Do not modify this if you</span><br>    <span class="hljs-comment"># wish to match the autograder and receive points!</span><br>    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)<br>    indices = [outsideWordIdx] + negSampleWordIndices<br>    <span class="hljs-string">&#x27;&#x27;&#x27;    </span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">    centerWordVec -- numpy ndarray, center word&#x27;s embedding</span><br><span class="hljs-string">                    in shape (word vector length, )</span><br><span class="hljs-string">                    (v_c in the pdf handout)</span><br><span class="hljs-string">    outsideWordIdx -- integer, the index of the outside word</span><br><span class="hljs-string">                    (o of u_o in the pdf handout)</span><br><span class="hljs-string">    outsideVectors -- outside vectors is</span><br><span class="hljs-string">                    in shape (num words in vocab, word vector length) </span><br><span class="hljs-string">                    for all words in vocab (tranpose of U in the pdf handout)</span><br><span class="hljs-string">    dataset -- needed for negative sampling, unused here.</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment">### YOUR CODE HERE (~10 Lines)</span><br>    <span class="hljs-comment">### Please use your implementation of sigmoid in here.</span><br>    U,u_o,v_c,o,s= outsideVectors,outsideVectors[outsideWordIdx],centerWordVec,outsideWordIdx,negSampleWordIndices<br>    gradCenterVec = np.zeros(v_c.shape)<br>    gradOutsideVecs = np.zeros(U.shape)<br>    u_negsam = U[s]<br>    center_o = np.dot(u_o, v_c)<br>    center_s = -np.dot(u_negsam,v_c)<br>    <span class="hljs-comment"># U_sampled = np.concatenate((u_o[np.newaxis,:],-u_negsam),axis=0)</span><br>    loss = -np.log(sigmoid(center_o))-np.<span class="hljs-built_in">sum</span>(np.log(sigmoid(center_s)))<br>    gradCenterVec += np.dot(u_o,sigmoid(center_o)-<span class="hljs-number">1</span>)<br>    gradOutsideVecs[o] = np.dot(sigmoid(center_o)-<span class="hljs-number">1</span>,v_c)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> s:<br>        u_k = U[i]<br>        gradCenterVec += np.dot(u_k, <span class="hljs-number">1</span>-sigmoid(-np.dot(u_k,v_c)))<br>        gradOutsideVecs[i] += np.dot(<span class="hljs-number">1</span>-sigmoid(-np.dot(u_k,v_c)),v_c)<br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">return</span> loss, gradCenterVec, gradOutsideVecs<br></code></pre></td></tr></table></figure><p><img src="/post/38d31aac/image-20230327163033096.png" alt="image-20230327163033096"></p><h4 id="（iv"><a href="#（iv" class="headerlink" title="（iv)"></a>（iv)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">skipgram</span>(<span class="hljs-params">currentCenterWord, windowSize, outsideWords, word2Ind,</span><br><span class="hljs-params">             centerWordVectors, outsideVectors, dataset,</span><br><span class="hljs-params">             word2vecLossAndGradient=naiveSoftmaxLossAndGradient</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Skip-gram model in word2vec</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Implement the skip-gram model in this function.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">    currentCenterWord -- a string of the current center word</span><br><span class="hljs-string">    windowSize -- integer, context window size</span><br><span class="hljs-string">    outsideWords -- list of no more than 2*windowSize strings, the outside words</span><br><span class="hljs-string">    word2Ind -- a dictionary that maps words to their indices in</span><br><span class="hljs-string">              the word vector list</span><br><span class="hljs-string">    centerWordVectors -- center word vectors (as rows) is in shape </span><br><span class="hljs-string">                        (num words in vocab, word vector length) </span><br><span class="hljs-string">                        for all words in vocab (V in pdf handout)</span><br><span class="hljs-string">    outsideVectors -- outside vectors is in shape </span><br><span class="hljs-string">                        (num words in vocab, word vector length) </span><br><span class="hljs-string">                        for all words in vocab (transpose of U in the pdf handout)</span><br><span class="hljs-string">    word2vecLossAndGradient -- the loss and gradient function for</span><br><span class="hljs-string">                               a prediction vector given the outsideWordIdx</span><br><span class="hljs-string">                               word vectors, could be one of the two</span><br><span class="hljs-string">                               loss functions you implemented above.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Return:</span><br><span class="hljs-string">    loss -- the loss function value for the skip-gram model</span><br><span class="hljs-string">            (J in the pdf handout)</span><br><span class="hljs-string">    gradCenterVecs -- the gradient with respect to the center word vector</span><br><span class="hljs-string">                     in shape (num words in vocab, word vector length)</span><br><span class="hljs-string">                     (dJ / dv_c in the pdf handout)</span><br><span class="hljs-string">    gradOutsideVecs -- the gradient with respect to all the outside word vectors</span><br><span class="hljs-string">                    in shape (num words in vocab, word vector length) </span><br><span class="hljs-string">                    (dJ / dU)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    loss = <span class="hljs-number">0.0</span><br>    gradCenterVecs = np.zeros(centerWordVectors.shape)<br>    gradOutsideVectors = np.zeros(outsideVectors.shape)<br><br>    <span class="hljs-comment">### YOUR CODE HERE (~8 Lines)</span><br>    <span class="hljs-comment">#获取center word的vector</span><br>    <span class="hljs-comment">#当前中心词的idx到vec</span><br>    c_i = word2Ind[currentCenterWord]<br>    v_c = centerWordVectors[c_i]<br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> outsideWords:<br>        <span class="hljs-comment">#对每个outside word都求一次loss和gradient</span><br>        o_i=word2Ind[word]<br>        ls,gc,go = word2vecLossAndGradient(v_c,o_i,outsideVectors,dataset)<br>        loss += ls<br>        <span class="hljs-comment">#梯度方法返回的是一个中心词的梯度，这里需要对应到idx</span><br>        gradCenterVecs[c_i] += gc<br>        <span class="hljs-comment">#梯度方法返回的已经是整体的梯度，因此不用再用o_idx</span><br>        gradOutsideVectors += go<br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">return</span> loss, gradCenterVecs, gradOutsideVectors<br></code></pre></td></tr></table></figure><p><img src="/post/38d31aac/image-20230327163115298.png" alt="image-20230327163115298"></p><h3 id="b-1"><a href="#b-1" class="headerlink" title="(b)"></a>(b)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sgd</span>(<span class="hljs-params">f, x0, step, iterations, postprocessing=<span class="hljs-literal">None</span>, useSaved=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        PRINT_EVERY=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Stochastic Gradient Descent</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Implement the stochastic gradient descent method in this function.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">    f -- the function to optimize, it should take a single</span><br><span class="hljs-string">         argument and yield two outputs, a loss and the gradient</span><br><span class="hljs-string">         with respect to the arguments</span><br><span class="hljs-string">    x0 -- the initial point to start SGD from</span><br><span class="hljs-string">    step -- the step size for SGD</span><br><span class="hljs-string">    iterations -- total iterations to run SGD for</span><br><span class="hljs-string">    postprocessing -- postprocessing function for the parameters</span><br><span class="hljs-string">                      if necessary. In the case of word2vec we will need to</span><br><span class="hljs-string">                      normalize the word vectors to have unit length.</span><br><span class="hljs-string">    PRINT_EVERY -- specifies how many iterations to output loss</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Return:</span><br><span class="hljs-string">    x -- the parameter value after SGD finishes</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># Anneal learning rate every several iterations</span><br>    ANNEAL_EVERY = <span class="hljs-number">20000</span><br><br>    <span class="hljs-keyword">if</span> useSaved:<br>        start_iter, oldx, state = load_saved_params()<br>        <span class="hljs-keyword">if</span> start_iter &gt; <span class="hljs-number">0</span>:<br>            x0 = oldx<br>            step *= <span class="hljs-number">0.5</span> ** (start_iter / ANNEAL_EVERY)<br><br>        <span class="hljs-keyword">if</span> state:<br>            random.setstate(state)<br>    <span class="hljs-keyword">else</span>:<br>        start_iter = <span class="hljs-number">0</span><br><br>    x = x0<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> postprocessing:<br>        postprocessing = <span class="hljs-keyword">lambda</span> x: x<br><br>    exploss = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">iter</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_iter + <span class="hljs-number">1</span>, iterations + <span class="hljs-number">1</span>):<br>        <span class="hljs-comment"># You might want to print the progress every few iterations.</span><br><br>        loss = <span class="hljs-literal">None</span><br>        <span class="hljs-comment">### YOUR CODE HERE (~2 lines)</span><br>        <span class="hljs-comment">#f函数计算loss和grad</span><br>        loss,grad = f(x)<br>        <span class="hljs-comment">#更新x,sgd的主要思想就是在负梯度方向下降某个步长，这里题目已经给出。</span><br>        x = x - step*grad<br>        <span class="hljs-comment">### END YOUR CODE</span><br><br>        x = postprocessing(x)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">iter</span> % PRINT_EVERY == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> exploss:<br>                exploss = loss<br>            <span class="hljs-keyword">else</span>:<br>                exploss = <span class="hljs-number">.95</span> * exploss + <span class="hljs-number">.05</span> * loss<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;iter %d: %f&quot;</span> % (<span class="hljs-built_in">iter</span>, exploss))<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">iter</span> % SAVE_PARAMS_EVERY == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> useSaved:<br>            save_params(<span class="hljs-built_in">iter</span>, x)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">iter</span> % ANNEAL_EVERY == <span class="hljs-number">0</span>:<br>            step *= <span class="hljs-number">0.5</span><br><br>    <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p><img src="/post/38d31aac/image-20230327163146208.png" alt="image-20230327163146208"></p><h3 id="c-1"><a href="#c-1" class="headerlink" title="(c)"></a>(c)</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs text">iter 10: 19.824024<br>iter 20: 20.136629<br>iter 30: 20.151500<br>iter 40: 20.211374<br>...<br>iter 11400: 12.003779<br>iter 11410: 12.043692<br>iter 11420: 12.086623<br>iter 11430: 12.043952<br>...<br>iter 39970: 9.330720<br>iter 39980: 9.410215<br>iter 39990: 9.418270<br>iter 40000: 9.367644<br></code></pre></td></tr></table></figure><p><img src="/post/38d31aac/image-20230327162701627.png" alt="image-20230327162701627"></p><p>词向量：</p><p><img src="/post/38d31aac/word_vectors.png" alt="word_vectors"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N作业A1：词向量基础</title>
      <link href="/post/5fb0daaa.html"/>
      <url>/post/5fb0daaa.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="row">    <embed src="../download/a1.pdf" width="100%" height="550" type="application/pdf"></div><h3 id><a href="#" class="headerlink" title></a></h3>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N课程P9：问答技巧（Question &amp; Answering）</title>
      <link href="/post/bf57f2be.html"/>
      <url>/post/bf57f2be.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="课程词汇"><a href="#课程词汇" class="headerlink" title="课程词汇"></a>课程词汇</h1><h1 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h1><h1 id="课后问题"><a href="#课后问题" class="headerlink" title="课后问题"></a>课后问题</h1>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N课程P8：预训练技巧（Pretrain）</title>
      <link href="/post/2dd58a8f.html"/>
      <url>/post/2dd58a8f.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="课程词汇"><a href="#课程词汇" class="headerlink" title="课程词汇"></a>课程词汇</h1><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs txt">morphology - 形态学<br>squash - 压缩<br>contrive - 发明；创造<br>caveat - 警告；警戒<br>interlude - 插曲<br>hedge - 保护；对冲<br>canonical - 典型的<br></code></pre></td></tr></table></figure><h1 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h1><h2 id="子词模型"><a href="#子词模型" class="headerlink" title="子词模型"></a>子词模型</h2><p>在自然语言处理的基本任务上，对单词本身的处理可以有两种比较极端的方法。</p><p>其一是从训练集建立起一个固定的数万个单词的词汇表，所有新的单词都映射为一个统一的UNK标志表示未知性。</p><p><img src="/post/2dd58a8f/image-20230315123209487.png" alt="image-20230315123209487"></p><p>其二是以字符作为编码的基本单位，将单词表示为字符的集合，这在一些语法复杂的语言中效果比较好，因为这些语言的单词多而频率相对低。</p><p><img src="/post/2dd58a8f/image-20230315123346566.png" alt="image-20230315123346566"></p><p>这两种方法各有优缺点，单词固定表示的映射效率高但很难处理新词，而字符表示拥有可变性但对单词的特点不太容易表示，因此目前主流的单词表示方法是两者的折中：子词模型（Subwords Model）。子词模型最初用于NLP机器翻译，现在类似的方法(WordPiece)被用于预训练模型。NLP中的子词建模包含了广泛的方法，用于推理词级以下的结构。(部分单词、字符、字节)，其构造方法大致如下：</p><ol><li>从只包含字符和“end-of-word”符号的词汇表开始；</li><li>使用文本语料库，找到最常见的相邻字符（e.g. “a,b”）;添加“ab”作为子词</li><li>用新的子词替换字符对的实例;重复直到所需的词汇表大小</li></ol><p>在子词生成过程中，常见词因为共现频率高，因此会被识别为一个单词，而新词会被切分为n个常见词。</p><p><img src="/post/2dd58a8f/image-20230315125002438.png" alt="image-20230315125002438"></p><p>子词模型的一个缺点是对子词是否“常见”的界定，最坏的情况子词模型会将一个单词（n个字符）切分为n个单词，大致退化为字符级表示方法，这就违背了我们折中的意愿。</p><h2 id="预训练的引入"><a href="#预训练的引入" class="headerlink" title="预训练的引入"></a>预训练的引入</h2><h3 id="词嵌入的预训练"><a href="#词嵌入的预训练" class="headerlink" title="词嵌入的预训练"></a>词嵌入的预训练</h3><p>预训练的首先使用是在词嵌入模型中，在Circa 2017<a href="https://arxiv.org/abs/1511.01432">Semi-supervised Sequence Learning</a>中首次提到了预训练技术，其中将词嵌入的部分进行预训练保存嵌入向量，将上下文分析的部分（LSTM&#x2F;Transformer）作为下游任务处理。</p><p><img src="/post/2dd58a8f/image-20230315165310431.png" alt="image-20230315165310431"></p><p>其中需要思考的一些问题:</p><ul><li>为下游任务(如问题回答)准备的训练数据必须足以教授语言的所有上下文</li><li>网络中的大多数参数都是<strong>随机初始化</strong>的，没有进行预训练</li></ul><h3 id="整体模型的预训练"><a href="#整体模型的预训练" class="headerlink" title="整体模型的预训练"></a>整体模型的预训练</h3><p>在现代NLP模型中，预训练模型是将模型的所有参数通过预训练进行初始化，以进一步用于下游任务。预训练的基本思路都是一致的：<strong>隐藏部分输入，并通过模型重建这些输入</strong>，模型的参数就是我们需要保存的信息。</p><p><img src="/post/2dd58a8f/image-20230315174056757.png" alt="image-20230315174056757"></p><p>整体模型预训练对于语言表示、strong nlp model的参数初始化、可采样的语言概率分布等方面有着十分突出的表现。</p><p>对于重构后的输入，我们可以从中学习许多层次的知识，包括但不限于词汇的指代关系、相对位置关系、推理关系等等；</p><p><img src="/post/2dd58a8f/image-20230317232212968.png" alt="image-20230317232212968"></p><h3 id="微调（Finetune）"><a href="#微调（Finetune）" class="headerlink" title="微调（Finetune）"></a>微调（Finetune）</h3><p>预训练模型在语言模型中，通过训练神经网络对大量文本进行语言建模，然后保存参数。<strong>预训练模型是对下游模型的训练起点进行移动</strong>，将起点放到一个更适合或更容易的位置。下游训练时通过少量数据对任务本身进行finetune。</p><p><img src="/post/2dd58a8f/image-20230315233827342.png" alt="image-20230315233827342"></p><p>从“训练神经网络”的角度来看，预训练的过程是损失最小化$\mathcal L_{\rm pretrain}(\theta)$得到$\hat \theta$，然后微调是从$\hat \theta$开始最小化$\mathcal L_{\rm finetune}(\theta)$。在微调的随机梯度下降时，预训练得到的起始参数可能会和local minima或者global minima很接近，这也是预训练为什么能够高效的原因之一。</p><h4 id="轻量化微调"><a href="#轻量化微调" class="headerlink" title="轻量化微调"></a>轻量化微调</h4><p>全微调技术虽然效果很好，但每次对所有参数进行微调会对内存有很高的要求，因此引入了轻量化微调的技术。</p><p><img src="/post/2dd58a8f/image-20230317233105537.png" alt="image-20230317233105537"></p><p>轻量化微调技术引入了参数的prefix的概念，首先冻结所有预训练参数，并将参数前缀送到模型处理，对前缀的处理和单词一样。推理时批次的每个元素都可以运行不同的调优模型。</p><p><img src="/post/2dd58a8f/image-20230317233824627.png" alt="image-20230317233824627"></p><h4 id="低秩适应"><a href="#低秩适应" class="headerlink" title="低秩适应"></a>低秩适应</h4><p>低秩适应学习预训练和微调权重矩阵之间的低秩“差异”，是微调技术本身的优化。虽然模型的参数众多，但其实模型主要依赖low intrinsic dimension ，那adaption应该也依赖于此，所以提出了Low-Rank Adaptation。低秩适应允许通过优化适应过程中密集层变化的秩分解矩阵来间接训练神经网络中的一些密集层，同时保持预训练的权重不变。</p><p>低秩适应的思路在于，在原始Pretrained Language model旁边增加一个旁路，做一个降维再升维的操作，来模拟所谓的 <code>intrinsic rank</code> 。训练的时候固定PLM的参数，只训练降维矩阵A与升维矩阵B。</p><p><img src="/post/2dd58a8f/image-20230317233945273.png" alt="image-20230317233945273"></p><h2 id="预训练模型的结构"><a href="#预训练模型的结构" class="headerlink" title="预训练模型的结构"></a>预训练模型的结构</h2><h3 id="Pretrained-Decoder"><a href="#Pretrained-Decoder" class="headerlink" title="Pretrained Decoder"></a>Pretrained Decoder</h3><p>Decoder预训练可以有两种模式：</p><ol><li><p>不考虑Decoder用于概率分布$p(w_t|w_{1:t-1})$预测的用途，即不将其看作语言模型，在Decoder的最后一个隐状态加上一个分类器（线性或非线性）<br>$$<br>\begin{align}<br>h_1,…,h_T&amp;&#x3D;{\rm Decoder}(w_1,…,w_T) \\<br>y &amp;\sim Ah_T+b<br>\end{align}<br>$$<br><img src="/post/2dd58a8f/image-20230317161425473.png" alt="image-20230317161425473"></p><p>需要注意的是，这里的线性模型$A$和$b$都是外部参数，没有进行预训练；但是在预训练模型上finetune（梯度下降）的过程需要走整个模型（从上走到下)</p></li><li><p>将Decoder用作语言模型，建模$p(w_t|w_{1:t-1})$，微调概率分布中的参数$\theta$。这在训练前有充足的序列数据、且输出同样为序列数据时很有效，如对话、文本总结等等。<br>$$<br>\begin{align}<br>h_1,…,h_T&amp;&#x3D;{\rm Decoder}(w_1,…,w_T) \\<br>w_t &amp;\sim Ah_{t-1}+b<br>\end{align}<br>$$<br><img src="/post/2dd58a8f/image-20230317161455115.png" alt="image-20230317161455115"></p><p>在这种模式下，参数$A$和$b$都是预训练中需要训练的参数，因此会更有一般性，更容易进行下游任务的建构。</p></li></ol><h4 id="GPT（Generative-Pretrained-Transformer）"><a href="#GPT（Generative-Pretrained-Transformer）" class="headerlink" title="GPT（Generative Pretrained Transformer）"></a>GPT（Generative Pretrained Transformer）</h4><p>2018年，GPT作为Decoder预训练模型获得了巨大成功。</p><p><img src="/post/2dd58a8f/image-20230316002223069.png" alt="image-20230316002223069"></p><p>在GPT中，为了微调方便（Finetune），也为了能够让更多的任务适配，微调的输入都有着一个格式规定，定义一些特殊的token表示训练时的动作信息，如</p><p><img src="/post/2dd58a8f/image-20230316002437234.png" alt="image-20230316002437234"></p><p>这其中有[start]、[delim]和[extract]，分类任务就是从结尾的[extract]标记的表示作为输入进行训练。</p><h4 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h4><p>GPT-2基于GPT，GPT-2 是在更多数据上训练的更大版本 (1.5B) 的 GPT，它被证明可以生成相对令人信服的自然语言样本。</p><h4 id="GPT-3"><a href="#GPT-3" class="headerlink" title="GPT-3"></a>GPT-3</h4><p>GPT-3基于GPT-2的部分，作为一个非常大的语言模型，其可以简单地从您在其<strong>上下文中提供的示例中</strong>执行某种没有梯度步骤的学习，从而降低模型学习步骤的复杂度。GPT-3 有 1750 亿个参数。</p><p><img src="/post/2dd58a8f/image-20230317232137877.png" alt="image-20230317232137877"></p><p>在GPT-3的模型训练学习过程中，其认为上下文中的例子指定了要执行的任务，条件分布在一定程度上模拟了执行任务，从而可以通过直接学习上下文来获得有效的表示与信息。</p><p><img src="/post/2dd58a8f/image-20230317232119220.png" alt="image-20230317232119220"></p><h3 id="Pretrained-Encoder"><a href="#Pretrained-Encoder" class="headerlink" title="Pretrained Encoder"></a>Pretrained Encoder</h3><p>和Decoder面向于语言模型建模不同，Encoder的主要目标为对语句构建有效的表示，Encoder是双向模型，其可以获取到双向的上下文信息，为了能够实现语言的缺失表示，可以引入屏蔽技术。具体来说，将Encoder的输入覆盖掉一部分，用[MASK]代替，然后Encoder训练时来预测这些词，训练的损失只从这些屏蔽掉的词中获取，令$\tilde x$是单词$x$的被屏蔽版本，那么模型的损失函数就是$p_\theta(x|\tilde x)$的最大化，即两者越相似越好。<br>$$<br>\begin{align}<br>h_1,…,h_T &amp;&#x3D; {\rm Encoder}(w_1,…,w_T) \\<br>y_i &amp;\sim Aw_i+b<br>\end{align}<br>$$</p><p><img src="/post/2dd58a8f/image-20230317161626222.png" alt="image-20230317161626222"></p><h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p>基于此，Devlin 等人在 2018 年提出了“Masked LM”目标并发布了预训练Transformer的权重，他们将这个模型标记为 BERT。在BERT中，对于[MASK]的使用，论文中希望不要让模型确定的知道[MASK]的具体位置，如果模型知道了[MASK]固定会在哪些位置出现，那么模型就不会对没有屏蔽的单词提供强大的表示，这样就违背了模型对语句所有位置提供强大表示的目标。具体的，BERT对[MASK]的使用有如下规则：</p><ul><li>随机选择15%的子词标记（Tokens）：<ul><li>对其中输入单词的80%用[MASK]替换</li><li>对其中输入单词的10%用其他随机token替换</li><li>剩下的10%不变</li></ul></li></ul><p>[MASK]的应用更多的是让模型的表示能力加强，而不是在训练完成之后作为采样来用于下游，因为[MASK]生成的结果仍然有随机性。</p><p><img src="/post/2dd58a8f/image-20230317163033916.png" alt="image-20230317163033916"></p><p>BERT的输入包括了两个独立的连续的chunks，其中BERT训练时需要判断两个chunks中后面的chunk是前面chunk在数据集中紧随的语句，还是一个随机采样的语句，这样BERT也提高了数据集输入分布的随机性。</p><p><img src="/post/2dd58a8f/image-20230317163255579.png" alt="image-20230317163255579"></p><p>但这种随机性的作用大小有待考究，目前也有研究在致力于将BERT的这个随机输入直接改成更长的语句，以提高模型的长文本表达能力。</p><p>BERT的模型架构细节如下，其中需要注意，BERT的预训练时间成本比较高，但Finetune十分实用而且可以多次微调。</p><p><img src="/post/2dd58a8f/image-20230317164826700.png" alt="image-20230317164826700"></p><p>BERT在许多任务上都有十分显著的效果。</p><p><img src="/post/2dd58a8f/image-20230317164910788.png" alt="image-20230317164910788"></p><p>虽然Pretrained Encoder在很多任务上效果都很好，但因为Encoder在语言建模上主要是对词语离散性的去预测， BERT和其他预训练编码器不会自然地导致良好的自回归（一次一个字）生成方法。因此在语言模型上还是Decoder比较出色。</p><p><img src="/post/2dd58a8f/image-20230317214838337.png" alt="image-20230317214838337"></p><h4 id="BERT的扩展"><a href="#BERT的扩展" class="headerlink" title="BERT的扩展"></a>BERT的扩展</h4><p>RoBERTa：去掉下一句预测的环节；BERT的训练时长更长，参数规模更大。</p><p><img src="/post/2dd58a8f/image-20230317220711590.png" alt="image-20230317220711590"></p><p>SpanBERT：连续屏蔽多个词，使模型更难去预测序列。</p><p><img src="/post/2dd58a8f/image-20230317220724206.png" alt="image-20230317220724206"></p><h3 id="Pretrained-Encoder-Decoder"><a href="#Pretrained-Encoder-Decoder" class="headerlink" title="Pretrained Encoder-Decoder"></a>Pretrained Encoder-Decoder</h3><p>预训练Encoder-Decoder结构是由Encoder和Decoder两个部分构成，其中Encoder接收输入用于获得强大的语言表示，Decoder接收输入和Encoder表示用于语言建模。对于Encoder-Decoder，我们可以做一些类似语言建模的事情，但是每个输入的前缀都提供给编码器而不是预测。<br>$$<br>\begin{align}<br>h_1,…,h_T &amp; &#x3D; {\rm Encoder}(w_1,…,w_T) \\<br>h_{T+1},…,h_{2T} &amp;&#x3D; {\rm Decoder}(w_1,…,w_T,h_1,…,h_T)\\<br>y_i &amp;\sim Ah_i+b,\ \ \ \  i&gt;T<br>\end{align}<br>$$<br><img src="/post/2dd58a8f/image-20230317221348067.png" alt="image-20230317221348067"></p><p>目前在业界，Encoder-Decoder中比较优秀的是Raffel et al., 2018所提出的T5模型，在其中加入了span corruption的技术，即Encoder部分替换不同长度的spans并用独一无二的标记表示，Decoder解码出被删除的部分。</p><p><img src="/post/2dd58a8f/image-20230317231312248.png" alt="image-20230317231312248"></p><p><img src="/post/2dd58a8f/image-20230317231446618.png" alt="image-20230317231446618"></p><p>在T5模型中，一个比较出众的部分是在微调阶段，T5可以根据问答的结果从其参数中检索知识，并微调模型以逐渐适配问题，这种类动态性十分厉害。</p><p><img src="/post/2dd58a8f/image-20230317231653307.png" alt="image-20230317231653307"></p><h1 id="课后问题"><a href="#课后问题" class="headerlink" title="课后问题"></a>课后问题</h1><h4 id="Q：子词模型中的”-”代表的是什么意思？"><a href="#Q：子词模型中的”-”代表的是什么意思？" class="headerlink" title="Q：子词模型中的”##”代表的是什么意思？"></a>Q：子词模型中的”##”代表的是什么意思？</h4><p>A：代表的是类似于分词的符号，##告诉我们其前面的单词不是一个单独的完整单词，比如subwords也许会被模型分为sub和words，表示为sub##words，这里的sub是属于原词的一部分而不是一个单独完整的词。</p><h4 id="Q：预训练模型有没有过拟合的风险在？"><a href="#Q：预训练模型有没有过拟合的风险在？" class="headerlink" title="Q：预训练模型有没有过拟合的风险在？"></a>Q：预训练模型有没有过拟合的风险在？</h4><p>A：目前的大模型训练来说，更多的风险在于欠拟合而不是过拟合。</p><h4 id="Q：T5模型中的-lt-x-gt-、-lt-y-gt-是怎么继承的呢？"><a href="#Q：T5模型中的-lt-x-gt-、-lt-y-gt-是怎么继承的呢？" class="headerlink" title="Q：T5模型中的&lt;x&gt;、&lt;y&gt;是怎么继承的呢？"></a>Q：T5模型中的<code>&lt;x&gt;、&lt;y&gt;</code>是怎么继承的呢？</h4><p>A：Decoder的预测部分首先需要取到标记，知道这块需要预测，然后模型预测出结果，这里就已经实现了一一对应。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N课程P7：注意力机制与Transformer</title>
      <link href="/post/d7b08fd6.html"/>
      <url>/post/d7b08fd6.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="课程词汇"><a href="#课程词汇" class="headerlink" title="课程词汇"></a>课程词汇</h1><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs txt">equation - 等式；方程<br>interpretability - 可解释性<br>Quadratic - 二次<br>saturate - 使饱和；使浸透<br>portion - 一份；部分<br></code></pre></td></tr></table></figure><h1 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h1><h2 id="Decoding的优化"><a href="#Decoding的优化" class="headerlink" title="Decoding的优化"></a>Decoding的优化</h2><h3 id="Greedy-decoding"><a href="#Greedy-decoding" class="headerlink" title="Greedy decoding"></a>Greedy decoding</h3><p>在Seq2seq的Decoder中，Decoder的每一步生成目标语句的单词时，都是从Decoder隐状态给出的分布中选择一个概率最大的，即求argmax，这种不断通过argmax的结果作为下一步Decoder输入的过程我们称之为Greedy decoding，即每次都选最好的，这体现了一种贪心的思想。</p><p><img src="/post/d7b08fd6/image-20230304233634360.png" alt="image-20230304233634360"></p><p>Greedy Decoder有一个比较大的缺陷，在于其每次选择之后就<strong>不能回退并修改</strong>，如果模型在其中一步预测有误时，其会一直影响之后目标语句的结果和网络的输出。为了解决这种问题，可以采取迭代的方式，将各种可能性都考虑到，然后选出其中的最优。理论上，在机器翻译中的decoder的目标为：</p><p><img src="/post/d7b08fd6/image-20230304234300819.png" alt="image-20230304234300819"></p><p>在每一步t，我们可以查看词汇表$V^t$中的所有可能的翻译，然后选择最优。这种方法能够解决Greedy decoding的错误影响，但每次的工作量巨大，其时间复杂度$O(V^t)$也不好。为此引入Beam search decoding</p><h3 id="Beam-search-decoding"><a href="#Beam-search-decoding" class="headerlink" title="Beam search decoding"></a>Beam search decoding</h3><p>Beam search decoding的主要作用在于降低查找规模，在decoding的每一步t，保留k个最有可能的部分翻译（称之为<strong>假设</strong>），k被称之为beam size（一般在5-10）。</p><p>设一个假设为$y_1,…,y_t$，这个假设会有一个分数，这个分数就是它的对数概率：<br>$$<br>{\rm score}(y_1,…,y_t) &#x3D; \log P_{LM}(y_1,…,y_t|x) &#x3D; \sum_{i&#x3D;1}^t \log P_{LM}(y_i|y_1,…,y_{i-1},x)<br>$$<br>分数都是负数，且分数越高其表现越好。Beam search不保证每次能够找到最优解，但其处理效率会远远好于全查找（Exhausted search）。</p><p>Beam search的流程如下。每次搜索节点先计算当前位置的$\log P_{LM}$，然后取到上游结果并相加，找到最优的k个假设；然后在同一层级的$k^2$个假设中找到分数最高的$k$个作为下一步搜索的根节点，一直做到结束</p><p><img src="/post/d7b08fd6/gif.75team.com.gif" alt="gif.75team.com"></p><p>在beam search中，我们依旧以生成<code>&lt;end&gt;</code>作为结束符，而不同假设结束的步可能不同；通常，当我们达到了预定义的截断步$T$，或生成了预定义的$n$个假设时，搜索结束，然后计算每个假设的分数并取前k个。这里会存在一个问题：越长的假设其分数可能越低（经验），因此计算分数时需要长度归一化：<br>$$<br>{\rm score}(y_1,…,y_t) &#x3D; {1 \over t} \sum_{i&#x3D;1}^t \log P_{LM}(y_i|y_1,…,y_{i-1},x)<br>$$</p><h2 id="Attention（注意力机制）"><a href="#Attention（注意力机制）" class="headerlink" title="Attention（注意力机制）"></a>Attention（注意力机制）</h2><p>Seq2seq是由encoder和decoder两部分组成，其中encoder作为decoder的conditioner，提供源语句的信息；然而在提供信息时，既有的方法主要是将encoder的结果作为decoder的初始隐状态，这在信息的表达能力上有<strong>瓶颈</strong>（Bottleneck）。因此引入注意力机制解决瓶颈问题。</p><p>注意力机制的核心思想是：在decoder的每一步，decoder的隐状态都能够与encoder建立<strong>直接连接</strong>，关注于<strong>源语句的特定部分</strong>。</p><p>注意力机制的过程如图所示：</p><p><img src="/post/d7b08fd6/gif.75team.com-16780047440113.gif" alt="gif.75team.com"></p><p>注意力机制的过程如下：</p><ol><li><p>有Encoder的隐状态$h_1,…,h_N\in \mathbb R^h$和每一步t的decoder的隐状态$s_t\in \mathbb R^h$</p></li><li><p>首先获得Encoder在第t步的注意力分数$\pmb e^t$<br>$$<br>\pmb e^t &#x3D; [\pmb s_t^\top \pmb h^1+…+\pmb s_t^\top \pmb h^N] \in \mathbb R^N<br>$$</p></li><li><p>然后过一个softmax形成注意力分布$\alpha^t$<br>$$<br>\alpha^t &#x3D; {\rm softmax}(\pmb e^t)\in \mathbb R^N<br>$$</p></li><li><p>用注意力分布$\alpha^t$计算encoder隐状态的加权和作为注意力输出$\pmb a_t$<br>$$<br>\pmb a_t &#x3D; \sum_{i&#x3D;1}^N \alpha_ i^t\pmb h_i \in \mathbb R^h<br>$$</p></li><li><p>最后将注意力分布$\pmb a_t$和decoder隐状态$s_t$连接起来，按照非注意力seq2seq模型继续下游工作。<br>$$<br>[\pmb a_t;\pmb s_t] \in \mathbb R^{2h}<br>$$</p></li><li></li></ol><p>注意力机制的使用成功解决了decoder对源语句的关注，除此之外，attention还有以下的优点：</p><ul><li>改进了NMT的性能，允许decoder关注源语句的特定部分；</li><li>提供了一种类似人类的模型回顾机制，可以回看源语句的信息而不用全盘记忆；</li><li>解决了seq2seq等双网络模型的bottleneck问题，允许decoder直接连接encoder；</li><li>缓解了梯度消失问题，decoder的直连边传递梯度；</li><li>提供了一部分的模型可解释性，通过注意力分布可以了解到decoder的关注信息，从而了解NMT的对齐是怎样工作的。</li></ul><h3 id="Attention的共性流程"><a href="#Attention的共性流程" class="headerlink" title="Attention的共性流程"></a>Attention的共性流程</h3><p>Attention虽然有各种变种，但其主要流程是基本不变的，给定查询的值$\pmb h_1,…,\pmb h_N\in \mathbb R^{d_1}$和一个查询$\pmb s\in \mathbb R^{d_2}$，Attention的基本流程大致为：</p><ol><li>计算注意力分数$\pmb e \in \mathbb R^N$；</li><li>使用softmax等将分数转成分布$\alpha &#x3D; {\rm softmax}(\pmb e)\in \mathbb R^N$；</li><li>使用注意力分布计算注意力输出$\pmb a &#x3D; \sum_{i&#x3D;1}^N \alpha_i\pmb h_i \in \mathbb R^{d_1}$；</li></ol><h3 id="Attention的变体"><a href="#Attention的变体" class="headerlink" title="Attention的变体"></a>Attention的变体</h3><p>在计算注意力分数$\pmb e$上有许多变体的方法：</p><ul><li>基础点积attention：$\pmb e_i &#x3D; \pmb s^\top \pmb h_i \in \mathbb R$<br>这种方法假设$d_1&#x3D;d_2$，这也是我们前面看到的版本</li><li>乘法attention（Multiplicative Attention）：$\pmb e_i &#x3D; \pmb s^\top \pmb W\pmb h_i \in \mathbb R$<br>解决了上述$d_1&#x3D;d_2$的问题，然而$\pmb W$的引入增加了参数规模</li><li>降级的乘法注意力（Reduced-rank Multiplicative Attention）：$\pmb e_i &#x3D; \pmb s^\top (\pmb U^\top\pmb V)\pmb h_i &#x3D; (\pmb {Us} )^\top (\pmb V\pmb h_i)\in \mathbb R$<br>降低了$W$的参数规模，$\pmb U\in \mathbb R^{k\times d_2}$，$\pmb V \in \mathbb R^{k \times d_1}$，$k \ll d_1,d_2$，这也是Transformer的基础</li><li>可加性attention（Additive Attention）：$\pmb e_i &#x3D; \pmb v^\top {\rm tanh}(\pmb W_1 \pmb h_i+\pmb W_2\pmb s) \in \mathbb R$<br>将注意力变成神经网络的可训练形式，直接统一训练，$\pmb W_1\in \mathbb R^{d_3 \times d_1}$，$\pmb W_2 \in \mathbb R^{d_3\times d_2}$，$\pmb v\in \mathbb R^{d_3}$都是权重，其中$d_3$属于超参数。</li></ul><p>Attention在各种网络中都有所应用，只需要满足其查找的基本要求就可以引入Attention机制提供部分关注的效果。</p><p><img src="/post/d7b08fd6/image-20230305182326713.png" alt="image-20230305182326713"></p><p>Attention是基于query的一种关注机制，对values的信息有选择进行加权加和，更高层的说，Attention可以理解为一个存储访问的系统，其中values在memory中保存，query是一种访问机制，Attention提供的是一种合理有效的推荐访问机制，给出的是最适合模型的分布结构。</p><h2 id="自注意力机制（Self-Attention）"><a href="#自注意力机制（Self-Attention）" class="headerlink" title="自注意力机制（Self-Attention）"></a>自注意力机制（Self-Attention）</h2><p>自注意力机制，顾名思义就是“自已关注自己”，前文提到的注意力机制是Decoder对Encoder的关注，其实际上是Decoder对外部信息的一个关注，query来自Decoder而values来自Encoder，而自注意力机制就是自己关注自己的输入，<strong>我的理解是Decoder是query，同时Decoder也是value，并且key也是Decoder。</strong></p><p>（下附一个我认为讲的很好的博主的原文：<a href="https://blog.csdn.net/At_a_lost/article/details/108469516">Attention机制与Self-Attention机制的区别_selfattention_At_a_lost的博客-CSDN博客</a>）</p><blockquote><p>传统的Attention机制在一般任务的Encoder-Decoder model中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。简单的讲就是Attention机制中的权重的计算需要Target来参与的，即在Encoder-Decoder model中Attention权值的计算不仅需要Encoder中的隐状态而且还需要Decoder 中的隐状态。</p><p>而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target&#x3D;Source这种特殊情况下的注意力计算机制。例如在Transformer中在计算权重参数时将文字向量转成对应的KQV，只需要在Source处进行对应的矩阵操作，用不到Target中的信息。</p></blockquote><p>自注意力是注意力的一种，注意力机制解决了RNN中的一系列问题：</p><ul><li>线性交互距离问题：距离的问题会导致较远的状态无法交流信息<br><img src="/post/d7b08fd6/image-20230308202525216.png" alt="image-20230308202525216"></li><li>并行性缺失：在执行前向或后向传播时由于RNN的时序性，无法并行处理所有状态<br><img src="/post/d7b08fd6/image-20230308202606943.png" alt="image-20230308202606943"></li></ul><p>自注意力的大致结构如下：</p><p><img src="/post/d7b08fd6/image-20230308223335908.png" alt="image-20230308223335908"></p><p>在自注意力的结构下，首先每个隐状态$h_i$在作为query时，其计算过程都是单独的，序列长度的增加不会引起不可并行操作数量的增加，且单词的交互距离为$O(1)$，因为每个单词都可以单独对每次单词做连接。</p><p>我们可以将Attention的过程看作对key-value对的一种模糊查找。</p><p><img src="/post/d7b08fd6/image-20230309153935299.png" alt="image-20230309153935299"></p><p><strong>自注意力的keys、values和querys都来自于同一个语句序列，</strong>直观理解，querys表示对信息的查询请求，keys代表对信息的索引，values代表需要查询的信息，自注意力乃至注意力机制都是通过计算querys和keys的亲和度，从而找到最为适合的values，其中keys和values更像是键值对的关系。</p><p>定义语句为$\pmb w_{1:n}$，其中$\forall \pmb w_i \in V$。对于每个单词$\pmb w_i$，做词嵌入，令$\pmb x_i &#x3D; E\pmb w_i$，其中$E\in \mathbb R^{d\times |V|}$是嵌入矩阵。自注意力需要计算以下内容：</p><ol><li><p>用权重矩阵$Q、K、V\in \mathbb R^{d\times d}$变换每个词嵌入：<br>$$<br>\pmb q_i &#x3D; Q\pmb x_i \ \ \ \ \pmb k_i &#x3D; K\pmb x_i \ \ \ \ \pmb v_i &#x3D; V\pmb x_i<br>$$</p></li><li><p>计算注意力分布：计算键和查询之间的成对相似度并用softmax归一化（在key的维度上作归一化）<br>$$<br>\pmb e_{ij} &#x3D; \pmb q_i^\top\pmb k_j \ \ \ \ \pmb \alpha_{ij} &#x3D; { { {\rm exp}(\pmb e_{ij}) } \over {\sum_{j’} \exp(\pmb e_{ij’} ) } }<br>$$</p></li><li><p>计算每个单词的输出作为值的加权和：<br>$$<br>\pmb o_i &#x3D; \sum_j\pmb \alpha_{ij} \pmb v_i<br>$$</p></li></ol><h3 id="Self-Attention的模块化处理"><a href="#Self-Attention的模块化处理" class="headerlink" title="Self-Attention的模块化处理"></a>Self-Attention的模块化处理</h3><p>为了能够将self-attention用作模块来搭建网络，需要对self-attention加入一些细节，解决其模块化的障碍所在。</p><h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><p>自注意力在每个单词上都做一次注意力计算，在计算中并没有考虑单词的相对顺序，因为self-attention并不是建立在顺序信息中，故我们需要在我们的键、查询和值中编码句子的顺序。考虑将每个序列索引表示为一个向量，称之为<strong>位置编码</strong>：<br>$$<br>\pmb p_i\in \mathbb R^d, \ \ \ \ i\in{1,2,…,n}<br>$$<br>自注意力通过将位置编码的信息直接加到输入中，在深度自注意力网络中我们在第一层加入位置编码信息。<br>$$<br>\tilde {\pmb x}_i &#x3D; \pmb x_i + \pmb p_i<br>$$<br>位置编码可以通过周期函数来进行实现，其中横轴为序列索引（1-&gt;n），横轴为维度d</p><p><img src="/post/d7b08fd6/image-20230309161810657.png" alt="image-20230309161810657"></p><p>sin函数的周期性表明“绝对位置”可能不那么重要，我们只需要了解相对位置就可以实现将位置信息引入自注意力；而且，周期性的变化可以适配于长序列，因为函数可以多次回到起始点不断循环。但因为sin函数这样的周期性函数是模型开始时就规定好的，因此这个位置编码不可学习，无法实现动态调整。</p><p>因此，位置编码信息的学习被提出，令$\pmb p\in \mathbb R^{d\times n}$为可学习的参数，并令$\pmb p_i$为每个矩阵的列，让模型学习绝对位置表示。可学习的位置编码具有灵活可调整的特性，但其无法推断出超过index范围的编码（如n+1），目前，人们仍然在位置编码领域有所研究，现有研究有线性相对位置编码（相对位置）、基于依赖语法的位置编码（加入人类理解）等。<img src="/post/d7b08fd6/image-20230309162839143.png" alt="image-20230309162839143"></p><h4 id="非线性模块"><a href="#非线性模块" class="headerlink" title="非线性模块"></a>非线性模块</h4><p>自注意力的本质是先计算注意力分布后取平均，其本身没有非线性部分加入，非线性的部分可以为网络加入更多的表示能力。其中一种简单的解决方法：为每个输出向量添加一个前馈网络：<img src="/post/d7b08fd6/image-20230309220441522.png" alt="image-20230309220441522"><br>$$<br>\pmb m_i &#x3D; {\rm MLP(output_i)} &#x3D; \pmb W_2 \times {\rm ReLU}(\pmb W_1 \times {\rm output} _i+\pmb b_i)+\pmb b_2<br>$$</p><h4 id="自注意力掩码（解码器）"><a href="#自注意力掩码（解码器）" class="headerlink" title="自注意力掩码（解码器）"></a>自注意力掩码（解码器）</h4><p>要在解码器中使用自注意力，由于其作为语言模型需要严格从一个方向（从左到右）做预测，因此未来的一些信息是在自注意力中不能看到的，因此需要引入掩盖的技术来关掉自注意力在未来部分的视野。</p><p>对自注意力视野的掩盖，比较直观的方法是在每个时间步中，更改key和query，以只包括过去的单词，但是这种方法效率很低，而且比较难并行训练，为了实现并行化，我们通过将注意力分数设置为$-∞$来屏蔽对未来单词的注意力。<br>$$<br>\begin{cases}<br>\pmb e_{ij} &#x3D; \pmb q_i^\top \pmb k_j,\ \ \ \ j\le i \\<br>\pmb e_{ij} &#x3D; -∞, \ \ \ \ \ j &gt;i<br>\end{cases}<br>$$<br>自注意力掩盖可以形象为以下表示：</p><p><img src="/post/d7b08fd6/image-20230309222253550.png" alt="image-20230309222253550"></p><p>综上所述，对于自注意力中有关模块化训练的一些问题都对应进行了解决。</p><p><img src="/post/d7b08fd6/image-20230309222833919.png" alt="image-20230309222833919"></p><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>Transformer解码器是我们构建语言模型等系统的方法。它很像我们最小的自注意力架构，但有更多的组件，其中嵌入和位置嵌入是相同的。</p><h3 id="注意力的堆叠"><a href="#注意力的堆叠" class="headerlink" title="注意力的堆叠"></a>注意力的堆叠</h3><p>首先，为了能够使得注意力作为模块被使用，我们需要将上文中提到的注意力计算变成矩阵计算的形式，从而提高计算效率，实现并行计算的愿景。对于key-query-value 注意力来说计算过程如下：</p><ol><li>令$X&#x3D;[\pmb x_1,…,x_n]\in \mathbb R^{n\times d}$为输入向量的连接</li><li>记录$XK\in \mathbb R^{n\times d}$、$XQ\in \mathbb R^{n\times d}$和$XV\in \mathbb R^{n\times d}$</li><li>直接计算注意力输出${\rm output} &#x3D; {\rm softmax}((XQ(K^\top X^\top))XV \in \mathbb R^{n\times d}$</li></ol><p><img src="/post/d7b08fd6/image-20230309234727765.png" alt="image-20230309234727765"></p><h3 id="多头注意力机制（Multi-head-Attention）"><a href="#多头注意力机制（Multi-head-Attention）" class="headerlink" title="多头注意力机制（Multi-head Attention）"></a>多头注意力机制（Multi-head Attention）</h3><p>假设我们想一次查找句子中的多个位置，而简单自注意力每次只关注Q和K最为亲和的一个，因此这时就比较难以完成任务。多头注意力旨在引入多个Q-K-V来实现多个部分的查找。令$Q_l,K_l,V_l \in  \mathbb R^{d\times {d \over h} }$，其中h代表注意力头的数量，$l\in \lbrace 1,…,n \rbrace$。这时，多头注意力的计算过程就变成了：</p><ol><li>令$X&#x3D;[\pmb x_1,…,x_n]\in \mathbb R^{n\times d}$为输入向量的连接</li><li>记录$XK\in R^{n\times {d \over h} }$、$XQ\in R^{n\times {d \over h} }$、$XV\in R^{n\times {d \over h} }$</li><li>计算注意力输出${\rm output}_l &#x3D; {\rm softmax}(XQ_l(K_l^\top X^\top))*XV_l \in \mathbb R^{d \over h}$</li><li>将输出拼接起来${\rm output} &#x3D; [{\rm output}_1;…;{\rm output}_h]Y$，其中$Y\in \mathbb R^{d \times d}$，这个$Y$的作用我的理解是多头注意力的一个简单加权。</li></ol><p><img src="/post/d7b08fd6/image-20230310001801743.png" alt="image-20230310001801743"></p><p>在多头注意力中，每个注意力头都分别关注句子中的不同层面，比如语法结构、实体本身等等。</p><p><img src="/post/d7b08fd6/image-20230310001441536.png" alt="image-20230310001441536"></p><h3 id="Transformer的训练技巧"><a href="#Transformer的训练技巧" class="headerlink" title="Transformer的训练技巧"></a>Transformer的训练技巧</h3><p>为了能够更有效的训练Transformer，一些必要的技巧（trick）需要引入，但这些技巧不会影响训练本身的结果</p><h4 id="缩放点积（Scaled-dot-production）"><a href="#缩放点积（Scaled-dot-production）" class="headerlink" title="缩放点积（Scaled dot production）"></a>缩放点积（Scaled dot production）</h4><p>缩放点积技巧引入的原因在于点积本身的增长和维度$d$有关（$d$会和加权计算的次数相关），这会使得点积作为softmax函数的输入可以很大，从而过分挤占其他部分的梯度空间，使得梯度十分小。因此对于点积的部分，除以一个缩放量$\sqrt{d&#x2F;h}$<br>$$<br>{\rm output}_l &#x3D; {\rm softmax}({ {XQ_l(K_l^\top X^\top)} \over {\sqrt{d&#x2F;h}} })*XV_l \in \mathbb R^{d \over h}<br>$$</p><h4 id="残差连接（Residual-connections-Add）"><a href="#残差连接（Residual-connections-Add）" class="headerlink" title="残差连接（Residual connections,Add）"></a>残差连接（Residual connections,Add）</h4><p>残差连接是帮助<strong>模型更好训练</strong>的一个技巧。残差连接主要解决梯度消失的问题，是一种经典的训练技巧，假设我们的模型是输入经过一个很复杂很复杂的函数获得输出<br>$$<br>X^{(i)} &#x3D; {\rm Layer}(X^{(i-1)})<br>$$<br>引入残差连接，直接将输入和输出相连，只需要学习上一层的“残差”<br>$$<br>X^{(i)} &#x3D; X^{(i-1)}+{\rm Layer}(X^{(i-1)})<br>$$<br><img src="/post/d7b08fd6/image-20230310005116347.png" alt="image-20230310005116347"></p><h4 id="层归一化（Layer-normalization-Norm）"><a href="#层归一化（Layer-normalization-Norm）" class="headerlink" title="层归一化（Layer normalization,Norm）"></a>层归一化（Layer normalization,Norm）</h4><p>层归一化是一种帮助<strong>模型训练更快</strong>的技巧。层归一化的想法是希望通过归一化为每层内的单位均值和标准差来减少隐藏向量值的无信息变化。<strong>LN就是在每个样本上统计所有维度的值，计算均值和方差</strong>，当我们使用梯度下降法做优化时，随着网络深度的增加，数据的分布会不断发生变化，为了<strong>保证数据特征分布的稳定性</strong>，我们加入Layer Normalization，这样可以加速模型的收敛速度。</p><p>层归一化的思路在于对样本本身做归一化，$x\in \mathbb R^{d}$表示模型中的单个(词)向量，层归一化需要计算出样本在所有维度上的均值和方差（标准差）：<br>$$<br>\mu &#x3D; \sum_{i&#x3D;1}^d x_i, \ \ \ \ \sigma &#x3D; \sqrt{ {1 \over d} \sum_{i&#x3D;1}^d(x_i-\mu)^2},\ \ \ <br>$$</p><p>其中$\mu,\sigma\in \mathbb R$。层归一化引入了增益项$\gamma \in \mathbb R^d$和偏差项$\beta \in \mathbb R^d$，这两个参数都是可学习的参数，层归一化的输出为：<br>$$<br>{\rm output} &#x3D; { {x-\mu} \over {\sqrt{\sigma^2+\epsilon} } }*\gamma+\beta<br>$$</p><h3 id="Transformer的Encoder和Decoder"><a href="#Transformer的Encoder和Decoder" class="headerlink" title="Transformer的Encoder和Decoder"></a>Transformer的Encoder和Decoder</h3><p>在Transformer的Encoder和Decoder中，我们都采用了位置编码-多头注意力-Add&amp;Norm-前馈网络-Add&amp;Norm的结构，其中Decoder的注意力中加入mask来实现单向语言模型。实际上在Transformer建构时，Decoder对于Encoder的回顾主要是交叉注意力，因此这里需要引入交叉注意力的概念。</p><p><img src="/post/d7b08fd6/image-20230310203323856.png" alt="image-20230310203323856"></p><h4 id="交叉注意力"><a href="#交叉注意力" class="headerlink" title="交叉注意力"></a>交叉注意力</h4><p>交叉注意力用于像上文提到的Seq2seq这样的Encoder-Decoder结构，而在Transformer结构中也采取了这样的方式，这需要Decoder对Encoder的信息进行回顾，这种回顾的机制融入到了注意力参数生成本身。</p><p>令$\pmb h&#x3D;\lbrace \pmb h_1,…,\pmb h_n\rbrace,\pmb h_i\in\mathbb R^d$为<strong>Encoder的输出</strong>，$\pmb z &#x3D; \lbrace \pmb z_1,…,\pmb z_n\rbrace \in \mathbb R^d$为<strong>Decoder的输入</strong>，交叉注意力的key和value来自于Encoder：<br>$$<br>\pmb k_i &#x3D; K\pmb h_i,\ \ \ \ \pmb v_i &#x3D; V\pmb h_i<br>$$<br>交叉注意力的query来自Decoder：<br>$$<br>\pmb q_i &#x3D; Q\pmb z_i<br>$$<br>这样就实现了之前的那种通过Decoder的query来查找Encoder的value的效果了。</p><h3 id="Transformer的表现"><a href="#Transformer的表现" class="headerlink" title="Transformer的表现"></a>Transformer的表现</h3><p>由于Transformer的高可并行性，基于Transformer的训练就可以使用预训练模型进行微调，以高效实现任务，因此目前在比较受欢迎的benchmack平台GLUE上，各个任务排名靠前的模型基本都是Transformer为基础的模型。</p><p><img src="/post/d7b08fd6/image-20230310205159890.png" alt="image-20230310205159890"></p><h3 id="Transformer的缺点"><a href="#Transformer的缺点" class="headerlink" title="Transformer的缺点"></a>Transformer的缺点</h3><p>Transformer尽管有着十分高效的训练速度和强悍的表现，但其在参数上还是有一些缺点在</p><ul><li>计算量会随着序列长度的增加而二次单位的增大（每个单词都得和其他所有单词连接一遍）<br><img src="/post/d7b08fd6/image-20230310210836117.png" alt="image-20230310210836117"></li><li>简单的绝对下标作为位置编码可能不太和人类的阅读习惯相匹配</li></ul><p>对于第一点，现有工作中已经开始针对其进行研究，比较主流的是将序列长度中针对Key和Value的部分映射到低维空间降低计算量。</p><p><img src="/post/d7b08fd6/image-20230310211306729.png" alt="image-20230310211306729"></p><p>实际上，几乎没有大型 Transformer 语言模型使用我们在这里展示的二次成本注意力，都是用的成本相对更低的注意力模型，但其在大型数据上的效果往往不太好，这需要对低成本注意力或大规模数据做继续研究。</p><p><img src="/post/d7b08fd6/image-20230310212050537.png" alt="image-20230310212050537"></p><h1 id="课后问题"><a href="#课后问题" class="headerlink" title="课后问题"></a>课后问题</h1><h4 id="Q：自注意力和前馈神经网络都进行了全连接，这两个有什么不同"><a href="#Q：自注意力和前馈神经网络都进行了全连接，这两个有什么不同" class="headerlink" title="Q：自注意力和前馈神经网络都进行了全连接，这两个有什么不同"></a>Q：自注意力和前馈神经网络都进行了全连接，这两个有什么不同</h4><p>A：不同有二：一是前馈神经网络的权重是随着训练不断迭代的，而自注意力的注意力值会随着内容的变化而变化，是动态的；二是自注意力提供了一种生成归纳偏置的一种机制（不太懂）</p><h4 id="Q：Decoder的掩码是只在第一层用还是对所有都用"><a href="#Q：Decoder的掩码是只在第一层用还是对所有都用" class="headerlink" title="Q：Decoder的掩码是只在第一层用还是对所有都用"></a>Q：Decoder的掩码是只在第一层用还是对所有都用</h4><p>A：都用，只有都掩盖掉，未来的信息才不会在任意一个层中泄露。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N课程P6：LSTM和机器翻译</title>
      <link href="/post/c4e50226.html"/>
      <url>/post/c4e50226.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="课程词汇"><a href="#课程词汇" class="headerlink" title="课程词汇"></a>课程词汇</h1><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs txt">allude - 暗示；提到<br>residual - 残余的；剩余的<br>versatile - 万能的；多种用途的<br>canonical - 经典的；规范的<br>rule of thumb - 经验法则；概测法<br>tractable - 易处理的；易控制的<br>beam - 横梁；光束；船宽<br>utterance - 话语；意义<br>augmentation - 增强；扩增<br>specify - 指定；详列<br></code></pre></td></tr></table></figure><h1 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h1><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h3 id="引入：梯度消失问题的解决"><a href="#引入：梯度消失问题的解决" class="headerlink" title="引入：梯度消失问题的解决"></a>引入：梯度消失问题的解决</h3><p>需要解决梯度消失的问题，本质上是需要解决经过多个时间步之后，远处的信息如何得到保存的问题，为了解决这个问题，LSTM引入了一个分开的存储来保存信息。</p><p><img src="/post/c4e50226/image-20230227181945218.png" alt="image-20230227181945218"></p><h3 id="LSTM的结构"><a href="#LSTM的结构" class="headerlink" title="LSTM的结构"></a>LSTM的结构</h3><p>LSTM的结构如下：在每一步$t$中设置一个隐状态（hidden state）$\pmb h^{(t)}$和一个细胞状态（cell state）$\pmb c^{(t)}$：</p><ul><li>两个都是$n$维向量</li><li>细胞状态存储的是长程信息</li><li>LSTM可以从细胞状态中读取、修改或写入信息，决定信息的操作的是三个对应的门，门的开关通过现有上下文来进行计算。</li></ul><p>LSTM的两个状态的计算公式如下：</p><p><img src="/post/c4e50226/image-20230227193921320.png" alt="image-20230227193921320"></p><p>需要注意的是每个门对应的信息。其中遗忘门对应于遗忘之前cell state里的信息，输入门对应于写入cell state，输出门对应于将cell state的信息输出到hidden state。这里用到了Hadamard乘积。</p><p>LSTM的可视化解释为：</p><p><img src="/post/c4e50226/image-20230227234457318.png" alt="image-20230227234457318"></p><p>这张图给出了LSTM的运行机理，首先神经元是沿着从$c_{t-1}$到$c_t$的<strong>主干道</strong>传送长程信息，然后在主干道上，先通过遗忘门判断保存$c_{t-1}$的哪些信息，然后过输入门，并计算新的cell信息，给到cell state，最后过输出门，判断是否将$c_i$的部分信息输出到$h_t$，从而实现整体运行。</p><p>LSTM相对于Vanilla RNN，其在保留长程信息上表现十分优秀，甚至来说，将遗忘门置1并将输入门置0，则cell state的信息会无限的保存下去，而RNN中一直使用的$\pmb W_h$会导致其容易出现梯度问题。实践下来，LSTM可以保存近百步的信息，而Vanilla RNN大概只能保存7步左右。</p><h3 id="其他解决梯度消失的结构"><a href="#其他解决梯度消失的结构" class="headerlink" title="其他解决梯度消失的结构"></a>其他解决梯度消失的结构</h3><p>梯度消失的问题在各个深层神经网络中都有出现，其中以时序网络更为明显，为了解决梯度消失的问题，一些神经网络引入了直连边，允许梯度在边上流动，如十分重要的ResNet：</p><p><img src="/post/c4e50226/image-20230227235820182.png" alt="image-20230227235820182"></p><p>更多的，在状态之间引入稠密连接，每层都与其他未来的层进行相连，就是DenseNet的主要思路。以及像LSTM一样设置一个主干道连接，这是HighwayNet的基本思路。总而言之，梯度消失是在网络中十分普遍的问题，如今的人们也在致力于完美解决这个问题。</p><h2 id="LSTM的优化结构"><a href="#LSTM的优化结构" class="headerlink" title="LSTM的优化结构"></a>LSTM的优化结构</h2><h3 id="BiLSTM"><a href="#BiLSTM" class="headerlink" title="BiLSTM"></a>BiLSTM</h3><p>如果将LSTM中的隐状态看作单词的一种表示（如词嵌入），则LSTM也可以理解成对句子的上下文表示进行学习，不过LSTM只有一个方向（左边）的context作为特征来学习，故引入了双向LSTM网络（BiLSTM），分别从左边和右边构建一个LSTM网络，最后将两个方向的结果简单连接起来。</p><p><img src="/post/c4e50226/image-20230228124754455.png" alt="image-20230228124754455"></p><p>在第$t$个时间步：<br>$$<br>\begin{align}<br>{\rm Forward\ RNN }\ \ \overrightarrow {\pmb h}^{\ (t)} &amp; &#x3D; {\rm RNN_ {FM} }\left( \overrightarrow {\pmb h}^{\ (t-1)}, \pmb x^{(t)} \right) \\<br>{\rm Backward\ RNN }\ \overleftarrow {\pmb h}^{(t)} &amp; &#x3D; {\rm RNN_ {BM} }\left( \overleftarrow {\pmb h}^{\ (t+1)}, \pmb x^{(t)} \right) \\<br>{\rm Hidden\ State}\ {\pmb h}^{(t)} &amp; &#x3D;[\overrightarrow {\pmb h}^{\ (t)} ; \overleftarrow {\pmb h}^{(t)} ]<br>\end{align}<br>$$<br>BiLSTM在图示上可以简化为：</p><p><img src="/post/c4e50226/image-20230228132603867.png" alt="image-20230228132603867"></p><p>BiLSTM适用于需要处理整体输入序列，双方向的任务（如编码），而不适用于像语言模型这样单方向的语言任务，BiLSTM在现有的语言领域中使用十分广泛，如现在十分火热的BERT架构，其中的“B”就是BiLSTM的缩写。</p><h3 id="Multi-layer-RNN"><a href="#Multi-layer-RNN" class="headerlink" title="Multi-layer RNN"></a>Multi-layer RNN</h3><p>上文所讲述的RNN都是基于一个维度的加深，即从左到右深度较深，timestep许多但在另一个维度上很窄，因此引入了多层RNN的架构，在层数上加深。这能够使RNN计算出更为复杂，更高层的表示信息，Muiti-layer RNNs又称为stacked RNNs。</p><p>目前来说，表现较好的RNN模型通常都为多层结构（不像卷积网络或前馈网络那么深层），进入到更深层（超过4层）则通常需要使用skip-connections或dense-connections来防止梯度问题，基于Transformer的RNN网络则在深度上会更高（一般在12或24层）。</p><p><img src="/post/c4e50226/image-20230302153041359.png" alt="image-20230302153041359"></p><h2 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h2><p>机器翻译任务（Machine Translation,MT）是将基于一种语言的句子（Source language）翻译到另一种语言（Target language），其中心思想为从语料数据中学习一个概率模型，下文均基于法译英展开。</p><p>机器翻译的任务可以表示为：<br>$$<br>{\rm argmax}_y P(y|x)<br>$$<br>使用贝叶斯公式可以将任务拆分为两个部分：<br>$$<br>{\rm argmax}_y P(y|x) &#x3D; {\rm argmax}_yP(x|y)P(y)<br>$$<br><img src="/post/c4e50226/image-20230302154357091.png" alt="image-20230302154357091"></p><h3 id="统计机器翻译（SMT，1990s—2010s）"><a href="#统计机器翻译（SMT，1990s—2010s）" class="headerlink" title="统计机器翻译（SMT，1990s—2010s）"></a>统计机器翻译（SMT，1990s—2010s）</h3><p>统计机器翻译在当时是一个十分巨大的研究领域，其最优的系统都十分复杂，有数以百计的重要细节需要考量。而且系统中包含了许多分别设计的子部件，因此需要十分庞大的特征工程，额外的资源和人力。</p><h3 id="神经网络机器翻译（NMT，2010s至今）"><a href="#神经网络机器翻译（NMT，2010s至今）" class="headerlink" title="神经网络机器翻译（NMT，2010s至今）"></a>神经网络机器翻译（NMT，2010s至今）</h3><p>神经网络机器翻译是通过单个的端到端网络来实现机器翻译的方法，其中神经网络的架构被称为<strong>Sequence-to-sequence</strong>模型（seq2seq），其中包括两个RNN，一个为编码器（Encoder），一个为解码器（Decoder）。</p><p><img src="/post/c4e50226/image-20230302163546790.png" alt="image-20230302163546790"></p><p>如图所示，左边的红色部分为Encoder部分，负责将源句子编码表示，其中Encoder的结果作为Decoder的初始隐状态，Decoder负责基于Encoder的条件，作为语言模型生成目标语句。这体现了<strong>条件网络</strong>的概念，即将之前的Encoder的结果作为一种条件指导Decoder的训练。目前来说，条件网络的条件可以有以下几种：</p><ul><li>作为初始隐状态</li><li>作为下个网络的输入</li></ul><h4 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h4><p><img src="/post/c4e50226/image-20230302221042200.png" alt="image-20230302221042200"></p><p>Seq2seq在nlp的许多应用场景中都有所使用，如：</p><ul><li>Summarization（long text $\rightarrow$ short text）</li><li>Dialogue（previous utterances $\rightarrow$ next utterance）</li><li>Parsing（input text $\rightarrow$ output parse as sequence）</li><li>Code generation（natural language$\rightarrow$ Python code）</li></ul><p>Seq2seq在这里的模型是一个典型的条件语言模型的例子：</p><ul><li><strong>语言模型：</strong>Decoder每个隐状态预测一次目标语句中的下一个单词</li><li><strong>条件：</strong>Decoder的预测由源语句经Encoder编码后条件约束。</li></ul><p>NMT直接计算了概率分布$P(y|x)$以给出机器翻译结果，具体来说是不断计算$P(y|x)$在贝叶斯分解后的每个部分，其训练主要是需要一个相对大的并行语料，但目前也有无监督NMT和数据增强等一些新兴的工作。</p><p><img src="/post/c4e50226/image-20230302222256755.png" alt="image-20230302222256755"></p><h4 id="NMT的训练和多层NMT"><a href="#NMT的训练和多层NMT" class="headerlink" title="NMT的训练和多层NMT"></a>NMT的训练和多层NMT</h4><p>NMT的<strong>前向训练</strong>就如上文中的图示一样，先训练Encoder获取表示，后Decoder训练语言模型，其中每个隐状态都需要计算真实语句和预测语句中对应位置的<strong>负对数损失</strong>，最后做加和；反向传播的过程中，Seq2seq通常被看作一个整体。Encoder的输出也有将之前的隐状态做一次平均的情况存在，但大多数就是从前到后做训练。</p><p><img src="/post/c4e50226/image-20230302224039510.png" alt="image-20230302224039510"></p><p>将NMT加深层次，多层NMT就可以更有效地捕捉语句信息。</p><p><img src="/post/c4e50226/image-20230302224913920.png" alt="image-20230302224913920"></p><h2 id="机器翻译的评估"><a href="#机器翻译的评估" class="headerlink" title="机器翻译的评估"></a>机器翻译的评估</h2><p>机器翻译的评估通常采用BLEU（Bilingual Evaluation Understudy）。BLEU通过比较机器翻译和人类手工翻译并计算其中的相似度分数，分数的计算基于两个部分：</p><ol><li>n-gram precision（Usually for 1,2,3 and 4-grams）</li><li>对系统翻译过短的惩罚项</li></ol><p>具体的细节要到A4作业看，BLEU作为评估指标上是很有效的，但其确实也有一些缺点，因为好的翻译的评价是很多元化的，因此可能会出现翻译在我们人类看来是很出色但是BLEU分数很低（因为用词不一样）的情况出现。故BLEU也只是覆盖到了大部分的情况。</p><h2 id="机器翻译的分析"><a href="#机器翻译的分析" class="headerlink" title="机器翻译的分析"></a>机器翻译的分析</h2><p>对比SMT，NMT有以下<strong>优点：</strong></p><ul><li>更好的翻译表现<ul><li>更加流利的译后语言</li><li>对context和phrase similarities利用得更完备</li></ul></li><li>一个单一的神经网络可以端到端优化<ul><li>没有子组件需要特别进行优化</li></ul></li><li>人力和特征工程资源要求降低<ul><li>没有特征工程</li><li>对所有语言方法一致</li></ul></li></ul><p>但NMT也有以下<strong>弱点</strong>需要注意：</p><ul><li>NMT的可解释性更差<ul><li>难以debug</li></ul></li><li>NMT的可控性更差<ul><li>例如：无法轻松指定翻译规则或准则</li><li>安全性考量</li></ul></li></ul><p>最后，NMT近些年的优秀表现，使得绝大部分的企业都选择了NMT作为机器翻译的首选。</p><p><img src="/post/c4e50226/image-20230303000923604.png" alt="image-20230303000923604"></p><h1 id="课后问题"><a href="#课后问题" class="headerlink" title="课后问题"></a>课后问题</h1><h4 id="Q：门的工作原理"><a href="#Q：门的工作原理" class="headerlink" title="Q：门的工作原理"></a>Q：门的工作原理</h4><p>所有的门控都需要学习，都是在模型中学习出来的，实际上效果是可以的</p><h4 id="Q：为什么LSTM中的加和操作-Add-是十分重要的？"><a href="#Q：为什么LSTM中的加和操作-Add-是十分重要的？" class="headerlink" title="Q：为什么LSTM中的加和操作(Add)是十分重要的？"></a>Q：为什么LSTM中的加和操作(Add)是十分重要的？</h4><p>因为加和相对于乘积，其梯度的不断下降的程度会弱一些，这也间接防止了梯度消失的快速出现。而且加和操作可以大致实现信息的整合，需要加哪些信息，需要忘记哪些信息。</p><h4 id="Q：在多层RNN中，低级语义和高级语义分别指什么？示例？"><a href="#Q：在多层RNN中，低级语义和高级语义分别指什么？示例？" class="headerlink" title="Q：在多层RNN中，低级语义和高级语义分别指什么？示例？"></a>Q：在多层RNN中，低级语义和高级语义分别指什么？示例？</h4><p>低级特征是在句子中更为基础的特征，如单词的词性等等，而高级特征更关注于高维的表示，如句子的结构、句子的情绪价值等等。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N课程P5：语言模型和循环神经网络RNN</title>
      <link href="/post/858fce89.html"/>
      <url>/post/858fce89.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="课程词汇"><a href="#课程词汇" class="headerlink" title="课程词汇"></a>课程词汇</h1><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs txt">magnitude - 大小；量级<br>consecutive - 连续的<br>granularity - 粒度<br>incoherent - 不连贯的<br>subtlety - 微妙之处；巧妙<br>chug - 一饮而尽；一次性处理<br>thrall - 使成为奴隶；受奴役的<br></code></pre></td></tr></table></figure><h1 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h1><h2 id="网络优化与正则化"><a href="#网络优化与正则化" class="headerlink" title="网络优化与正则化"></a>网络优化与正则化</h2><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>一个完整的损失函数一般会包含一个正则化项，用于降低模型过拟合程度：</p><p><img src="/post/858fce89/image-20230222003634859.png" alt="image-20230222003634859"></p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>参见<a href="/post/cbfc2cb0.html#%E5%8F%82%E6%95%B0%E5%92%8C%E6%95%B0%E6%8D%AE">[网络优化与正则化笔记 - Ywj226]</a>。Dropout可以有效解决因特征依赖导致的特征的不完全性。</p><p><img src="/post/858fce89/image-20230222004026447.png" alt="image-20230222004026447"></p><p><img src="/post/858fce89/image-20230222141846312.png" alt="image-20230222141846312"></p><h3 id="非线性函数"><a href="#非线性函数" class="headerlink" title="非线性函数"></a>非线性函数</h3><p><img src="/post/858fce89/image-20230222141942561.png" alt="image-20230222141942561"></p><h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>常用的参数初始化方法是将权重设置为(-r,r)的均匀分布，且不能将其设置为0矩阵，防止出现对称问题，但是偏置bias从经验来说是可以置0的。现有的初始化方法中，还有根据扇入扇出的规模做初始化的Xavier方法。</p><p><img src="/post/858fce89/image-20230222152537188.png" alt="image-20230222152537188"></p><h3 id="优化方法和学习率"><a href="#优化方法和学习率" class="headerlink" title="优化方法和学习率"></a>优化方法和学习率</h3><p>参见<a href="/post/cbfc2cb0.html#%E5%AD%A6%E4%B9%A0%E7%8E%87">[网络优化与正则化笔记 - Ywj226]</a>。</p><p><img src="/post/858fce89/image-20230222154353928.png" alt="image-20230222154353928"></p><h2 id="语言模型和RNN"><a href="#语言模型和RNN" class="headerlink" title="语言模型和RNN"></a>语言模型和RNN</h2><p>语言模型的定义为：预测下一个单词是什么的任务；如输入法的智能推荐，搜索引擎的下拉框，都是语言模型的具体应用。更为正式的，定义语言模型的形式如下：给定一个单词序列$x^{(1)},x^{(2)},…,x^{(T)}$，此段文本的出现概率为：<br>$$<br>P(x^{(1)},…,x^{(T)})&#x3D;P(x^{(1)}) \times P(x^{(2)}|x^{(1)})\times …\times P(x^{(T)}|x^{(T-1)},…,x^{(1)}) &#x3D; \prod_{t&#x3D;1}^T P(x^{(t)}|x^{(t-1)},…,x^{(1)})<br>$$<br>其中$x^{(t)}$可以是词汇$V&#x3D;\lbrace w_1,…,w_{|V|}\rbrace$中的单词，最后的$P(x^{(t)}|x^{(t-1)},…,x^{(1)})$就是语言模型的概率表示。我们也可以将语言模型理解为对一段文本出现的概率做估计或推断。</p><h3 id="n-gram语言模型"><a href="#n-gram语言模型" class="headerlink" title="n-gram语言模型"></a>n-gram语言模型</h3><p>典型的对语言模型的学习可以通过学习一个n-gram语言模型来实现。一个n-gram是n个连续单词所形成的组(chunk)，如：</p><p><img src="/post/858fce89/image-20230223101802873.png" alt="image-20230223101802873"></p><p>对n-gram的出现频率做计算，可以有效的预测下一个单词是什么，也是思路比较简单的做法。n-gram模型基于马尔可夫独立假设：$x^{(t+1)}$的预测只基于前面的$n-1$个单词。所以语言模型的概率表示可以等价为：</p><p><img src="/post/858fce89/image-20230223103952303.png" alt="image-20230223103952303"></p><p><img src="/post/858fce89/image-20230223104322631.png" alt="image-20230223104322631"></p><p>在n-gram模型中，我们可以将概率问题用计数代替，而由于特定的n-gram的块在语料中出现频率极少甚至为0，其可能会导致模型的稀疏性，即绝大部分的概率为零，这会降低模型本身的效果。针对于此，在分子和分母上的稀疏性做简要改进：</p><ul><li>分子上：平滑技术，在每个单词$w\in V$的计数上加上一个小值$\delta$，防止其置0；</li><li>分母上：回退技术，将(n-1)-gram回退成(n-2)-gram,…直到分母不为零（unigram）。</li></ul><p><img src="/post/858fce89/image-20230223105726127.png" alt="image-20230223105726127"></p><p>在n-gram中，由于语言模型的概率基于n-gram的计数，因此模型需要存储在语料中的<strong>所有n-gram的数量，</strong>这个存储规模是很大的，而且增加n-gram的n和增加语料规模都会大幅增加存储。</p><p>n-gram的工作流程为，每次移动到一个n-gram块，获取下一次单词的概率分布并从中采样（采样结果可能不是概率最大的单词），逐个往下预测直到句子结束或遇到结束符。使用n-gram生成的文本一般在语法上有很惊人的效果，但在连贯性上有所缺点。</p><p><img src="/post/858fce89/image-20230223195826684.png" alt="image-20230223195826684"></p><p><img src="/post/858fce89/image-20230223110946985.png" alt="image-20230223110946985"></p><h3 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h3><h4 id="基于窗口的神经网络模型"><a href="#基于窗口的神经网络模型" class="headerlink" title="基于窗口的神经网络模型"></a>基于窗口的神经网络模型</h4><p>将n-gram的每个块作为一个窗口，放到基于窗口的神经网络模型中训练相对位置关系。</p><p><img src="/post/858fce89/image-20230223111645649.png" alt="image-20230223111645649"></p><p>固定窗口的网络模型可以解决n-gram稀疏的问题和存储的问题，但因为窗口固定，其固定涉及到的范围较小，且更改窗口大小需要一起扩大权重$W$的维度，且是平方级增大；在固定窗口中由于不同位置的权重不同，存在因位置导致的不对称的问题，$x^{(1)}$和$x^{(2)}$乘以完全不同的权重，输入的处理不对称。</p><p><img src="/post/858fce89/image-20230223155724007.png" alt="image-20230223155724007"></p><h3 id="循环神经网络RNN（Recurrent-Neural-Network）"><a href="#循环神经网络RNN（Recurrent-Neural-Network）" class="headerlink" title="循环神经网络RNN（Recurrent Neural Network）"></a>循环神经网络RNN（Recurrent Neural Network）</h3><p>一个简单的RNN语言模型架构图如下：</p><p><img src="/post/858fce89/image-20230223160155425.png" alt="image-20230223160155425"></p><p>单词离散向量经词嵌入之后，需要过一个从头到尾的一个模型，模型的每个隐状态由当前词和历史隐状态组成。</p><p>RNN大致解决了固定窗口神经网络的问题，但是由于速度和实际用起来的一些问题，简单RNN还需要有所优化。</p><p><img src="/post/858fce89/image-20230223160334272.png" alt="image-20230223160334272"></p><h4 id="RNN语言模型的训练"><a href="#RNN语言模型的训练" class="headerlink" title="RNN语言模型的训练"></a>RNN语言模型的训练</h4><p>给定一个大规模的语料，将其喂到RNN-LM中，在每个时间步$t$计算其计算输出分布，在第$t$步的损失函数是预测概率分布$\pmb {\hat y}^{(t)}$和真实的下一个单词$\pmb y^{(t)}$（$\pmb x^{(t+1)}$的one-hot向量）的交叉熵：<br>$$<br>J^{(t)}(\theta)&#x3D; CE( \pmb y^{(t)},\pmb {\hat y}^{(t)} ) &#x3D; -\sum_{ w\in V }\pmb y_w^{ (t) } \log \pmb { \hat y }_ w^{(t)} &#x3D; -\log \pmb{\hat y}_ {\pmb x_{ t+1} }^{(t)}<br>$$<br>模型的总体损失是对每个$t$的损失加和：<br>$$<br>J(\theta) &#x3D; {1 \over T}\sum_{t&#x3D;1}^T J^{(t)}(\theta) &#x3D; {1 \over T}\sum_{t&#x3D;1}^T -\log \pmb{ \hat y }_ {\pmb x_ {t+1} }^{(t)}<br>$$<br><img src="/post/858fce89/image-20230224205344605.png" alt="image-20230224205344605"></p><p>训练RNN的整理流程就是从左到右按照每个时刻$t$分别获得预测概率，计算与下一个单词的损失loss，最后其加和就是总的loss，需要注意的是：</p><ol><li>在实际训练中，句子的前后都会加上开始标记[start]和结束标记[end]</li><li>这里用到了Teacher Forcing的技巧，与Free-Running相对应，其训练方式有所不同：Free-Running不会干扰模型的训练过程，每次生成的单词作为下一步的单词输入；而Teacher Forcing每次的输入都是句子里对应位置的单词，而非模型生成，以防止模型因之前的错误影响后面的训练。</li></ol><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs txt">例如：Mary is watching you.输入[start]，模型训练得到第一个单词为&quot;a&quot;，原序列第一个单词为&quot;Mary&quot;。<br>Free-Running过程会直接将其作为下一步的输入，即下一步输入&quot;a&quot;;<br>Teacher-Forcing会记录当前的惩罚loss，并将原来的下一个单词作为输入，即下一步输入&quot;is&quot;。<br></code></pre></td></tr></table></figure><p>在计算RNN的损失和梯度时，每次都需要计算一次，在对整个语料一次性计算的过程，其时间和空间代价都十分的昂贵，实际上操作时，会将$\pmb x^{(1)},…,\pmb x^{(T)}$看作一组句子或一组文档，并参照随机梯度下降的思路，每次只随机的采用一个或一个batch的句子计算损失和梯度，更新参数，这样就可以降低RNN的计算难度。</p><p>以上的过程主要通过前向过程计算了模型的损失，同样的，模型也需要后向的过程以计算梯度。</p><p><img src="/post/858fce89/image-20230224212741199.png" alt="image-20230224212741199"></p><p>问题在于如何计算关于<strong>重复的</strong>权重矩阵$W_h$的偏导数$J^{(t)}(\theta)$，其答案和loss一样，同样是对每个时间步$i$的偏导做加和：<br>$$<br>{ {\partial J^{(t)} } \over {\partial \pmb W_h } }&#x3D; \sum_{i&#x3D;1}^t { {\partial J^{(t)} } \over {\partial \pmb W_h } } \bigg|_{(i)}<br>$$<br>整体梯度等于每个时间$i$的梯度加和，其推导过程就是链式法则中的分叉过程，对每段进行加和：</p><p><img src="/post/858fce89/image-20230224215758241.png" alt="image-20230224215758241"></p><p>而梯度的合理计算过程就是从模型最后的时刻$t$开始反向传播，每次传播计算一次梯度，将其加到总和中，反向传播到最开始，即可以计算出整体的梯度值。需要注意的是，实际上反向传播的过程中会应用<strong>截断</strong>的技巧，将反向传播的步数限制在~20，这样就可以更为有效的实现传播和梯度的计算，梯度的值也不会有过多的影响。</p><p><img src="/post/858fce89/image-20230224220030722.png" alt="image-20230224220030722"></p><h4 id="语言模型的评估"><a href="#语言模型的评估" class="headerlink" title="语言模型的评估"></a>语言模型的评估</h4><p>语言模型的标准评估指标是困惑度(perplexity)。</p><p><img src="/post/858fce89/image-20230225130306741.png" alt="image-20230225130306741"></p><p>困惑度的表达式和交叉熵损失$J(\theta)$的指数幂是一致的，越小的困惑度代表其表现越好。<br>$$<br>\prod_{t&#x3D;1}^T \left( {1 \over {\pmb {\hat y}_ {\pmb x_{t+1} }^{(t)} } }\right) ^{1&#x2F;T} &#x3D; \exp \left( {1\over T} \sum_{t&#x3D;1}^T-\log \pmb{\hat y}_ {\pmb x_{t+1}^{(t)} } \right) &#x3D; \exp(J(\theta))<br>$$<br>目前已有的模型中，LSTM在困惑度评估中的表现相对比较好，因为语言中有一些固定情况是没办法准确预测下一个单词的（如something指代的是什么），因此困惑度一般都会有20左右的起底。</p><p><img src="/post/858fce89/image-20230225132714269.png" alt="image-20230225132714269"></p><h3 id="梯度消失与梯度爆炸"><a href="#梯度消失与梯度爆炸" class="headerlink" title="梯度消失与梯度爆炸"></a>梯度消失与梯度爆炸</h3><h4 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h4><p>在反向传播的梯度计算中，当梯度的数值比较小时，随着梯度的传播，这些小值会通过乘积变得越来越小，以导致反向传播的后面的神经元，其接收到的梯度会及其小，过小的梯度会导致参数更新放缓甚至不更新，从而使长文本的更新失效。</p><p><img src="/post/858fce89/image-20230227161352840.png" alt="image-20230227161352840"></p><p><img src="/post/858fce89/image-20230227170222450.png" alt="image-20230227170222450"></p><p>梯度消失的简要证明如下：</p><p>首先，RNN的隐状态公式为：<br>$$<br>\pmb h^{(t)} &#x3D; \sigma\left(\pmb W_h\pmb h^{(t-1)}+\pmb W_x \pmb x^{(t)}+\pmb b_1 \right)<br>$$<br>考虑将非线性消去，即$\sigma(x)&#x3D;x$，则有<br>$$<br>\begin{align}<br>{ {\partial \pmb h^{(t)} } \over {\partial \pmb h^{(t-1)}} } &amp; &#x3D; {\rm diag} \left( \sigma’ \left(\pmb W_h\pmb h^{(t-1)}+\pmb W_x \pmb x^{(t)}+\pmb b_1 \right)\right )\pmb W_h \\ &amp; &#x3D; \pmb {I\ W}_ h &#x3D; \pmb W_ h<br>\end{align}<br>$$<br>考虑损失函数$J^{(i)}(\theta)$在第$i$步的梯度，对于在前面第$j$步的隐状态$\pmb h^{(j)}$，令$l &#x3D; i-j$<br>$$<br>\begin{align}<br>{ {\partial J^{(i)}(\theta)} \over {\partial \pmb h^{(j)}} } &amp; &#x3D; { {\partial J^{(i)}(\theta)} \over {\partial \pmb h^{(j)}} } \prod_{j\le t\le i} { {\partial \pmb h^{(t)}} \over {\partial \pmb h^{(t-1)}} } \\<br>&amp; &#x3D; { {\partial J^{(i)}(\theta)} \over {\partial \pmb h^{(i)}} } \prod_{j\le t\le i} \pmb W_h \\<br>&amp; &#x3D; { {\partial J^{(i)}(\theta)} \over {\partial \pmb h^{(i)}} } \pmb W_h^l<br>\end{align}<br>$$<br>如果$\pmb W_h$很小，则经过$l$次乘积之后，这个乘积会趋近于极小，从而导致梯度消失问题。这个“很小”的评价可以用特征值，考虑$\pmb W_h$的特征值全部都小于1（充分不必要条件），即有$\lambda_1,…,\lambda_n &lt; 1$，对应特征向量为$\pmb q_1,…,\pmb q_n$，我们可以将上面的结果写作特征表示：<br>$$<br>{ {\partial J^{(i)}(\theta)} \over {\partial \pmb h^{(i)}} } \pmb W_h^l &#x3D; \sum_{i&#x3D;1}^n c_i \lambda_i^l\pmb q_i \approx 0<br>$$<br>对于非线性映射$\sigma$，对上述的推理逻辑没有影响，除了需要证明$\lambda_i &lt; \gamma$。</p><h4 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h4><p>如果梯度变得过大，随机梯度下降的更新步会变得非常大，这可能会导致梯度摆荡甚至可能收敛到一个无效位置（如INF或NaN），进而导致网络收敛变慢或无法收敛的情况。<br>$$<br>\theta_{new}&#x3D; \theta_{old}-\alpha\nabla_{\theta}J(\theta)<br>$$<br>针对于梯度爆炸的问题，可以采取梯度截断的办法，设定一个梯度的阈值，当梯度超过既定的阈值则将其裁剪，相当于原方向继续走一步，但是走相对小的一步，防止梯度爆炸情况出现。</p><p><img src="/post/858fce89/image-20230227174320318.png" alt="image-20230227174320318"></p><p>实际上，梯度裁剪是一个十分重要的细节，但梯度爆炸是一个相对比较容易解决的问题。</p><h2 id="RNN生成文本"><a href="#RNN生成文本" class="headerlink" title="RNN生成文本"></a>RNN生成文本</h2><p>使用RNN的过程也可以应用到文本生成的任务中，具体来说，从开始标记<code>&lt;s&gt;</code>训练，每个timestep会生成下一个单词的概率分布，对每个timestep做一次sample，然后将sample的结果放到下一个timestep训练，直至遇到结束标记<code>&lt;/s&gt;</code>，将生成的单词连成句子，就大致实现了文本的生成过程。使用这种过程就可以大致实现风格化的文本生成，如生成一个哈利波特风格的文本段落等等，还蛮有趣的。</p><p><img src="/post/858fce89/image-20230224222100023.png" alt="image-20230224222100023"></p><p><img src="/post/858fce89/image-20230224222745535.png" alt="image-20230224222745535"></p><p><img src="/post/858fce89/image-20230224223820258.png" alt="image-20230224223820258"></p><h2 id="RNN的应用"><a href="#RNN的应用" class="headerlink" title="RNN的应用"></a>RNN的应用</h2><p><img src="/post/858fce89/image-20230227180339522.png" alt="image-20230227180339522"></p><p><img src="/post/858fce89/image-20230227180407007.png" alt="image-20230227180407007"></p><p><img src="/post/858fce89/image-20230227180421040.png" alt="image-20230227180421040"></p><p><img src="/post/858fce89/image-20230227180433386.png" alt="image-20230227180433386"></p><h1 id="课后问题"><a href="#课后问题" class="headerlink" title="课后问题"></a>课后问题</h1>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo博客实现搜索引擎优化SEO</title>
      <link href="/post/d3496c78.html"/>
      <url>/post/d3496c78.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="hexo博客实现搜索引擎优化SEO"><a href="#hexo博客实现搜索引擎优化SEO" class="headerlink" title="hexo博客实现搜索引擎优化SEO"></a>hexo博客实现搜索引擎优化SEO</h1><p>hexo搭建完成之后会发现其被搜索引擎爬到的效率很低，为了能提高一点点效率，故搞一下SEO优化，主要是弄必应和谷歌的优化，百度这个站点因为一些众所周知的问题比较难搞，故这次暂时不搞。</p><h2 id="1、生成sitemap文件"><a href="#1、生成sitemap文件" class="headerlink" title="1、生成sitemap文件"></a>1、生成sitemap文件</h2><p>站点地图可以方便各大搜索引擎快速收录文章。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">npm install hexo-generator-sitemap --save<br>npm install hexo-generator-baidu-sitemap --save<br></code></pre></td></tr></table></figure><p>在站点配置文件<code>_config.yml</code>中加入站点地图的配置。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">sitemap:</span> <br>  <span class="hljs-attr">path:</span> <span class="hljs-string">sitemap.xml</span><br>  <span class="hljs-attr">rel:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-comment">#关闭tag和category的地图配置，提高收录效率</span><br>  <span class="hljs-attr">tags:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">categories:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">baidusitemap:</span><br>  <span class="hljs-attr">path:</span> <span class="hljs-string">baidusitemap.xml</span><br>  <span class="hljs-attr">tags:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">categories:</span> <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure><h2 id="2、加入robots文件"><a href="#2、加入robots文件" class="headerlink" title="2、加入robots文件"></a>2、加入robots文件</h2><p>在站点的source文件夹下新建robots.txt文件，文件内容如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs txt">User-agent: *<br>Allow: /<br>Allow: /posts/<br>Disallow: /archives/<br>Disallow: /images/<br>Disallow: /categories/<br>Disallow: /tags/ <br>Disallow: /about/<br>Disallow: /links/<br>Disallow: /message/<br>Disallow: /download/<br><br>Sitemap: https://paopao0226.site/sitemap.xml<br>Sitemap: https://paopao0226.site/baidusitemap.xml<br></code></pre></td></tr></table></figure><p>其中Allow和Disallow字段分别对应于搜索引擎可爬取和不可爬取的内容，其中的配置需要自行修改；Sitemap对应于上面的两个站点地图文件，其中站点url需要自己修改。</p><h2 id="3、提交站点到搜索引擎"><a href="#3、提交站点到搜索引擎" class="headerlink" title="3、提交站点到搜索引擎"></a>3、提交站点到搜索引擎</h2><h3 id="3-1-google收录"><a href="#3-1-google收录" class="headerlink" title="3.1 google收录"></a>3.1 google收录</h3><h4 id="1、google-search-console"><a href="#1、google-search-console" class="headerlink" title="1、google search console"></a>1、google search console</h4><p>进入console主界面，选择服务（左边的是全域优化，右边的是url），我这里选择的是左边的优化，输入自己的域名（如<code>paopao0226.site</code>）。</p><p><img src="/post/d3496c78/image-20230222144019085.png" alt="image-20230222144019085"></p><p>系统会指引你继续进行DNS配置，将给定的txt复制一下。</p><p><img src="/post/d3496c78/image-20230222144208608.png" alt="image-20230222144208608"></p><p>再进入你自己域名的DNS服务器，将这条信息配置进去，我这里使用的是<code>Netlify DNS</code>，内容一致。</p><p><img src="/post/d3496c78/image-20230222144335095.png" alt="image-20230222144335095"></p><p>稍微等待一会，点击DNS Verify就可以看到验证成功，如果失败的话可以等过一天再看，如果还没成功的话就是DNS配置有误或者DNS服务器找错了（我就找错了一次）</p><p><img src="/post/d3496c78/image-20230222143819255.png" alt="image-20230222143819255"></p><p>这样就完成了网站的推流，进入管理台后将之前生成的sitemap.xml提交，就可以实现搜索引擎的引导收录了。</p>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SEO </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N课程P4：依存分析</title>
      <link href="/post/d4a7d8fc.html"/>
      <url>/post/d4a7d8fc.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="课程词汇"><a href="#课程词汇" class="headerlink" title="课程词汇"></a>课程词汇</h1><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs txt">nested - 嵌套的；巢状的<br>constituent - 成分；选民；委托人<br>lexicon - 词汇；词典<br>preposition - 介词<br>compress - 压缩；<br>isomorphisms - 同构；<br>interpret - 解释；解析<br>taxonomy - 分类学；分类法<br>acyclic - 无环的<br>syntax - 语法；句法<br>postulates - 假设<br>coverage - 报道；覆盖范围<br>rigorously - 严格的；严密的<br>plausible - 似乎是真的；貌似有理的<br>intervening - 中间的<br>parsing - 解析<br>intermediate - 中间的<br>detour - 绕道；迂回<br></code></pre></td></tr></table></figure><h1 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h1><h3 id="Two-views-of-linguistic-structure"><a href="#Two-views-of-linguistic-structure" class="headerlink" title="Two views of linguistic structure"></a>Two views of linguistic structure</h3><ul><li><strong>Constituency &#x3D; phrase structure &amp; grammar &#x3D; context-free grammars (CFGs)</strong></li><li><strong>Dependency structure</strong></li></ul><h2 id="成分语法和上下文无关语法"><a href="#成分语法和上下文无关语法" class="headerlink" title="成分语法和上下文无关语法"></a>成分语法和上下文无关语法</h2><p>上下文无关语法使用短语作为非终结符，句子是由短语作为基本单位组成，短语可以嵌套，终结符是最小的短语（基本上是单词），含嵌套的短语作为变元，其可以一直拆解到终结符；通过变元不断组成产生式，句子可以被表示出来。</p><p><img src="/post/d4a7d8fc/image-20230211013642113.png" alt="image-20230211013642113"></p><p>示例如下：</p><p><img src="/post/d4a7d8fc/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xlZTMyNTg=,size_16,color_FFFFFF,t_70-16760506212073.png" alt="在这里插入图片描述"></p><h2 id="依存语法的引入"><a href="#依存语法的引入" class="headerlink" title="依存语法的引入"></a>依存语法的引入</h2><p>依存结构显示了哪些单词**依赖于(修饰、附加到或是其他单词的参数)**哪些单词； 依赖语法提供了一种更简单的方法，即描述词之间的关系，并确定几个中心词作为句子的主体结构，由其他成分依赖中心词。</p><p>如下图所示，整个结构是由依赖关系组成，其中介词in是名词crate的一个依赖，这个是由于依存文法需要保证其在大部分的语言系统中能够使用，因此其应用了格标记系统，将[in the crate]看作一个标记（这里暂时没弄懂，记住罢）。</p><p><img src="/post/d4a7d8fc/image-20230211014034985.png" alt="image-20230211014034985"></p><p>正规的标记系统示例如下：</p><p><img src="/post/d4a7d8fc/20210619154108552.png" alt="img"></p><h2 id="语言歧义"><a href="#语言歧义" class="headerlink" title="语言歧义"></a>语言歧义</h2><h3 id="介词短语连接的歧义"><a href="#介词短语连接的歧义" class="headerlink" title="介词短语连接的歧义"></a>介词短语连接的歧义</h3><p>在英语中，由于介词短语位置的问题，其修饰对象的不同会导致句子出现多种理解，进而导致二义性甚至多义性。</p><p>如下面的句子，其中句子后面的[by … ]&#x2F;[of … ]&#x2F;[for … ]&#x2F;[at … ]都是介词短语，英语句法遵循后面的介词短语修饰前面的成分，这个句子的<strong>修饰可能性遵循卡特兰数</strong>，人类语言的句子其实是十分模糊的，这里的$n&#x3D;4$。</p><p>卡特兰数是一个指数增长模型，因此句子的解析难度会随着长度而指数增加。</p><p><img src="/post/d4a7d8fc/image-20230212001946174.png" alt="image-20230212001946174"></p><h3 id="协调范围模糊"><a href="#协调范围模糊" class="headerlink" title="协调范围模糊"></a>协调范围模糊</h3><blockquote><p><strong>[Shuttle veteran]</strong> and <strong>[longtime NASA executive Fred Gregory]</strong> appointed to board —— 2 people</p><p><strong>[Shuttle veteran and longtime NASA executive]</strong> Fred Gregory appointed to board —— 1 people</p></blockquote><h3 id="形容词-x2F-副词修饰歧义"><a href="#形容词-x2F-副词修饰歧义" class="headerlink" title="形容词&#x2F;副词修饰歧义"></a>形容词&#x2F;副词修饰歧义</h3><blockquote><p>Students get <strong>[first hand]</strong> <strong>[job]</strong> experience</p><p>Students get <strong>[first]</strong> <strong>[hand job]</strong> experience</p></blockquote><h2 id="依存语法和依存结构"><a href="#依存语法和依存结构" class="headerlink" title="依存语法和依存结构"></a>依存语法和依存结构</h2><p>依存语法假定句法结构由词汇项之间的关系组成，通常为二元非对称关系(“箭头”)，称为依赖关系。在依存结构中，箭头通常会标记成语法关系的类型（如aux&#x2F;obl&#x2F;…），每个箭头连接一个起始头和依赖项，这些依赖关系会构成一个连通的、无环的、单根的依赖树。</p><p><img src="/post/d4a7d8fc/image-20230212005757314.png" alt="image-20230212005757314"></p><p>箭头的指向方式习惯是由头部指向依赖项，通常我们也会加入一个root点作为假的根节点，从而保证所有词语都有一个树中的父节点。</p><p><img src="/post/d4a7d8fc/image-20230212011246317.png" alt="image-20230212011246317"></p><h3 id="含注释的标注数据"><a href="#含注释的标注数据" class="headerlink" title="含注释的标注数据"></a>含注释的标注数据</h3><p><strong>从一开始，构建 treebank 似乎比构建语法慢得多，也没有那么有用</strong></p><p>但其会带来许多东西，有以下优点：</p><ul><li>标签的可重用性</li><li>广泛的应用范围，不只是直觉上的应用</li><li>使用频率和可分配信息</li><li>评估NLP系统的一种方法或指标</li></ul><h2 id="依存分析的首选项"><a href="#依存分析的首选项" class="headerlink" title="依存分析的首选项"></a>依存分析的首选项</h2><ul><li>双词汇亲和（Bilexical affinities）：discussing issues会比较合理，而discussing outstanding就不合理；</li><li>依存间距（Dependency distance）：一般相邻的词语才会有依存关系</li><li>中间词语（Intervening material）：如果中间词语是动词或标点，则两边的词语不太可能有依存</li><li>关系头的效价（Valency of Heads）：不同词性的词在作为关系首部时有自己习惯的依存关系，如限定词（Determiner）一般都会在名词（N）左边，如a cat，反过来则很少成立。</li></ul><p><img src="/post/d4a7d8fc/image-20230212231316436.png" alt="image-20230212231316436"></p><h2 id="依存分析的过程"><a href="#依存分析的过程" class="headerlink" title="依存分析的过程"></a>依存分析的过程</h2><p>依存分析的过程是：一个句子通过对每个单词选择一个头部（包括假根），使其能够成为一个依存关系（A sentence is parsed by choosing for each word what other word (including ROOT) it is a dependent of）</p><p>在依存分析的过程中需要注意以下要求：</p><ol><li>只有一个单词可以是ROOT的依赖项</li><li>不可出现依赖循环（A-&gt;B &amp;&amp; B-&gt;A）</li></ol><p>这可以使得依存分析的结果形成一棵依存分析<strong>树</strong>。</p><p><img src="/post/d4a7d8fc/image-20230212234835080.png" alt="image-20230212234835080"></p><h2 id="投影问题"><a href="#投影问题" class="headerlink" title="投影问题"></a>投影问题</h2><p>在上面的解析过程中，我们可以发现give-&gt;tomorrow和networks-&gt;talk两个关系出现了交叉，<strong>出现交叉的解析结果我们称之为非投影的</strong>，反义则为可投影。</p><p>投影解析的定义如下：当单词以线性顺序排列时，没有交叉依赖弧，所有的弧都在单词之上（There are no crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words）</p><p>与CFG树并行的依赖树必须是可投影的，通过将每个类别的一个子类别作为头来形成依赖关系。但依赖理论通常会允许非投射结构，用来解释移位的成分，如果没有这些非投射的依赖关系，部分结构的语义会很难获得或理解</p><p>例如：</p><p><img src="/post/d4a7d8fc/image-20230213014653529.png" alt="image-20230213014653529"></p><p>在这句话中，from放在了who的后面，形成了非投影关系，但这句话若没有交叉依赖的话，则会表示成：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs txt">who did Bill buy the coffee from yesterday?<br> ||<br> \/<br>from who did Bill buy the coffee yesterday?<br></code></pre></td></tr></table></figure><p>可投影的依赖表示在这句英语中会很难表达其结构的语义，from who的结构在英语问句中十分少见，因此需要对非投射结构进行兼容。</p><h2 id="依存分析的方法"><a href="#依存分析的方法" class="headerlink" title="依存分析的方法"></a>依存分析的方法</h2><ol><li>动态规划（Dynamic programming）：Eisner(1996)提出了一种复杂度为 O(n3) 的聪明算法，它生成头部位于末尾而不是中间的解析项；</li><li>图算法：为一个句子创建一个<strong>最小生成树</strong>McDonald et al.’s (2005) MSTParser 使用ML分类器独立地对依赖项进行评分(他使用MIRA进行在线学习，但它也可以是其他东西)；</li><li>约束解析（Constraint Satisfaction）：去掉不满足硬约束的边 Karlsson(1990), etc.</li><li>基于转换的分析或确定性依赖解析（“Transition-based parsing” or “deterministic dependency parsing”） ：良好的机器学习分类器 MaltParser(Nivreet al. 2008) 指导下的依存贪婪选择。已证明非常有效。</li></ol><h2 id="基于转换的依存分析模型"><a href="#基于转换的依存分析模型" class="headerlink" title="基于转换的依存分析模型"></a>基于转换的依存分析模型</h2><p>源于论文[<strong>Greedy transition-based parsing</strong> [Nivre 2003]]的解析器是贪婪判别依赖解析器的一种简单形式，解析器执行一系列自底向上的操作，大致类似于shift-reduce解析器中的“shift”或“reduce”，但“reduce”操作专门用于创建头在左或右的依赖项（这块我不是太清楚）。</p><p>解析器由以下几部分构成：</p><ul><li>栈$\sigma$：以ROOT符号开始，由若干个单词$w_i$组成；</li><li>缓存$\beta$：以输入序列开始，由若干个单词$w_i$组成；</li><li>一个依存弧的集合$A$，一开始为空，每条边的形式为$(w_i,r,w_j)$，其中$r$描述了节点的依存关系；</li><li>一组操作。</li></ul><p>下面给出一个基本的基于转换的依存解析器的例子：</p><p><img src="/post/d4a7d8fc/image-20230213215501551.png" alt="image-20230213215501551"></p><p>具体的，shift操作会将下一个单词取出，放到栈$\sigma$中；左弧和右弧操作都需要将栈顶的两个单词取出，其中左弧操作和右弧操作的成弧指向不同，放入集合$A$的弧也有所不同，且余留在栈中的单词也不同，但一定是为首选项的那个。</p><p>下面给出一个句子并完成其依存分析的过程：（Analysis of “I ate fish”），注意这只是其中一种转换方案，在这里算是正确的方案，可选择的转换方案其实有很多种。</p><p><img src="/post/d4a7d8fc/image-20230213224723915.png" alt="image-20230213224723915"></p><p><img src="/post/d4a7d8fc/image-20230213221436485.png" alt="image-20230213221436485"></p><p>源于<strong>MaltParser</strong> [Nivre and Hall 2005]的解析器在预测下一步动作上有所进步，使用了机器学习的有区别分类器来预测下一步的动作：最多三种无类型的选择，当带有类型时最多有$|\mathcal R|\times 2+1$种选择。分类器的特征包括：栈顶单词,POS;缓存中第一个单词,POS;等等。</p><p>在最简单的形式中是没有搜索的，但是，如果你愿意，你可以有效地执行一个 Beam search 束搜索(虽然速度较慢，但效果更好)：你可以在每个时间步骤中保留$k$个好的解析前缀。该模型的精度略低于依赖解析的最高水平，但它提供了非常快的<strong>线性时间解析</strong>，性能非常好。</p><h2 id="传统特征表示"><a href="#传统特征表示" class="headerlink" title="传统特征表示"></a>传统特征表示</h2><p>在依存解析器中，传统的特征表示使用二元的稀疏向量表示特征，特征模板通常由配置中的1-3个元素组成，由于配置中的元素组合可能性有极多，因此稀疏向量的维度一般在$10^6-10^7$的规模。<img src="/post/d4a7d8fc/image-20230213223709314.png" alt="image-20230213223709314"></p><h2 id="依存分析的评估：两个指标"><a href="#依存分析的评估：两个指标" class="headerlink" title="依存分析的评估：两个指标"></a>依存分析的评估：两个指标</h2><p>在依存分析的评估中，常用的有UAS(Unlabeled Accuracy Score)和LAS(Labeled Accuracy Score)两种：</p><p><img src="/post/d4a7d8fc/image-20230213224250272.png" alt="image-20230213224250272"></p><p>如图所示，首先句子从ROOT开始标记序号，然后分别写出&lt;序号、依赖首选项序号、单词、关系类型&gt;，最后和正确结果做比对，UAS不关注关系类型是否预测正确，只关注依赖弧是否正确；而LAS则一同关注依赖弧和关系类型。</p><h2 id="神经网络依存分析器"><a href="#神经网络依存分析器" class="headerlink" title="神经网络依存分析器"></a>神经网络依存分析器</h2><p>在前文讲到的依存分析的指示特征中，特征的向量规模达到了$10^6-10^7$的等级，过于稀疏，且特征的出现依赖于固定词汇出现与否，具有不完全性；多于95%的时间都用在了特征计算上，这显然不是我们希望看到的，因此依存分析也可以和word2vec一样引入稠密向量，向量的距离可以表示其相似程度。</p><p>源于论文[<strong>A neural dependency parser</strong> [Chen and Manning 2014]]我们将每个词都表示成$d$维稠密向量，相似的词汇有相近的距离；同时，对于单词词性（POS)和依存关系标签也都表示为$d$维向量。</p><p><img src="/post/d4a7d8fc/image-20230220150121555.png" alt="image-20230220150121555"></p><p>我们根据堆栈&#x2F;缓冲区位置提取一组Token：</p><p><img src="/post/d4a7d8fc/image-20230220150248032.png" alt="image-20230220150248032"></p><p>其中$s_i$表示Stack中的第i个单词，$b_i$表示缓冲区的第$i$个单词，$lc(·)$和$rc(·)$分别代表左弧和右弧两个操作，和之前的依存解析器是同一种表示，所有这些向量表示的连接就是一个构型的神经表示。</p><p>其次，在稠密向量的神经网络解析器中，深度学习分类器是一个非线性的分类器，顶层使用softmax分类器，底层结合多层神经网络，将原训练空间通过矩阵乘法进行变换，实现非线性。</p><p><img src="/post/d4a7d8fc/image-20230220154120736.png" alt="image-20230220154120736"></p><p><img src="/post/d4a7d8fc/image-20230220154132077.png" alt="image-20230220154132077"></p><p>一个简单的前馈神经网络多类别分类器大致如下：</p><p><img src="/post/d4a7d8fc/image-20230220152853981.png" alt="image-20230220152853981"></p><p>其中包括以下几个部分：</p><ul><li>输入层$x$：词汇的稠密向量</li><li>隐藏层$h$：非线性变换（如ReLU，可能有n个），适度降低输入维度<br>非线性的方法有很多，这里举了比较典型的ReLU做例子。</li><li>输出层$y$：softmax分类、</li></ul><p>在密集的输入层之下，实际上还有一层转换层，用于将含词语词性等特征的独热编码向量转换为稠密向量，这个过程可以理解为矩阵乘法变换。</p><h1 id="课后问题"><a href="#课后问题" class="headerlink" title="课后问题"></a>课后问题</h1><p>本节课暂无。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N课程P3：反向传播和神经网络</title>
      <link href="/post/169c01c6.html"/>
      <url>/post/169c01c6.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="课程词汇"><a href="#课程词汇" class="headerlink" title="课程词汇"></a>课程词汇</h1><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs txt">concatenation - 连接；<br>hairier - 更糟；<br>interpretation - 解释；理解；<br>scalar - 标量；<br>intuitive - 直觉的；直观的<br>arbitrary - 任意的；任何的<br>symbolic - 符号；<br></code></pre></td></tr></table></figure><h1 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h1><h2 id="用神经网络实现NER（命名实体识别）"><a href="#用神经网络实现NER（命名实体识别）" class="headerlink" title="用神经网络实现NER（命名实体识别）"></a>用神经网络实现NER（命名实体识别）</h2><p><img src="/post/169c01c6/image-20230208232225382.png" alt="image-20230208232225382"></p><p>主要逻辑是通过人工标注的数据训练逻辑分类器，根据<strong>窗口中</strong>单词向量的连接对<strong>每个类</strong>的<strong>中心单词</strong>{yes&#x2F;no}进行分类</p><p><img src="/post/169c01c6/image-20230208232152448.png" alt="image-20230208232152448"></p><p>具体过程大致可以分为：</p><ol><li>设置窗口，将每个窗口的单词组成矩阵，如窗口为$+2,-2$，则$x\in \mathbb R^{5d},d\ is\ the\ num\ of\  words$；</li><li>经过神经网络层，先线性计算后非线性映射</li><li>和向量$u^\top$相乘，整合特征</li><li>过一个softmax，表示成概率，这个概率代表中心词对应于特定label的概率。</li></ol><p><img src="/post/169c01c6/image-20230208235337710.png" alt="image-20230208235337710"></p><p><img src="/post/169c01c6/image-20230209000741788.png" alt="image-20230209000741788"></p><h2 id="矩阵求导"><a href="#矩阵求导" class="headerlink" title="矩阵求导"></a>矩阵求导</h2><p><strong>遵循惯例：导数的形状是参数的形状 （形状约定）</strong></p><h3 id="行向量"><a href="#行向量" class="headerlink" title="行向量"></a>行向量</h3><p>给定一个模型，n个输入，1个输出：$f(\pmb x)&#x3D;f(x_1,x_2,…,x_n)$。</p><p>求导结果为：${ {\partial f} \over {\partial \pmb x} } &#x3D; [{ {\partial f} \over {\partial x_1} },{ {\partial f} \over {\partial x_2} },…,{ {\partial f} \over {\partial x_n} }]$</p><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><p>给定一个模型，n个输入，m个输出：$f(\pmb x)&#x3D;[f_1(x_1,x_2,…,x_n),f_2(x_1,x_2,…,x_n),…,f_m(x_1,x_2,…,x_n)]$。</p><p>求导结果为Jacobi行列式<br>$$<br>\begin{bmatrix}<br>{ {\partial f_1} \over {\partial x_1} }&amp;…&amp;{ {\partial f_1} \over {\partial x_n} }\\<br>\vdots&amp;\ddots&amp;\vdots\\<br>{ {\partial f_m} \over {\partial x_1} }&amp;…&amp;{ {\partial f_m} \over {\partial x_n} }\\<br>\end{bmatrix}<br>$$</p><h3 id="含向量的链式法则"><a href="#含向量的链式法则" class="headerlink" title="含向量的链式法则"></a>含向量的链式法则</h3><p>给定一个模型，n个输入，n个输出：$\pmb h&#x3D;f(\pmb z)$</p><p>求导结果：<br>$$<br>{ {\partial \pmb h} \over {\partial \pmb z} } &#x3D;\begin{bmatrix}<br>f’(z_i)&amp;0&amp;0 \\<br>0 &amp; \ddots &amp; 0 \\<br>0 &amp; 0 &amp; f’(z_n)\\<br>\end{bmatrix}<br>$$<br>其中，$({ {\partial \pmb h} \over {\partial \pmb z} })_{ij} &#x3D; { {\partial} \over {\partial z_j} }f(z_i) &#x3D; \begin{cases} f’(z_i) \ \ \ \ i &#x3D; j \\ 0\ \ \ \ otherwise \end{cases}  $</p><h3 id="其他Jacobi式求导"><a href="#其他Jacobi式求导" class="headerlink" title="其他Jacobi式求导"></a>其他Jacobi式求导</h3><p>$$<br>\begin{align}<br>{ {\partial } \over {\partial \pmb x} }(\pmb {Wx} + \pmb b)&amp; &#x3D; \pmb W \\<br>{ {\partial } \over {\partial \pmb b} }(\pmb {Wx} + \pmb b)&amp; &#x3D; \pmb I \\<br>{ {\partial } \over {\partial \pmb u} }(\pmb u^\top\pmb h)&amp; &#x3D; \pmb h^\top \\<br>\end{align}<br>$$</p><h3 id="反向传播的引入"><a href="#反向传播的引入" class="headerlink" title="反向传播的引入"></a>反向传播的引入</h3><p>在计算${ {\partial s} \over {\partial \pmb b } }$和${ {\partial s} \over {\partial \pmb W } }$时，链式法则中前面的非线性求导的部分是一样的，引入$\delta$来保存这个变量：</p><p><img src="/post/169c01c6/image-20230209153346535.png" alt="image-20230209153346535"></p><p>$\delta$被称为<strong>局部误差信号</strong>，从高层传送到低层，用于简化计算。</p><h3 id="整体求导"><a href="#整体求导" class="headerlink" title="整体求导"></a>整体求导</h3><p><img src="/post/169c01c6/image-20230209153042201.png" alt="image-20230209153042201"></p><p><img src="/post/169c01c6/image-20230209162135454.png" alt="image-20230209162135454"></p><p>根据链式法则，${ {\partial s} \over {\partial \pmb W } }&#x3D;\delta $ ${ {\partial \pmb z} \over {\partial \pmb W } }$，经形状推导之后的结果为</p><p><img src="/post/169c01c6/image-20230209163438592.png" alt="image-20230209163438592"></p><p><img src="/post/169c01c6/image-20230209163110330.png" alt="image-20230209163110330"></p><p>相似的，${ {\partial s} \over {\partial \pmb b } } &#x3D; \pmb h^\top \circ f’(\pmb z)$的结果是一个行向量，但<strong>形状约定</strong>要求我们的梯度应该是一个列向量，因为$\pmb b$是列向量。所以，在Jacobian形式（使链式法则容易计算）和形状约定中（使优化方法SGD容易计算）是有冲突的。为了解决这个问题有两种办法：</p><ol><li>先用Jacobian形式完成矩阵计算，最后重塑形状到形状约定形式（如${ {\partial s} \over {\partial \pmb b } }$）</li><li>始终遵循形状约定计算结果（如${ {\partial s} \over {\partial \pmb W } }$）</li></ol><p><img src="/post/169c01c6/image-20230209164233479.png" alt="image-20230209164233479"></p><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h3 id="计算图和反向传播"><a href="#计算图和反向传播" class="headerlink" title="计算图和反向传播"></a>计算图和反向传播</h3><p>前文提到了，在相同层的参数有许多重复的计算，可以使用一个$\delta$变量简化梯度计算过程，反向传播就是这个思想的具体实现。</p><p>具体的，引入神经网络的计算图，首先对神经网络执行前向过程，计算出损失与结果；然后从最后面的节点往前回溯，反向计算各个节点的梯度。</p><p><img src="/post/169c01c6/image-20230210002344337.png" alt="image-20230210002344337"></p><p>对计算图中除首节点的每个结点，都可以执行反向传播的过程，从上游传回梯度，和节点的局部梯度计算，再将结果作为下游梯度继续回传。</p><p><img src="/post/169c01c6/image-20230210002837907.png" alt="image-20230210002837907"></p><p>在计算图中具体是有三种类型：</p><ol><li><p>单输入单输出：如上图所示</p></li><li><p>多输入单输出：对输入的每一个岔路都做一遍反向传播</p></li><li><p>单输入多输出：将多个上游的结果做加和<br>$$<br>{ {\partial f} \over {\partial y} } &#x3D;{ {\partial f} \over {\partial a} }{ {\partial a} \over {\partial y} }+{ {\partial f} \over {\partial b} }{ {\partial b} \over {\partial y} }<br>$$</p></li></ol><p>反向传播的示例如下，需要注意的是，每次计算梯度时，我们应用的应该是回传到节点的中间值，上游梯度，也称$\delta$，而不是重新从结尾计算梯度。</p><p><img src="/post/169c01c6/image-20230210003239986.png" alt="image-20230210003239986"></p><p>对上图计算图中的节点作直观理解：</p><ul><li>‘+’ distributes the upstream gradient</li><li>‘max’ routes the upstream gradient</li><li>‘*’ switches the upstream gradient</li></ul><h3 id="反向传播的一般模型"><a href="#反向传播的一般模型" class="headerlink" title="反向传播的一般模型"></a>反向传播的一般模型</h3><p>反向传播的一般模型也遵循着先前向后反向的过程，需要注意的是输出分叉的加和。</p><p><img src="/post/169c01c6/image-20230210003837804.png" alt="image-20230210003837804"></p><p>反向传播的计算过程理论上可以通过计算机自主实现，其实际也是在主流深度学习框架中已经有所应用，上述的自动计算过程被称为<strong>自动微分</strong>。</p><h3 id="反向传播的实现"><a href="#反向传播的实现" class="headerlink" title="反向传播的实现"></a>反向传播的实现</h3><ul><li>首先实现计算图，并定义整体逻辑（前向和反向）<img src="/post/169c01c6/image-20230210004158959.png" alt="image-20230210004158959"></li><li>然后定义门控节点的前向和反向过程<img src="/post/169c01c6/image-20230210004321092.png" alt="image-20230210004321092"></li></ul><h3 id="梯度计算的数值检查"><a href="#梯度计算的数值检查" class="headerlink" title="梯度计算的数值检查"></a>梯度计算的数值检查</h3><p>梯度计算的数值检查可以通过手动计算函数斜率的方式实现，这种方式的近似过程十分缓慢，需要对参数逐个作计算，但是一种十分有效的方法。</p><p><img src="/post/169c01c6/image-20230210004500222.png" alt="image-20230210004500222"></p><h1 id="课后问题"><a href="#课后问题" class="headerlink" title="课后问题"></a>课后问题</h1><p>本讲暂无</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nndl编程练习3：logistic回归和softmax回归练习题解</title>
      <link href="/post/58d83785.html"/>
      <url>/post/58d83785.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h1><h3 id="生成数据集，-看明白即可无需填写代码"><a href="#生成数据集，-看明白即可无需填写代码" class="headerlink" title="生成数据集， 看明白即可无需填写代码"></a>生成数据集， 看明白即可无需填写代码</h3><h4 id="‘-‘-从高斯分布采样-X-Y-N-3-6-1-1-0"><a href="#‘-‘-从高斯分布采样-X-Y-N-3-6-1-1-0" class="headerlink" title="‘+‘ 从高斯分布采样 (X, Y) ~ N(3, 6, 1, 1, 0)."></a>‘<font color="blue">+</font>‘ 从高斯分布采样 (X, Y) ~ N(3, 6, 1, 1, 0).<br></h4><h4 id="‘o‘-从高斯分布采样-X-Y-N-6-3-1-1-0"><a href="#‘o‘-从高斯分布采样-X-Y-N-6-3-1-1-0" class="headerlink" title="‘o‘ 从高斯分布采样  (X, Y) ~ N(6, 3, 1, 1, 0)"></a>‘<font color="green">o</font>‘ 从高斯分布采样  (X, Y) ~ N(6, 3, 1, 1, 0)<br></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> animation, rc<br><span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> HTML<br><span class="hljs-keyword">import</span> matplotlib.cm <span class="hljs-keyword">as</span> cm<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>%matplotlib inline<br><br>dot_num = <span class="hljs-number">100</span><br>x_p = np.random.normal(<span class="hljs-number">3.</span>, <span class="hljs-number">1</span>, dot_num)<br>y_p = np.random.normal(<span class="hljs-number">6.</span>, <span class="hljs-number">1</span>, dot_num)<br><span class="hljs-comment">#标签为1</span><br>y = np.ones(dot_num)<br>C1 = np.array([x_p, y_p, y]).T<br><br>x_n = np.random.normal(<span class="hljs-number">6.</span>, <span class="hljs-number">1</span>, dot_num)<br>y_n = np.random.normal(<span class="hljs-number">3.</span>, <span class="hljs-number">1</span>, dot_num)<br><span class="hljs-comment">#标签为0</span><br>y = np.zeros(dot_num)<br>C2 = np.array([x_n, y_n, y]).T<br><br>plt.scatter(C1[:, <span class="hljs-number">0</span>], C1[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;b&#x27;</span>, marker=<span class="hljs-string">&#x27;+&#x27;</span>)<br>plt.scatter(C2[:, <span class="hljs-number">0</span>], C2[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;g&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br><br>data_set = np.concatenate((C1, C2), axis=<span class="hljs-number">0</span>)<br>np.random.shuffle(data_set)<br><br></code></pre></td></tr></table></figure><p><img src="/post/58d83785/image-20230206235625055.png" alt="image-20230206235625055"></p><h2 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h2><p>建立模型类，定义loss函数，定义一步梯度下降过程函数</p><p>填空一：实现sigmoid的交叉熵损失函数(不使用tf内置的loss函数)</p><p>logistic线性模型的损失函数为交叉熵损失<br>$$<br>\mathcal L(\theta) &#x3D; -{1\over N}\sum_{n&#x3D;1}^N (y^{(n)} \log\hat y^{(n)}+(1-y^{(n)})\log (1-\hat y^{(n)}))<br>$$<br>在这里解代码在预测值后面加上了一个极小量$\epsilon&#x3D;10^{-12}$，这个的意义我暂时还不太知晓。</p><blockquote><p>这里的代码做了两个改动，分别对应于两个报错：</p><ol><li><p>TypeError: Input ‘b’ of ‘MatMul’ Op has type float32 that does not match type float64 of argument ‘a’.<br>解决办法：在报错的行，用tf.cast作强制转型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#原代码</span><br><span class="hljs-comment">#logits = tf.matmul(inp, self.W) + self.b # shape(N, 1)</span><br><span class="hljs-comment">#修改后代码</span><br>logits = tf.matmul(tf.cast(inp,tf.float32), self.W) + self.b <span class="hljs-comment"># shape(N, 1)</span><br></code></pre></td></tr></table></figure></li><li><p>TypeError: Expected float32, but got Tensor(“label:0”, shape&#x3D;(), dtype&#x3D;float64) of type ‘Tensor’.<br>   解决办法：去掉@tf.function，防止强制转换float为tensor<br>   见代码注释</p></li></ol></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python">epsilon = <span class="hljs-number">1e-12</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LogisticRegression</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.W = tf.Variable(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">1</span>], dtype=tf.float32, <br>            initial_value=tf.random.uniform(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">1</span>], minval=-<span class="hljs-number">0.1</span>, maxval=<span class="hljs-number">0.1</span>))<br>        self.b = tf.Variable(shape=[<span class="hljs-number">1</span>], dtype=tf.float32, initial_value=tf.zeros(shape=[<span class="hljs-number">1</span>]))<br>        <br>        self.trainable_variables = [self.W, self.b]<br>    <span class="hljs-comment"># delete @tf.function</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, inp</span>):<br>        <span class="hljs-comment"># 增加tf.cast转换数据格式</span><br>        logits = tf.matmul(tf.cast(inp,tf.float32), self.W) + self.b <span class="hljs-comment"># shape(N, 1)</span><br>        pred = tf.nn.sigmoid(logits)<br>        <span class="hljs-keyword">return</span> pred<br><span class="hljs-comment"># delete @tf.function</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">pred, label</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(label, tf.Tensor):<br>        label = tf.constant(label, dtype=tf.float32)<br>    pred = tf.squeeze(pred, axis=<span class="hljs-number">1</span>)<br>    <span class="hljs-string">&#x27;&#x27;&#x27;=============================&#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment">#输入label shape(N,), pred shape(N,)</span><br>    <span class="hljs-comment">#输出 losses shape(N,) 每一个样本一个loss</span><br>    <span class="hljs-comment">#todo 填空一，实现sigmoid的交叉熵损失函数(不使用tf内置的loss 函数)</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;=============================&#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment">#</span><br>    losses = - label*tf.math.log(pred+epsilon) - (<span class="hljs-number">1.</span>-label)*tf.math.log(<span class="hljs-number">1.</span>-pred+epsilon)<br>    loss = tf.reduce_mean(losses)<br>    <br>    pred = tf.where(pred&gt;<span class="hljs-number">0.5</span>, tf.ones_like(pred), tf.zeros_like(pred))<br>    accuracy = tf.reduce_mean(tf.cast(tf.equal(label, pred), dtype=tf.float32))<br>    <span class="hljs-keyword">return</span> loss, accuracy<br><span class="hljs-comment"># delete @tf.function</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_one_step</span>(<span class="hljs-params">model, optimizer, x, y</span>):<br>    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:<br>        pred = model(x)<br>        loss, accuracy = compute_loss(pred, y)<br>        <br>    grads = tape.gradient(loss, model.trainable_variables)<br>    optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(grads, model.trainable_variables))<br>    <span class="hljs-keyword">return</span> loss, accuracy, model.W, model.b<br></code></pre></td></tr></table></figure><h3 id="实例化一个模型，进行训练"><a href="#实例化一个模型，进行训练" class="headerlink" title="实例化一个模型，进行训练"></a>实例化一个模型，进行训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    model = LogisticRegression()<br>    opt = tf.keras.optimizers.SGD(learning_rate=<span class="hljs-number">0.01</span>)<br>    x1, x2, y = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(*data_set))<br>    x = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(x1, x2))<br>    animation_fram = []<br>    <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">200</span>):<br>        loss, accuracy, W_opt, b_opt = train_one_step(model, opt, x, y)<br>        animation_fram.append((W_opt.numpy()[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], W_opt.numpy()[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], b_opt.numpy(), loss.numpy()))<br>        <span class="hljs-keyword">if</span> i%<span class="hljs-number">20</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;loss: <span class="hljs-subst">&#123;loss.numpy():<span class="hljs-number">.4</span>&#125;</span>\t accuracy: <span class="hljs-subst">&#123;accuracy.numpy():<span class="hljs-number">.4</span>&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="/post/58d83785/image-20230207010103726.png" alt="image-20230207010103726"></p><h2 id="结果展示，无需填写代码"><a href="#结果展示，无需填写代码" class="headerlink" title="结果展示，无需填写代码"></a>结果展示，无需填写代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python">f, ax = plt.subplots(figsize=(<span class="hljs-number">6</span>,<span class="hljs-number">4</span>))<br>f.suptitle(<span class="hljs-string">&#x27;Logistic Regression Example&#x27;</span>, fontsize=<span class="hljs-number">15</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Y&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;X&#x27;</span>)<br>ax.set_xlim(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>)<br>ax.set_ylim(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>)<br><br>line_d, = ax.plot([], [], label=<span class="hljs-string">&#x27;fit_line&#x27;</span>)<br>C1_dots, = ax.plot([], [], <span class="hljs-string">&#x27;+&#x27;</span>, c=<span class="hljs-string">&#x27;b&#x27;</span>, label=<span class="hljs-string">&#x27;actual_dots&#x27;</span>)<br>C2_dots, = ax.plot([], [], <span class="hljs-string">&#x27;o&#x27;</span>, c=<span class="hljs-string">&#x27;g&#x27;</span> ,label=<span class="hljs-string">&#x27;actual_dots&#x27;</span>)<br><br><br>frame_text = ax.text(<span class="hljs-number">0.02</span>, <span class="hljs-number">0.95</span>,<span class="hljs-string">&#x27;&#x27;</span>,horizontalalignment=<span class="hljs-string">&#x27;left&#x27;</span>,verticalalignment=<span class="hljs-string">&#x27;top&#x27;</span>, transform=ax.transAxes)<br><span class="hljs-comment"># ax.legend()</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init</span>():<br>    line_d.set_data([],[])<br>    C1_dots.set_data([],[])<br>    C2_dots.set_data([],[])<br>    <span class="hljs-keyword">return</span> (line_d,) + (C1_dots,) + (C2_dots,)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">animate</span>(<span class="hljs-params">i</span>):<br>    xx = np.arange(<span class="hljs-number">10</span>, step=<span class="hljs-number">0.1</span>)<br>    a = animation_fram[i][<span class="hljs-number">0</span>]<br>    b = animation_fram[i][<span class="hljs-number">1</span>]<br>    c = animation_fram[i][<span class="hljs-number">2</span>]<br>    yy = a/-b * xx +c/-b<br>    line_d.set_data(xx, yy)<br>        <br>    C1_dots.set_data(C1[:, <span class="hljs-number">0</span>], C1[:, <span class="hljs-number">1</span>])<br>    C2_dots.set_data(C2[:, <span class="hljs-number">0</span>], C2[:, <span class="hljs-number">1</span>])<br>    <br>    frame_text.set_text(<span class="hljs-string">&#x27;Timestep = %.1d/%.1d\nLoss = %.3f&#x27;</span> % (i, <span class="hljs-built_in">len</span>(animation_fram), animation_fram[i][<span class="hljs-number">3</span>]))<br>    <br>    <span class="hljs-keyword">return</span> (line_d,) + (C1_dots,) + (C2_dots,)<br><span class="hljs-comment">#FuncAnimation函数绘制动图，f是画布，animate是自定义动画函数，init_func自定义开始帧，即传入init初始化函数，</span><br><span class="hljs-comment">#frames动画长度，一次循环包含的帧数，在函数运行时，其值会传递给函数animate(i)的形参“i”，interval更新频率，以ms计，blit选择更新所有点，还是仅更新产生变化的点。</span><br>anim = animation.FuncAnimation(f, animate, init_func=init,<br>                               frames=<span class="hljs-built_in">len</span>(animation_fram), interval=<span class="hljs-number">30</span>, blit=<span class="hljs-literal">True</span>)<br><br>HTML(anim.to_html5_video())<br></code></pre></td></tr></table></figure><p><video src="data:video/mp4;base64,AAAAIGZ0eXBNNFYgAAACAE00ViBpc29taXNvMmF2YzEAAAAIZnJlZQABEP1tZGF0AAACoQYF//+d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" position="absolute" width="80%" height="80%" controls="controls" align="center"></video></p><p><img src="/post/58d83785/image-20230207010259507.png" alt="image-20230207010259507"></p><h1 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h1><h3 id="生成数据集，-看明白即可无需填写代码-1"><a href="#生成数据集，-看明白即可无需填写代码-1" class="headerlink" title="生成数据集， 看明白即可无需填写代码"></a>生成数据集， 看明白即可无需填写代码</h3><h4 id="‘-‘-从高斯分布采样-X-Y-N-3-6-1-1-0-1"><a href="#‘-‘-从高斯分布采样-X-Y-N-3-6-1-1-0-1" class="headerlink" title="‘+‘ 从高斯分布采样 (X, Y) ~ N(3, 6, 1, 1, 0)."></a>‘<font color="blue">+</font>‘ 从高斯分布采样 (X, Y) ~ N(3, 6, 1, 1, 0).<br></h4><h4 id="‘o‘-从高斯分布采样-X-Y-N-6-3-1-1-0-1"><a href="#‘o‘-从高斯分布采样-X-Y-N-6-3-1-1-0-1" class="headerlink" title="‘o‘ 从高斯分布采样  (X, Y) ~ N(6, 3, 1, 1, 0)"></a>‘<font color="green">o</font>‘ 从高斯分布采样  (X, Y) ~ N(6, 3, 1, 1, 0)<br></h4><h4 id="‘-‘-从高斯分布采样-X-Y-N-7-7-1-1-0"><a href="#‘-‘-从高斯分布采样-X-Y-N-7-7-1-1-0" class="headerlink" title="‘*‘ 从高斯分布采样  (X, Y) ~ N(7, 7, 1, 1, 0)"></a>‘<font color="red">*</font>‘ 从高斯分布采样  (X, Y) ~ N(7, 7, 1, 1, 0)<br></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> animation, rc<br><span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> HTML<br><span class="hljs-keyword">import</span> matplotlib.cm <span class="hljs-keyword">as</span> cm<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>%matplotlib inline<br><br>dot_num = <span class="hljs-number">100</span><br>x_p = np.random.normal(<span class="hljs-number">3.</span>, <span class="hljs-number">1</span>, dot_num)<br>y_p = np.random.normal(<span class="hljs-number">6.</span>, <span class="hljs-number">1</span>, dot_num)<br><span class="hljs-comment">#标签为1</span><br>y = np.ones(dot_num)<br>C1 = np.array([x_p, y_p, y]).T<br><br>x_n = np.random.normal(<span class="hljs-number">6.</span>, <span class="hljs-number">1</span>, dot_num)<br>y_n = np.random.normal(<span class="hljs-number">3.</span>, <span class="hljs-number">1</span>, dot_num)<br><span class="hljs-comment">#标签为0</span><br>y = np.zeros(dot_num)<br>C2 = np.array([x_n, y_n, y]).T<br><br>x_b = np.random.normal(<span class="hljs-number">7.</span>, <span class="hljs-number">1</span>, dot_num)<br>y_b = np.random.normal(<span class="hljs-number">7.</span>, <span class="hljs-number">1</span>, dot_num)<br><span class="hljs-comment">#标签为2</span><br>y = np.ones(dot_num)*<span class="hljs-number">2</span><br>C3 = np.array([x_b, y_b, y]).T<br><span class="hljs-comment">#画出来</span><br>plt.scatter(C1[:, <span class="hljs-number">0</span>], C1[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;b&#x27;</span>, marker=<span class="hljs-string">&#x27;+&#x27;</span>)<br>plt.scatter(C2[:, <span class="hljs-number">0</span>], C2[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;g&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br>plt.scatter(C3[:, <span class="hljs-number">0</span>], C3[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;r&#x27;</span>, marker=<span class="hljs-string">&#x27;*&#x27;</span>)<br><br>data_set = np.concatenate((C1, C2, C3), axis=<span class="hljs-number">0</span>)<br>np.random.shuffle(data_set)<br><span class="hljs-built_in">print</span>(data_set)<br><br></code></pre></td></tr></table></figure><p><img src="/post/58d83785/image-20230207012314335.png" alt="image-20230207012314335"></p><h2 id="建立模型-1"><a href="#建立模型-1" class="headerlink" title="建立模型"></a>建立模型</h2><p>建立模型类，定义loss函数，定义一步梯度下降过程函数</p><p>填空一：在<code>__init__</code>构造函数中建立模型所需的参数</p><p>填空二：实现softmax的交叉熵损失函数(不使用tf内置的loss函数)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python">epsilon = <span class="hljs-number">1e-12</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SoftmaxRegression</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;=============================&#x27;&#x27;&#x27;</span><br>        <span class="hljs-comment">#todo 填空一，构建模型所需的参数 self.W, self.b 可以参考logistic-regression-exercise</span><br>        <span class="hljs-string">&#x27;&#x27;&#x27;=============================&#x27;&#x27;&#x27;</span><br>        self.W = tf.Variable(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=tf.float32, <br>            initial_value=tf.random.uniform(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], minval=-<span class="hljs-number">0.1</span>, maxval=<span class="hljs-number">0.1</span>))<br>        self.b = tf.Variable(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>], dtype=tf.float32, initial_value=tf.zeros(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>]))<br>        self.trainable_variables = [self.W, self.b]<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, inp</span>):<br>        logits = tf.matmul(tf.cast(inp,tf.float32), self.W) + self.b <span class="hljs-comment"># shape(N, 3)</span><br>        pred = tf.nn.softmax(logits)<br>        <span class="hljs-keyword">return</span> pred    <br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">pred, label</span>):<br>    label = tf.one_hot(tf.cast(label, dtype=tf.int32), dtype=tf.float32, depth=<span class="hljs-number">3</span>)<br>    <span class="hljs-string">&#x27;&#x27;&#x27;=============================&#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment">#输入label shape(N, 3), pred shape(N, 3)</span><br>    <span class="hljs-comment">#输出 losses shape(N,) 每一个样本一个loss</span><br>    <span class="hljs-comment">#todo 填空二，实现softmax的交叉熵损失函数(不使用tf内置的loss 函数)</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;=============================&#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment">#对三个标签的损失求平均</span><br>    losses = - tf.reduce_mean(label * tf.math.log(pred+epsilon))<br>    <span class="hljs-comment">#求总平均</span><br>    loss = tf.reduce_mean(losses) <br>    <br>    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(label,axis=<span class="hljs-number">1</span>), tf.argmax(pred, axis=<span class="hljs-number">1</span>)), dtype=tf.float32))<br>    <span class="hljs-keyword">return</span> loss, accuracy<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_one_step</span>(<span class="hljs-params">model, optimizer, x, y</span>):<br>    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:<br>        pred = model(x)<br>        loss, accuracy = compute_loss(pred, y)<br>        <br>    grads = tape.gradient(loss, model.trainable_variables)<br>    optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(grads, model.trainable_variables))<br>    <span class="hljs-keyword">return</span> loss, accuracy<br></code></pre></td></tr></table></figure><h3 id="实例化一个模型，进行训练-1"><a href="#实例化一个模型，进行训练-1" class="headerlink" title="实例化一个模型，进行训练"></a>实例化一个模型，进行训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">model = SoftmaxRegression()<br>opt = tf.keras.optimizers.SGD(learning_rate=<span class="hljs-number">0.01</span>)<br>x1, x2, y = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(*data_set))<br>x = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(x1, x2))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>):<br>    loss, accuracy = train_one_step(model, opt, x, y)<br>    <span class="hljs-keyword">if</span> i%<span class="hljs-number">50</span>==<span class="hljs-number">49</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;loss: <span class="hljs-subst">&#123;loss.numpy():<span class="hljs-number">.4</span>&#125;</span>\t accuracy: <span class="hljs-subst">&#123;accuracy.numpy():<span class="hljs-number">.4</span>&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="结果展示，无需填写代码-1"><a href="#结果展示，无需填写代码-1" class="headerlink" title="结果展示，无需填写代码"></a>结果展示，无需填写代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(C1[:, <span class="hljs-number">0</span>], C1[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;b&#x27;</span>, marker=<span class="hljs-string">&#x27;+&#x27;</span>)<br>plt.scatter(C2[:, <span class="hljs-number">0</span>], C2[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;g&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br>plt.scatter(C3[:, <span class="hljs-number">0</span>], C3[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;r&#x27;</span>, marker=<span class="hljs-string">&#x27;*&#x27;</span>)<br><br>x = np.arange(<span class="hljs-number">0.</span>, <span class="hljs-number">10.</span>, <span class="hljs-number">0.1</span>)<br>y = np.arange(<span class="hljs-number">0.</span>, <span class="hljs-number">10.</span>, <span class="hljs-number">0.1</span>)<br><br>X, Y = np.meshgrid(x, y)<br>inp = np.array(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(X.reshape(-<span class="hljs-number">1</span>), Y.reshape(-<span class="hljs-number">1</span>))), dtype=np.float32)<br><span class="hljs-built_in">print</span>(inp.shape)<br>Z = model(inp)<br>Z = np.argmax(Z, axis=<span class="hljs-number">1</span>)<br>Z = Z.reshape(X.shape)<br>plt.contour(X,Y,Z)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/post/58d83785/image-20230207012520185.png" alt="image-20230207012520185"></p><h1 id="附：对logistic回归的复现"><a href="#附：对logistic回归的复现" class="headerlink" title="附：对logistic回归的复现"></a>附：对logistic回归的复现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> animation, rc<br><span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> HTML<br><span class="hljs-keyword">import</span> matplotlib.cm <span class="hljs-keyword">as</span> cm<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>dot_num = <span class="hljs-number">100</span><br>x_p = np.random.normal(<span class="hljs-number">3.</span>, <span class="hljs-number">1</span>, dot_num)<br>y_p = np.random.normal(<span class="hljs-number">6.</span>, <span class="hljs-number">1</span>, dot_num)<br>y = np.ones(dot_num)<br>C1 = np.array([x_p, y_p, y]).T<br><br>x_n = np.random.normal(<span class="hljs-number">6.</span>, <span class="hljs-number">1</span>, dot_num)<br>y_n = np.random.normal(<span class="hljs-number">3.</span>, <span class="hljs-number">1</span>, dot_num)<br>y = np.zeros(dot_num)<br>C2 = np.array([x_n, y_n, y]).T<br><br>plt.scatter(C1[:, <span class="hljs-number">0</span>], C1[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;b&#x27;</span>, marker=<span class="hljs-string">&#x27;+&#x27;</span>)<br>plt.scatter(C2[:, <span class="hljs-number">0</span>], C2[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;g&#x27;</span>, marker=<span class="hljs-string">&#x27;o&#x27;</span>)<br><br>data_set = np.concatenate((C1, C2), axis=<span class="hljs-number">0</span>)<br>np.random.shuffle(data_set)<br><br><span class="hljs-comment">#logistic模型包括模型，损失函数，迭代函数三个部分</span><br>epsilon = <span class="hljs-number">1e-12</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LogisticRegression</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment">#tf.Variable：定义tf中的变量.</span><br>        <span class="hljs-comment">#tf.random.uniform：从均匀分布中输出随机值.</span><br>        self.W = tf.Variable(shape=[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>],dtype=<span class="hljs-string">&#x27;float32&#x27;</span>,initial_value=tf.random.uniform(shape=[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>],maxval=<span class="hljs-number">0.1</span>,minval=-<span class="hljs-number">0.1</span>))<br>        self.b = tf.Variable(shape=[<span class="hljs-number">1</span>],dtype=<span class="hljs-string">&#x27;float32&#x27;</span>,initial_value=tf.random.uniform(shape=[<span class="hljs-number">1</span>],maxval=<span class="hljs-number">0.1</span>,minval=-<span class="hljs-number">0.1</span>))<br>        <span class="hljs-comment">#自动求导的关键步骤</span><br>        self.trainable_variables = [self.W,self.b]<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self,inp</span>):<br>        logits = tf.matmul(tf.cast(inp,tf.float32),self.W)+self.b<br>        pred = tf.nn.sigmoid(logits)<br>        <span class="hljs-keyword">return</span> pred<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">pred,label</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(label, tf.Tensor):<br>        label = tf.constant(label, dtype=tf.float32)<br>    <span class="hljs-comment">#tf.squeeze：去掉维度为1的维度</span><br>    pred = tf.squeeze(pred,axis=<span class="hljs-number">1</span>)<br>    losses = -label*tf.math.log(pred+epsilon)-(<span class="hljs-number">1.</span>-label)*tf.math.log(<span class="hljs-number">1.</span>-pred+epsilon)<br>    loss = tf.reduce_mean(losses)<br>    <span class="hljs-comment">#tf.where：确定表达式为真的位置</span><br>    pred = tf.where(pred&gt;<span class="hljs-number">0.5</span>,tf.ones_like(pred),tf.zeros_like(pred))<br>    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,label),dtype=<span class="hljs-string">&#x27;float32&#x27;</span>))<br>    <span class="hljs-keyword">return</span> loss,accuracy<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_one_step</span>(<span class="hljs-params">model,optimizer,x,y</span>):<br>    <span class="hljs-comment"># tf自动求导API</span><br>    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:<br>        pred = model(x)<br>        loss,accuracy = compute_loss(pred,y)<br>    <span class="hljs-comment">#自动求导</span><br>    <span class="hljs-comment">#先求导</span><br>    gradient = tape.gradient(loss,model.trainable_variables)<br>    <span class="hljs-comment">#后优化</span><br>    optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(gradient,model.trainable_variables))<br>    <span class="hljs-keyword">return</span> loss,accuracy,model.W,model.b<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    alpha = <span class="hljs-number">0.01</span><br>    epoch = <span class="hljs-number">200</span><br>    model = LogisticRegression()<br>    opt = tf.keras.optimizers.SGD(learning_rate=alpha)<br>    <span class="hljs-comment">#zip(*data_set)相当于解压操作，(x1,x2,y)中的每一维都提取出来</span><br>    x1,x2,y = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(*data_set))<br>    x = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(x1,x2))<br>    animation_fram = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>        loss,accuracy,W,b = train_one_step(model,opt,x,y)<br>        animation_fram.append((W.numpy()[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], W.numpy()[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], b.numpy(), loss.numpy()))<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">20</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;loss: <span class="hljs-subst">&#123;loss.numpy():<span class="hljs-number">.4</span>&#125;</span>\t accuracy: <span class="hljs-subst">&#123;accuracy.numpy():<span class="hljs-number">.4</span>&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N课程P2：词向量与神经网络分类器</title>
      <link href="/post/f552d4fb.html"/>
      <url>/post/f552d4fb.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="课程词汇"><a href="#课程词汇" class="headerlink" title="课程词汇"></a>课程词汇</h1><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs txt">crude - 粗糙的；<br>retain - 保存；保留<br>ratios - 比例；比率<br>semantic - 语义<br>syntactic - 句法<br>ambiguity - 歧义；不一致<br></code></pre></td></tr></table></figure><h1 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h1><h2 id="分布式词向量：Word2Vec"><a href="#分布式词向量：Word2Vec" class="headerlink" title="分布式词向量：Word2Vec"></a>分布式词向量：Word2Vec</h2><h3 id="数据设置"><a href="#数据设置" class="headerlink" title="数据设置"></a>数据设置</h3><p><strong>词袋模型</strong>实际上是不关心词序或位置的模型，无论上下文词离中心词远或近，其概率估计结果是相同的。word2vec为实现对上下文的高概率，其需要将相似的词汇放到词汇空间中十分接近的位置。</p><h3 id="学习优化：梯度下降"><a href="#学习优化：梯度下降" class="headerlink" title="学习优化：梯度下降"></a>学习优化：梯度下降</h3><p>$$<br>\theta_{new} &#x3D; \theta_{old}-\alpha\triangledown_{\theta} J(\theta)<br>$$</p><p>因为$J(\theta)$在大规模的中心词存在的情况下使用梯度下降，效率很低，因此采用随机梯度下降(SGD)代替全数据梯度下降，提高性能。随机梯度下降每次更新采用一个或一批数据（单词）进行更新。</p><p><img src="/post/f552d4fb/image-20230205012216219.png" alt="image-20230205012216219"></p><p>单词用向量表示使用<strong>行向量</strong>。</p><p>在随机梯度下降的过程中，由于每次更新时只用到$2m+1(m为窗口大小)$的单词，如果有负采样则加上$2km(k为负采样次数)$的单词数量，这与所有单词总数相比十分的小，因此计算出来的梯度$\triangledown_\theta J_t(\theta)$十分的稀疏。</p><p><img src="/post/f552d4fb/image-20230205013013592.png" alt="image-20230205013013592"></p><p>我们有时只需要更新实际上出现过的单词，大部分单词不需要更新，解决方法是需要稀疏矩阵更新操作，只更新全嵌入矩阵U和V的某些行；或者你需要为词向量保留一个哈希值。</p><h3 id="Skip-gram-负采样"><a href="#Skip-gram-负采样" class="headerlink" title="Skip-gram+负采样"></a>Skip-gram+负采样</h3><p>使用两个向量的原因是方便优化计算，只有一个向量确实会帮助提升效率，但当中心词和上下文词出现相同<strong>词类型</strong>的时候，这种$x x^\top$的关系会影响到优化，造成混乱。</p><p>两种模型：</p><p><img src="/post/f552d4fb/image-20230205013907507.png" alt="image-20230205013907507"></p><p>在skip-gram模型中，对于朴素的softmax函数的概率，其分母的计算要求十分昂贵，需要遍历一遍所有的词汇，因此引入负采样。</p><p>负采样的思想在于将分母去掉，以中心词和外部词的点积表示其和外部的相似度，一般来说，取样后产生的点积应该越小越好，以防止对上下文词的影响。<strong>我们将中心词和上下文词的点积概率最大化，而把中心词和外部词的点积概率最小化。</strong>这个思想和softmax函数的思路是一致的，增大分子而缩小分母。<br>$$<br>J_{neg-sample}(\pmb u_o,\pmb v_c,U) &#x3D; -\log \sigma(\pmb u_o^\top\pmb v_c) - \sum_{k\in \lbrace K sampled\  indices \rbrace} \log \sigma(-\pmb u_k^\top \pmb v_c)<br>$$<br><img src="/post/f552d4fb/image-20230205161118518.png" alt="image-20230205161118518"></p><p>其中$\sigma(·)$是$logistic$函数，将数据压缩到0-1，加上负号的原因是最小化损失函数（习惯问题）。对于采样的选择，论文中采用了$P(w)&#x3D;{ U(w)^{3\over 4} \over Z}$作为对k个外部单词采样的概率分布，其中$U(w)$是单词的原数量分布，加上${3\over 4}$将频用词和鲜用词的相差概率压缩，但不至于到均匀分布的程度，然后除以$Z$概率还原到$0-1$。</p><h2 id="计数共现矩阵"><a href="#计数共现矩阵" class="headerlink" title="计数共现矩阵"></a>计数共现矩阵</h2><h3 id="共现矩阵"><a href="#共现矩阵" class="headerlink" title="共现矩阵"></a>共现矩阵</h3><p>将单词表现为共现矩阵的一行或一列，这样可以保存单词之间的共现关系，直接训练相邻的单词。</p><p><img src="/post/f552d4fb/image-20230205161904711.png" alt="image-20230205161904711"></p><p>共现矩阵的实现分为两种，一是确定窗口的单词共现矩阵，二是使用段落、单元等自有单位的文档-单词矩阵。</p><p><img src="/post/f552d4fb/image-20230205224740097.png" alt="image-20230205224740097"></p><h3 id="维度降低"><a href="#维度降低" class="headerlink" title="维度降低"></a>维度降低</h3><p>我们可以在固定的少量维度中存储有关单词分布和其他单词上下文的<strong>大部分</strong>重要信息，从而实现低维向量的表示。具体的这里需要用到奇异值分解的基本知识：</p><h4 id="奇异值分解（SVD）"><a href="#奇异值分解（SVD）" class="headerlink" title="奇异值分解（SVD）"></a>奇异值分解（SVD）</h4><blockquote><p>参考自<a href="https://zhuanlan.zhihu.com/p/29846048">漫漫成长-奇异值分解（SVD）</a></p></blockquote><p>一个$n \times n$的矩阵$A$可以特征分解为$n$个特征值$\lambda_1\le \lambda_2 \le … \le \lambda_n$，以及这$n$个特征值所对应的特征向量$w_1,w_2,…,w_n$，那么矩阵$A$就可以用特征式来表示：<br>$$<br>A &#x3D; W \Sigma W^{-1}<br>$$<br>其中$W$是这$n$个特征向量所张成的$n \times n$维矩阵，$\Sigma$是$n$个特征值为主对角线的$n \times n$维矩阵。一般我们会把W的这n个特征向量标准化，此时$W$的$n$个特征向量为标准正交基，满足$W^{\top}W&#x3D;I$，即$W^\top &#x3D; W^{-1}$，这样特征分解表达式可以写成：<br>$$<br>A &#x3D; W\Sigma W^\top<br>$$<br>如果$A$不是方阵，则需要引入奇异值分解，假设$A$是$m\times n$的矩阵，分解时分解成下列形式：<br>$$<br>A &#x3D; U \Sigma V^\top<br>$$<br>其中$U$是$m\times m$的矩阵；$\Sigma$是一个$m\times n$的矩阵，除主对角线上的元素外其余元素均为0，主对角线上的每个元素都称为奇异值；$V$是一个$n\times n$的矩阵。$U$和$V$都满足标准正交的性质。</p><p><img src="/post/f552d4fb/image-20230205181916489.png" alt="image-20230205181916489"></p><p>对三个部分的求解，请参考原文，在这里不再赘述。</p><h4 id="奇异值在降维的应用"><a href="#奇异值在降维的应用" class="headerlink" title="奇异值在降维的应用"></a>奇异值在降维的应用</h4><p>将奇异值分解应用到降维部分，可以用几个低维矩阵最大程度还原原始矩阵。</p><p><img src="/post/f552d4fb/image-20230205182430858.png" alt="image-20230205182430858"></p><p>其中，黄框中的内容是在主对角线中没有用到的，因此被压缩掉；为了减少尺度同时尽量保存有效信息，可保留对角矩阵的最大的$k$个值，其余置零，并将酉矩阵的相应的行列保留，其余置零，如蓝框内的部分。再然后我们使用$U$的行来作为字典中所有词的词向量。</p><p><img src="/post/f552d4fb/image-20230205233249862.png" alt="image-20230205233249862"></p><p>单纯使用SVD的效果并不太好，因此也有一些优化改进。</p><p><img src="/post/f552d4fb/image-20230205183535161.png" alt="image-20230205183535161"></p><h2 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h2><blockquote><p>参考自<a href="https://zhuanlan.zhihu.com/p/60208480">CS224N笔记(二)：GloVe</a></p></blockquote><p>GloVe模型解决了如何在词向量空间中捕捉共现概率比例作为线性意义的成分，共现矩阵中<strong>概率的比例</strong>可以编码<strong>词的意义成分</strong>（如：男人到女人、动物到植物等）。这也为之后能够对这些线性意义成分进行计算提供基础。</p><p>GloVe的核心在于使用词向量表达共现概率比值，而任意一个这样的比值需要三个词i、j和k的词向量。令i为单词ice，j为单词steam，论文中的一组比较可以体现这种共现性：</p><p><img src="/post/f552d4fb/image-20230205225027785.png" alt="image-20230205225027785"></p><p>例如对于solid固态，虽然$P(solid|ice)$与$P(solid|steam)$本身很小，不能透露有效的信息，但是它们的比值${ {P(solid|ice)} \over {P(solid|steam)} }$却较大，因为solid更常用来描述ice的状态而不是steam的状态，所以在ice的上下文中出现几率较大，对于gas则恰恰相反，而对于water这种描述ice与steam均可或者fashion这种与两者都没什么联系的单词，则比值接近于1。</p><p>定义一些符号：对于矩阵$X$，$X_{ij}$代表单词$j$出现在单词$i$上下文中的次数，则$X_i&#x3D;\sum_k X_{ik}$代表所有出现在单词$i$的上下文中的单词次数。我们用$P_{ij}&#x3D;P(j|i)&#x3D;X_{ij}&#x2F;X_i$表示单词$j$出现在单词$i$上下文中的概率。</p><p>基于对上面概率的观察，假设模型的函数有以下形式：<br>$$<br>F(w_i,w_j,\tilde{w_k})<br>$$<br>其中$\tilde w$代表背景词，如上例中的solid,gas等；$w_i,w_j$代表需要比较的两个词汇，如上例中的ice,steam。$F$的可选形式过多，因此论文作者在这里做出了限定：首先我们希望的是$F$能有效的在单词向量空间内表示概率比值，由于向量空间是线性空间，一个自然的假设是$F$是关于向量$w_i,w_j$的差的形式：<br>$$<br>F(w_i-w_j,\tilde w_k) &#x3D; { {P_{ik}  } \over {P_{jk} } }<br>$$<br>等式右边为标量形式，左边如何操作能将矢量转化为标量形式呢？一个自然的选择是矢量的点乘形式：<br>$$<br>F((w_i-w_j)^\top\tilde w_k) &#x3D; { {P_{ik}  } \over {P_{jk} } }<br>$$<br>又因为对称性，即对于单词-单词 共现矩阵，将向量划分为center word还是context word的选择是不重要的，即我们在交换$w\leftrightarrow \tilde w$和$X \leftrightarrow X^\top$的时候该式仍然成立。这里分两步进行，首先要求$F((w_i-w_j)^\top\tilde w_k) &#x3D; { {F(w_i^\top \tilde w_k)} \over {F(w_j^\top \tilde w_k)} }$，该方程解为$F&#x3D;exp$；同时与$F((w_i-w_j)^\top\tilde w_k) &#x3D; { {P_{ik}  } \over {P_{jk} } }$相比较有$F(w_i^\top \tilde w_k)&#x3D;P_{ik}&#x3D;{ {X_{ik} } \over {X_i} }$，故有<br>$$<br>w_i^\top \tilde w_k&#x3D;\log (P_{ik})&#x3D;\log(P(i|k)) &#x3D; \log(X_{ik}) - \log (X_i)<br>$$<br>这里$\log(X_i)$破坏了对称性，但这一项并不依赖于$k$，因此可以将其融合进关于$w_i$的偏置项$b_i$；同时为了平衡对称性（这里不太懂，可能纯粹是形式对称），再加入$\tilde w_k$的偏置项$\tilde b_k$，得到GloVe的损失函数如下，其中$f(·)$用于滤掉过高频出现的词语，如虚词副词等等。<br>$$<br>Loss:J&#x3D;\sum_{i,j&#x3D;1}^Vf(X_{ij})(w_i^\top w_j+b_i+b_j-\log X_{ij})^2<br>$$<br><img src="/post/f552d4fb/image-20230205232255937.png" alt="image-20230205232255937"></p><h2 id="评估过程"><a href="#评估过程" class="headerlink" title="评估过程"></a>评估过程</h2><p>包括内在评估和外在评估。内在评估通过设置与词向量任务相关的子任务来测试词向量的效果，外在评估直接设置真实项目来予以实验。</p><p><img src="/post/f552d4fb/image-20230205232645979.png" alt="image-20230205232645979"></p><h3 id="内在评估"><a href="#内在评估" class="headerlink" title="内在评估"></a>内在评估</h3><p>内在评估中，现有的评估大多针对于向量本身的评估，以及单词间相似性的评估。对于向量计算本身的评估，可以通过评估其向量计算后的单词类比来评估，即man-&gt;woman : king-&gt; ?这种类比过程能否通过向量计算成功类比。</p><p><img src="/post/f552d4fb/image-20230206005325593.png" alt="image-20230206005325593"></p><p>对于单词意义相似性的评估，现有人类已经有了相似单词定量化的数据集，因此可以通过比较词向量距离及其与人类判断的相关性来实现内在评估。</p><p><img src="/post/f552d4fb/image-20230206005623937.png" alt="image-20230206005623937"></p><h3 id="外在评估"><a href="#外在评估" class="headerlink" title="外在评估"></a>外在评估</h3><p>外在评估中，可以通过比较不同的词向量来看到词向量带来的收益。如下图中提到的命名实体识别的真实任务。</p><p><img src="/post/f552d4fb/image-20230206010503495.png" alt="image-20230206010503495"></p><h1 id="课后问题"><a href="#课后问题" class="headerlink" title="课后问题"></a>课后问题</h1><h4 id="Q：为什么Skip-gram在数据统计的使用效率上比较低？"><a href="#Q：为什么Skip-gram在数据统计的使用效率上比较低？" class="headerlink" title="Q：为什么Skip-gram在数据统计的使用效率上比较低？"></a>Q：为什么Skip-gram在数据统计的使用效率上比较低？</h4><p>A：因为Skip-gram每次只操作一个中心词，需要对每个中心词都做一次处理，而共现矩阵这样的操作可以直接操作一整个矩阵，其操作效率会高上许多。</p><h4 id="Q：可否再讲解一下GloVe的目标函数和双线性对数模型？"><a href="#Q：可否再讲解一下GloVe的目标函数和双线性对数模型？" class="headerlink" title="Q：可否再讲解一下GloVe的目标函数和双线性对数模型？"></a>Q：可否再讲解一下GloVe的目标函数和双线性对数模型？</h4><p>A：（返回看上面）每个词都有一个偏置项很重要，方便调整。</p><h4 id="Q：如何处理词汇的多义性？（这个问题是补充讲解）"><a href="#Q：如何处理词汇的多义性？（这个问题是补充讲解）" class="headerlink" title="Q：如何处理词汇的多义性？（这个问题是补充讲解）"></a>Q：如何处理词汇的多义性？（这个问题是补充讲解）</h4><p>A：比较疯狂的做法是对词语的每个语义作一个词向量，这可以通过意义聚类来尝试；但一是这种做法复杂度较高，在词向量生成时要加上一层词义分析；二是词汇中多个语义之间可能难以区分，无法达到固定标准。</p><p>对每个语义做一个词向量，这个尝试在早期就有人做，主要思想是对各个词义的词向量作加权组合，而且令人惊讶的是通过稀疏自编码的思想，可以将组合后的词向量再分离回去（前提是语义十分常见）。</p><p><img src="/post/f552d4fb/image-20230206012430908.png" alt="image-20230206012430908"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nndl编程练习2：线性模型练习题解</title>
      <link href="/post/2477d76.html"/>
      <url>/post/2477d76.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>请按照填空顺序编号分别完成 参数优化，不同基函数的实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">filename</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;载入数据。&quot;&quot;&quot;</span><br>    xys = []<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:<br>            xys.append(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">float</span>, line.strip().split()))<br>        xs, ys = <span class="hljs-built_in">zip</span>(*xys)<br>        <span class="hljs-keyword">return</span> np.asarray(xs), np.asarray(ys)<br></code></pre></td></tr></table></figure><h2 id="不同的基函数-basis-function-的实现"><a href="#不同的基函数-basis-function-的实现" class="headerlink" title="不同的基函数 (basis function)的实现"></a>不同的基函数 (basis function)的实现</h2><h4 id="填空顺序-2"><a href="#填空顺序-2" class="headerlink" title="填空顺序 2"></a>填空顺序 2</h4><p>请分别在这里实现“多项式基函数”以及“高斯基函数”</p><p>其中以及训练集的x的范围在0-25之间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">identity_basis</span>(<span class="hljs-params">x</span>):<br>    ret = np.expand_dims(x, axis=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> ret<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">multinomial_basis</span>(<span class="hljs-params">x, feature_num=<span class="hljs-number">10</span></span>):<br>    x = np.expand_dims(x, axis=<span class="hljs-number">1</span>) <span class="hljs-comment"># shape(N, 1)</span><br>    ret = [x]<br>    <span class="hljs-comment"># 分别乘幂</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>,feature_num+<span class="hljs-number">1</span>):<br>        ret.append(x**i)<br>    <span class="hljs-comment">#将乘幂的结果连起来</span><br>    ret=np.concatenate(ret,axis=<span class="hljs-number">1</span>)<br>    <span class="hljs-built_in">print</span>(ret.shape)<br>    <span class="hljs-keyword">return</span> ret<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gaussian_basis</span>(<span class="hljs-params">x, feature_num=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;高斯基函数&#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment">#==========</span><br>    <span class="hljs-comment">#todo &#x27;&#x27;&#x27;请实现高斯基函数&#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment">#==========</span><br>    <span class="hljs-comment"># 高斯基函数</span><br>    centers = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">25</span>,feature_num)<br>    s = centers[<span class="hljs-number">1</span>]-centers[<span class="hljs-number">0</span>]<br>    phi = np.expand_dims(x,axis=<span class="hljs-number">1</span>)<br>    x = np.concatenate([phi]*feature_num,axis=<span class="hljs-number">1</span>)<br>    t =( x - centers)/s<br>    ret = np.exp(- <span class="hljs-number">0.5</span> * t ** <span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> ret<br></code></pre></td></tr></table></figure><p>多项式基函数：<br>$$<br>\phi_j(x) &#x3D; x^j<br>$$<br>高斯基函数：<br>$$<br>\phi_j(x) &#x3D; \exp\lbrace -{ {(x-\mu_j)^2} \over {2s^2} } \rbrace &#x3D; \exp \lbrace - {1\over 2} ({ (x-\mu_j)\over s}) ^2 \rbrace<br>$$</p><h2 id="返回一个训练好的模型"><a href="#返回一个训练好的模型" class="headerlink" title="返回一个训练好的模型"></a>返回一个训练好的模型</h2><h4 id="填空顺序-1-用最小二乘法进行模型优化"><a href="#填空顺序-1-用最小二乘法进行模型优化" class="headerlink" title="填空顺序 1 用最小二乘法进行模型优化"></a>填空顺序 1 用最小二乘法进行模型优化</h4><h4 id="填空顺序-3-用梯度下降进行模型优化"><a href="#填空顺序-3-用梯度下降进行模型优化" class="headerlink" title="填空顺序 3 用梯度下降进行模型优化"></a>填空顺序 3 用梯度下降进行模型优化</h4><blockquote><p>先完成最小二乘法的优化 (参考书中第二章 2.3中的公式)</p></blockquote><blockquote><p>再完成梯度下降的优化 (参考书中第二章 2.3中的公式)</p></blockquote><p>在main中利用训练集训练好模型的参数，并且返回一个训练好的模型。</p><p>计算出一个优化后的w，请分别使用最小二乘法以及梯度下降两种办法优化w</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>(<span class="hljs-params">x_train, y_train,</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    训练模型，并返回从x到y的映射。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment">#加入1序列，作为训练时的常量$b$</span><br>    <br>    <span class="hljs-comment">#在这里更换基函数</span><br>    basis_func = multinomial_basis<br>    phi0 = np.expand_dims(np.ones_like(x_train), axis=<span class="hljs-number">1</span>)<br>    phi1 = basis_func(x_train)<br>    phi = np.concatenate([phi0, phi1], axis=<span class="hljs-number">1</span>)<br>    <br>    <br>    <span class="hljs-comment">#==========</span><br>    <span class="hljs-comment">#todo &#x27;&#x27;&#x27;计算出一个优化后的w，请分别使用最小二乘法以及梯度下降两种办法优化w&#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment">#==========</span><br>    <span class="hljs-comment">#最小二乘法，其中np.linalg.pinv()求的是phi的伪逆矩阵</span><br>    w = np.dot(np.linalg.pinv(phi),y_train)<br>    <span class="hljs-comment">#梯度下降</span><br>    <span class="hljs-comment">#损失函数</span><br><span class="hljs-comment">#     def dj(theta,phi,y):</span><br><span class="hljs-comment">#         return phi.T.dot(np.dot(phi,theta)-y) * 2.0 / len(phi)</span><br>    <br><span class="hljs-comment">#     def gradient(phi,y,initial_theta, eta=0.001, n_iters=10000):</span><br><span class="hljs-comment">#         w = initial_theta</span><br><span class="hljs-comment">#         for i in range(n_iters):</span><br><span class="hljs-comment">#             w = w - eta * dj(w,phi,y)</span><br><span class="hljs-comment">#         return w</span><br><span class="hljs-comment">#     initial_theta = np.zeros(phi.shape[1])</span><br><span class="hljs-comment">#     w = gradient(phi,y_train,initial_theta)</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>        phi0 = np.expand_dims(np.ones_like(x), axis=<span class="hljs-number">1</span>)<br>        phi1 = basis_func(x)<br>        phi = np.concatenate([phi0, phi1], axis=<span class="hljs-number">1</span>)<br>        y = np.dot(phi, w)<br>        <span class="hljs-keyword">return</span> y<br><br>    <span class="hljs-keyword">return</span> f<br></code></pre></td></tr></table></figure><p>最小二乘法训练模型是直接计算解<br>$$<br>\pmb w^* &#x3D; (\pmb {XX}^\top)^{-1} \pmb X\pmb y<br>$$<br>其中，$(\pmb {XX}^\top)^{-1} \pmb X$被称为$\pmb X^\top$的伪逆矩阵，因为$\pmb X^{-1}$不是满秩的，所以需要用伪逆矩阵表示，伪逆矩阵可以用<code>np.linalg.pinv()</code>计算。</p><p>梯度下降分为两步，其一是计算梯度，其二是梯度下降</p><p>首先，梯度计算式可以表示为<br>$$<br>\begin{align}<br>{\partial \mathcal L} \over {\partial w} &amp; &#x3D; { {\partial {1\over n} \sum_{n&#x3D;1}^n (wx^{(i)}+b-y^{(i)})}^2 \over {\partial w} } \\<br>&amp; &#x3D; {1\over n} \sum_{n&#x3D;1}^n { {\partial (wx^{(i)}+b-y^{(i)})}^2 \over {\partial w} }  \\<br>&amp; &#x3D; {1\over n}\sum_{n&#x3D;1}^n \ 2 ·  (wx^{(i)}+b-y^{(i)}) · { {\partial (wx^{(i)}+b-y^{(i)})} \over {\partial w} }  \\<br>&amp; &#x3D; {2\over n}\sum_{n&#x3D;1}^n \ (wx^{(i)}+b-y^{(i)}) ·x^{(i)}<br>\end{align}<br>$$<br>最后的式子可以转写为矩阵表达式<br>$$<br>{ {\partial \mathcal L} \over {\partial \pmb w} } &#x3D; {2\over n} ·\pmb x^\top (\pmb w\pmb x-\pmb y)<br>$$<br>然后，梯度下降表达式为<br>$$<br>\pmb w &#x3D; \pmb w-\eta { \partial L \over {\partial \pmb w} }<br>$$<br>权重的初始值可以全部赋0，这样就完成了梯度下降的搭建。</p><h2 id="评估结果"><a href="#评估结果" class="headerlink" title="评估结果"></a>评估结果</h2><blockquote><p>没有需要填写的代码，但是建议读懂</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">ys, ys_pred</span>):<br>    <span class="hljs-comment"># 计算标准差</span><br>    <span class="hljs-string">&quot;&quot;&quot;评估模型。&quot;&quot;&quot;</span><br>    std = np.sqrt(np.mean(np.<span class="hljs-built_in">abs</span>(ys - ys_pred) ** <span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">return</span> std<br><br><span class="hljs-comment"># 程序主入口（建议不要改动以下函数的接口）</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    train_file = <span class="hljs-string">&#x27;train.txt&#x27;</span><br>    test_file = <span class="hljs-string">&#x27;test.txt&#x27;</span><br>    <span class="hljs-comment"># 载入数据</span><br>    x_train, y_train = load_data(train_file)<br>    x_test, y_test = load_data(test_file)<br>    <span class="hljs-built_in">print</span>(x_train.shape)<br>    <span class="hljs-built_in">print</span>(x_test.shape)<br>    <span class="hljs-comment"># 使用线性回归训练模型，返回一个函数f()使得y = f(x)</span><br>    <span class="hljs-comment"># 定义训练函数</span><br>    f = main(x_train, y_train)<br>    <br>    y_train_pred = f(x_train)<br>    std = evaluate(y_train, y_train_pred)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练集预测值与真实值的标准差：&#123;:.1f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(std))<br>    <br>    <span class="hljs-comment"># 计算预测的输出值</span><br>    y_test_pred = f(x_test)<br>    <span class="hljs-comment"># 使用测试集评估模型</span><br>    std = evaluate(y_test, y_test_pred)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;预测值与真实值的标准差：&#123;:.1f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(std))<br>    <br>    <span class="hljs-comment"># 用plt输出图像</span><br>    <span class="hljs-comment">#显示结果</span><br>    plt.plot(x_train, y_train, <span class="hljs-string">&#x27;ro&#x27;</span>, markersize=<span class="hljs-number">3</span>)<br><span class="hljs-comment">#     plt.plot(x_test, y_test, &#x27;k&#x27;)</span><br>    plt.plot(x_test, y_test_pred, <span class="hljs-string">&#x27;k&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)<br>    plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br>    plt.title(<span class="hljs-string">&#x27;original basis&#x27;</span>)<br>    plt.legend([<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;pred&#x27;</span>])<br>    plt.show()<br>    <br></code></pre></td></tr></table></figure><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><h3 id="原始基函数"><a href="#原始基函数" class="headerlink" title="原始基函数"></a>原始基函数</h3><p><img src="/post/2477d76/image-20230131184451988.png" alt="image-20230131184451988"></p><h3 id="多项式基函数"><a href="#多项式基函数" class="headerlink" title="多项式基函数"></a>多项式基函数</h3><p><img src="/post/2477d76/image-20230131184539586.png" alt="image-20230131184539586"></p><h3 id="高斯基函数"><a href="#高斯基函数" class="headerlink" title="高斯基函数"></a>高斯基函数</h3><p><img src="/post/2477d76/image-20230131184620340.png" alt="image-20230131184620340"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N课程P1：简介和词向量</title>
      <link href="/post/a60c703c.html"/>
      <url>/post/a60c703c.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="课程词汇"><a href="#课程词汇" class="headerlink" title="课程词汇"></a>课程词汇</h1><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs txt">controversial - 矛盾的；有争议的<br>manipulate - 操作；使用<br>pithy - 精辟的；准确的<br>truncate - 截断<br>scalable - 可扩展的<br></code></pre></td></tr></table></figure><h1 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h1><h2 id="导引"><a href="#导引" class="headerlink" title="导引"></a>导引</h2><h4 id="1、什么是指称语义"><a href="#1、什么是指称语义" class="headerlink" title="1、什么是指称语义"></a>1、什么是指称语义</h4><p>是通过构造表达其语义的（叫做指称 (denotation)或意义的）数学对象来<strong>形式化</strong>计算机系统的语义的一种方法。即：实现语义和符号的对应关系。</p><p><img src="/post/a60c703c/image-20230131013506016.png" alt="image-20230131013506016"></p><h4 id="2、局部表示和分布式表示"><a href="#2、局部表示和分布式表示" class="headerlink" title="2、局部表示和分布式表示"></a>2、局部表示和分布式表示</h4><p>局部表示是用离散变量存储单词信息，这种符号化表示可以用one-hot编码表示</p><p><strong>缺点：</strong>由于单词向量正交，无法体现单词间的相关性</p><p>分布式表示的重点在于用<strong>上下文信息</strong>（固定窗口）表示原单词（<em>A word’s meaning is given by the words that frequently appear close-by</em>）</p><p><img src="/post/a60c703c/image-20230131234332948.png" alt="image-20230131234332948"></p><p>在分布式表示下，单词作为标记分布在高维空间中，距离（在这里可以指欧氏距离）相近的标记其语义也相似。</p><p><img src="/post/a60c703c/image-20230201000803733.png" alt="image-20230201000803733"></p><h4 id="3、类型和标记（Type-amp-Token）"><a href="#3、类型和标记（Type-amp-Token）" class="headerlink" title="3、类型和标记（Type &amp; Token）"></a>3、类型和标记（Type &amp; Token）</h4><p><strong>token（标记）</strong>：文本内<strong>出现的</strong>单词总数叫做文本标记数</p><p><strong>type（类型）</strong>：文本内<strong>出现的</strong>不同的单词总数叫做文本类型数</p><p>例子：She is <strong>directly</strong> <em>astonished</em> by the <em>astonishing</em> thing, and <strong>directly</strong> stared at him in <em>astonishment</em>.</p><p>句子一共出现了15个单词，故此句有15个token；句子中不同单词有14个（directly出现两次），故此句有14个type。</p><h2 id="分布式词向量：Word2Vec"><a href="#分布式词向量：Word2Vec" class="headerlink" title="分布式词向量：Word2Vec"></a>分布式词向量：Word2Vec</h2><h3 id="概述（Overview）"><a href="#概述（Overview）" class="headerlink" title="概述（Overview）"></a>概述（Overview）</h3><h4 id="思想（Idea）"><a href="#思想（Idea）" class="headerlink" title="思想（Idea）"></a>思想（Idea）</h4><ul><li>We have a large corpus (“body”) of text: a long list of words</li><li>Every word in a fixed vocabulary <strong>is represented by a vector</strong></li><li>Go through each position <em>t</em> in the text, which has <strong>a center word（中心词） <em>c</em> and context (“outside”) words （上下文词）<em>o</em></strong></li><li><strong>Use the similarity</strong> of the word vectors for <em>c</em> and <em>o</em> to calculate the probability of <em>o</em> given <em>c</em> (or vice versa)</li><li><strong>Keep adjusting the word vectors</strong> to maximize this probability</li></ul><p><img src="/post/a60c703c/image-20230201011325988.png" alt="image-20230201011325988"></p><h3 id="模型（Model）"><a href="#模型（Model）" class="headerlink" title="模型（Model）"></a>模型（Model）</h3><h4 id="目标函数（Objective-Function）"><a href="#目标函数（Objective-Function）" class="headerlink" title="目标函数（Objective Function）"></a>目标函数（Objective Function）</h4><p>对于每个位置$t\in[1,T],t\in Z$，给定单词$w_t$，预测在窗口为固定值$m$的范围内的上下文，其数值似然为</p><p><img src="/post/a60c703c/image-20230201011622706.png" alt="image-20230201011622706"></p><p>其中，对数似然可以使计算从乘法变成加法，简化计算难度；除以$T(词汇总量)$用于归一化；负号将最大化问题转化为最小化问题。后面两个处理是计算习惯使然。</p><h4 id="预测函数（Predict-Function）"><a href="#预测函数（Predict-Function）" class="headerlink" title="预测函数（Predict Function）"></a>预测函数（Predict Function）</h4><p>似然$p(w_{t+j}|w_t;\theta)$的计算需要引入两个词向量：$v_w$和$u_w$：</p><ul><li>$v_w$：$w$作为中心词时的词向量</li><li>$u_w$：$w$作为上下文词时的词向量</li></ul><p>然后引入似然概率计算公式，其中$V$是上下文词汇集合。<br>$$<br>P(o|c)&#x3D;{ {\exp(u_o^\top v_c)} \over {\sum_{w\in v}\exp(u_w^\top v_c)} }<br>$$<br>对于此公式，解释是这样的：</p><p><img src="/post/a60c703c/image-20230201012628417.png" alt="image-20230201012628417"></p><p>需要强调的是，点积的处理中是有相似性的度量因素在的。</p><h4 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h4><p>目标函数<br>$$<br>J(\theta) &#x3D; -{1\over T} \sum_{t&#x3D;1}^T \sum_{-m \le j \le m,j\ne 0} \log P(w_{t+j}|w_t;\theta)  \tag{1}<br>$$<br>优化函数<br>$$<br>P(o|c)&#x3D;{ {\exp(u_o^\top v_c)} \over {\sum_{w\in V}\exp(u_w^\top v_c)} } \tag{2}<br>$$<br>目标函数对中心词$v_c$求偏导，注意这里用到了向量求导的基本知识<br>$$<br>\begin{align}<br>{ \partial \over {\partial v_c} } \log  { {\exp(u_o^\top v_c)} \over {\sum_{w\in V}\exp(u_w^\top v_c)} } &amp; &#x3D; ①-② \\<br>① &amp; &#x3D; { \partial \over {\partial v_c} } \log {\exp(u_o^\top v_c) } &#x3D; { \partial \over {\partial v_c} } u_o^\top v_c &#x3D; u_o(向量导数)\\<br>② &amp; &#x3D; { \partial \over {\partial v_c} }  \log { \sum_{w\in V} \exp(u_w^\top v_c) } &#x3D; { 1 \over { \sum_{w\in V} \exp(u_w^\top v_c) } } \sum_{x\in V} \exp(u_x^\top v_c) · u_x(变换参数) \\<br>∴{ \partial \over {\partial v_c} } \log P(o|c) &amp; &#x3D; u_o - { {\sum_{x\in V} \exp(u_x^\top v_c) u_x} \over {\sum_{w\in V} \exp (u_w^\top v_c)} } \\<br>&amp; &#x3D; u_o - \sum_{x\in V} { {\exp(u_x^\top v_c) } \over {\sum_{w\in V} \exp (u_w^\top v_c)} }u_x (softmax形式) \\<br>&amp; &#x3D; u_o - \sum_{x\in V}P(x|c)u_x \\<br>&amp; &#x3D; observed - expected<br>\end{align}<br>$$<br>其中$u_o$可以看作观测结果，$\sum_{x\in V}P(x|c)u_x$是对上下文概率的平均，是期望值，$softmax$形式的式子，其开导数的结果很多都是表达为观测值和期望值的接近程度。</p><h1 id="课后问题"><a href="#课后问题" class="headerlink" title="课后问题"></a>课后问题</h1><h4 id="Q：中心词向量和上下文词向量如何能用一个向量表示？"><a href="#Q：中心词向量和上下文词向量如何能用一个向量表示？" class="headerlink" title="Q：中心词向量和上下文词向量如何能用一个向量表示？"></a>Q：中心词向量和上下文词向量如何能用一个向量表示？</h4><p>A：不同人使用不同的结合方法，最典型的是取平均，因为这两个向量再训练之后会十分相似。</p><h4 id="Q：一个词可能有多种意思，如何表示？"><a href="#Q：一个词可能有多种意思，如何表示？" class="headerlink" title="Q：一个词可能有多种意思，如何表示？"></a>Q：一个词可能有多种意思，如何表示？</h4><p>A：或许有一点奇怪，但将多个意思用一个词向量表示，其效果还不错。</p><h4 id="Q：可以学到Alexa的部分吗？"><a href="#Q：可以学到Alexa的部分吗？" class="headerlink" title="Q：可以学到Alexa的部分吗？"></a>Q：可以学到Alexa的部分吗？</h4><p>A：你应该学CS224S。</p><h4 id="Q：对立面的平衡是如何做的呢？（如电影很差-x2F-电影很好）"><a href="#Q：对立面的平衡是如何做的呢？（如电影很差-x2F-电影很好）" class="headerlink" title="Q：对立面的平衡是如何做的呢？（如电影很差&#x2F;电影很好）"></a>Q：对立面的平衡是如何做的呢？（如电影很差&#x2F;电影很好）</h4><p>A：词向量模型对对立面的关注做的很差，现在还没有实现捕获。</p><h4 id="Q：词向量在虚词（so-x2F-not-etc-）的效果好吗"><a href="#Q：词向量在虚词（so-x2F-not-etc-）的效果好吗" class="headerlink" title="Q：词向量在虚词（so&#x2F;not,etc.）的效果好吗"></a>Q：词向量在虚词（so&#x2F;not,etc.）的效果好吗</h4><p>A：很多情况下他们区别不太大，但我们确实也建立了这些词的词向量，word2vec对这些词的敏感度不强，之后会有对句子结构捕捉效果更好的模型。</p><h4 id="Q：对Word2Vec的优化"><a href="#Q：对Word2Vec的优化" class="headerlink" title="Q：对Word2Vec的优化"></a>Q：对Word2Vec的优化</h4><p>A：商业应用比如有Skip Grand、负采样等。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> Stanford CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nndl编程练习1：Numpy练习题解</title>
      <link href="/post/59d56acd.html"/>
      <url>/post/59d56acd.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="row">    <embed src="../download/numpy_tutorial.pdf" width="100%" height="550" type="application/pdf"></div><h3 id="np-expand-dim"><a href="#np-expand-dim" class="headerlink" title="np.expand_dim()"></a>np.expand_dim()</h3><p>参考自知乎<a href="https://www.zhihu.com/people/lifenglai">lifenglai</a>的解答，并从二维数组视角起步。</p><p>针对二维数组的变现，简单理解相当于：将原数组置于不含指定轴的平面，然后沿着指定轴方向拉伸1个单位长度，形成三维数组。</p><p><img src="/post/59d56acd/v2-71ea5f331efa6dd383e2982cf204ffe2_1440w.webp" alt="img"></p><p><img src="/post/59d56acd/v2-3e3f80870bb593322779c9881824cfb0_1440w.webp" alt="img"></p><p><img src="/post/59d56acd/v2-d042a14663f947eec388bfcaca9c30be_1440w.webp" alt="img"></p><p><img src="/post/59d56acd/v2-4f2e200c276ad72709badfa660feb066_1440w.webp" alt="img"></p><h3 id="np-concatenate"><a href="#np-concatenate" class="headerlink" title="np.concatenate"></a>np.concatenate</h3><p>一次性完成多个数组的拼接操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a=np.array([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>b=np.array([<span class="hljs-number">11</span>,<span class="hljs-number">22</span>,<span class="hljs-number">33</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>c=np.array([<span class="hljs-number">44</span>,<span class="hljs-number">55</span>,<span class="hljs-number">66</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.concatenate((a,b,c),axis=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 默认情况下，axis=0可以不写</span><br>array([ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>, <span class="hljs-number">11</span>, <span class="hljs-number">22</span>, <span class="hljs-number">33</span>, <span class="hljs-number">44</span>, <span class="hljs-number">55</span>, <span class="hljs-number">66</span>]) <span class="hljs-comment">#对于一维数组拼接，axis的值不影响最后的结果</span><br> <br> <br> <br><span class="hljs-meta">&gt;&gt;&gt; </span>a=np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>b=np.array([[<span class="hljs-number">11</span>,<span class="hljs-number">21</span>,<span class="hljs-number">31</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.concatenate((a,b),axis=<span class="hljs-number">0</span>)<br>array([[ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],<br>       [ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>],<br>       [<span class="hljs-number">11</span>, <span class="hljs-number">21</span>, <span class="hljs-number">31</span>],<br>       [ <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>]])<br> <br><span class="hljs-meta">&gt;&gt;&gt; </span>np.concatenate((a,b),axis=<span class="hljs-number">1</span>)  <span class="hljs-comment">#axis=1表示对应行的数组进行拼接</span><br>array([[ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>, <span class="hljs-number">11</span>, <span class="hljs-number">21</span>, <span class="hljs-number">31</span>],<br>       [ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>]])<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
            <tag> numpy </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无监督学习笔记</title>
      <link href="/post/888256ea.html"/>
      <url>/post/888256ea.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h1><p>监督学习旨在建立从数据到预测的一个映射，而无监督学习是指<strong>从无标签的数据中学习出一些有用的模式</strong>。无监督学习算法一般直接从原始数据中学习，不借助于任何人工给出标签或者反馈等指导信息。</p><p>无监督学习按现有类别可以分为：</p><ul><li>特征学习：从无标签的训练数据中挖掘有效的特征或表示，一般用于降维、数据可视化、数据预处理等；</li><li>聚类：将具有相似性的忘本划归为同一类，旨在建立从无标签数据到聚类预测结果的映射；</li><li>密度估计：是根据一组训练样本来估计样本空间的概率密度，可分为参数密度估计和非参数密度估计，通过数据挖掘底层分布。</li></ul><p><img src="/post/888256ea/image-20230127004055039.png" alt="image-20230127004055039"></p><h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><p>聚类是指将样本集合中相似的样本分配到相同的类&#x2F;簇(cluster)，不同的样本分配到不同的类&#x2F;簇，使得类内样本间距较小而类间样本间距较大。其中，类间距和相似度等参数都是一种主观定义，有多种表示方式。</p><p><img src="/post/888256ea/image-20230127004437026.png" alt="image-20230127004437026"></p><p>聚类常见应用于图像分割、文本聚类、社交网络分析等。</p><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><h4 id="样本间距离"><a href="#样本间距离" class="headerlink" title="样本间距离"></a>样本间距离</h4><p>样本间的距离用于表示样本间的相似性，样本可以视做一个个点，而距离的表示方法有多样，如$L1&#x2F;L2$距离、余弦距离、相关系数、汉明距离等。</p><p><img src="/post/888256ea/image-20230127004712901.png" alt="image-20230127004712901"></p><h4 id="类-x2F-簇"><a href="#类-x2F-簇" class="headerlink" title="类&#x2F;簇"></a>类&#x2F;簇</h4><p>类&#x2F;簇没有一个严格的定义，可以理解为一组相似的样本，用于提取相似度较高样本组的信息。</p><p><img src="/post/888256ea/image-20230127004819452.png" alt="image-20230127004819452"></p><h4 id="类内间距"><a href="#类内间距" class="headerlink" title="类内间距"></a>类内间距</h4><p>类内间距可以用样本间平均距离或样本间最大距离表示。</p><ul><li><p>样本间平均距离<br>$$<br>avg(C)&#x3D;{2 \over {|C|(|C|-1)} } \sum_{1\le i \lt j \le |C|} d_{ij}<br>$$</p></li><li><p>样本间最大距离<br>$$<br>dia(C)&#x3D;\max_{1\le i \lt j \le |C|}d_{ij}<br>$$</p></li></ul><h4 id="类间间距（设为-C-p-、-C-q-）"><a href="#类间间距（设为-C-p-、-C-q-）" class="headerlink" title="类间间距（设为$C_p$、$C_q$）"></a>类间间距（设为$C_p$、$C_q$）</h4><p>类间间距可以用样本间的最短距离和均值间距离表示。</p><ul><li><p>样本间最短距离<br>$$<br>D_{pq}&#x3D;\min \lbrace d_{ij}|x^{(i)}\in C_p,x^{(j)}\in C_q \rbrace<br>$$</p></li><li><p>样本均值间距离<br>$$<br>D_{pq}&#x3D;d_{\mu_p\mu_q}<br>$$</p></li></ul><h3 id="聚类指标"><a href="#聚类指标" class="headerlink" title="聚类指标"></a>聚类指标</h3><h4 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h4><p>指有外部因素用于参考聚类结果，即已知标签$y$，其作为最后的评价标准。常见的外部指标有$Jaccard$系数、$FM$系数、$Rand$系数等；</p><p><img src="/post/888256ea/image-20230128002254294.png" alt="image-20230128002254294"></p><h4 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h4><p>指评价体系只基于内部数据，无外部参考聚类结果，如$Dunn$指数、$DB$指数等<br>$$<br>DBI&#x3D;{1\over k}\sum_{i&#x3D;1}^k \max_{j\ne i} { {avg(C_i)+avg(C_j)} \over {d_{\mu_i\mu_j}} }<br>$$<br>在$DB$系数中，如上式所示，类内间距是分子部分，类间间距是分母部分，$DB$系数希望类内越近越好，类间越远越好，整理越小越好；$\max$代表其取的是所有$i \ne j$的最坏情况下的最小，保证其普适性成立。</p><h3 id="聚类方法"><a href="#聚类方法" class="headerlink" title="聚类方法"></a>聚类方法</h3><p>常见聚类方法有K均值聚类、层次聚类、密度聚类、谱聚类等；</p><h4 id="k均值聚类"><a href="#k均值聚类" class="headerlink" title="k均值聚类"></a>k均值聚类</h4><p>K均值的<strong>目标函数</strong>为：<br>$$<br>E&#x3D;\sum_{i&#x3D;1}^k\sum_{x\in C_i}||x-\mu_i||^2_2<br>$$<br>其中$\mu_i$是第$i$个簇$C_i$的均值向量，$E$值刻画了簇内样本围绕簇均值向量的紧密程度。</p><p>K均值聚类的<strong>过程</strong>分为两步</p><p>1）选择K个点作为聚类中心</p><ol><li>根据与聚类中心的距离对每个样本点进行聚类</li><li>求每类样本的平均值作为该类别的新的聚类中心</li></ol><p>2）不断迭代1和2，直至收敛（每个样本类别不再改变或其他更为宽松的条件）</p><p>对于K均值聚类的<strong>收敛性</strong>，由于其需要两个步骤不断迭代优化，其属于NP-Hard问题：</p><ol><li>固定均值向量，优化聚类划分（第1步）</li><li>固定聚类划分，优化均值向量（第2步）</li></ol><p>对于K均值聚类的<strong>超参</strong>，K的选择可以基于经验，类中心的初始化过程可以按照一定策略，如大于最小间距的随机点&#x2F;样本点、K个相互距离最远的样本点、K个等距网格点等。</p><h4 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h4><p>层次聚类通过计算不同类别数据点间的相似度来创建一棵有层次地嵌套聚类树，嵌套策略可以是聚合（自底向上）或者分裂（自顶向下）</p><p><img src="/post/888256ea/image-20230128001711033.png" alt="image-20230128001711033"></p><p><strong>聚合过程</strong>先将每个样本分到单独的类，然后不断迭代合并过程直至满足终止条件：</p><ol><li>计算两两类簇之间的距离，找到距离最小的两个类簇$c_1$和$c_2$</li><li>合并类簇$c_1$和$c_2$</li></ol><p><img src="/post/888256ea/image-20230128002135502.png" alt="image-20230128002135502"></p><p><strong>分裂过程</strong>和聚合过程相反，先将所有样本划归到同一个类，然后不断分裂直到满足终止条件：</p><ol><li>在同一个类簇（记为$c$）中计算两两样本之间的距离，找出距离最远的两个样本$a$和$b$</li><li>将样本$a$、$b$分配到不同类簇$c_1$和$c_2$中</li><li>计算原类簇$c$中剩余的其他样本点和$a$、$b$的距离，若是$dis(a)&lt;dis(b)$则将样本点归到$c_1$，否则归到$c_2$。</li></ol><p><img src="/post/888256ea/image-20230128002851790.png" alt="image-20230128002851790"></p><p>其余两种方法不在这里继续叙述。</p><h2 id="无监督特征学习"><a href="#无监督特征学习" class="headerlink" title="无监督特征学习"></a>无监督特征学习</h2><p>无监督特征学习一般遵循着先编码后解码，其目的在于提取有用特征，以去噪、降维、数据可视化等应用。</p><h3 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h3><p>数据的原始特征可能存在维度高、冗余等问题，其会直接导致维度灾难和学习效果降低，因此需要采用特征学习的方式进行降维，一种最常用的数据降维方法就是投影，将数据映射到另一个域，其中以线性投影应用最广。<br>$$<br>\pmb z&#x3D;\pmb W^\top\pmb x<br>$$<br>并满足对称性，以降低冗余<br>$$<br>\pmb W^\top \pmb W&#x3D;I<br>$$<br>主成分分析是特征学习在降维领域的一个经典应用，其优化原则为</p><ul><li><strong>最大投影方差：</strong>使得在转换后的空间中的数据方差最大，尽可能多的保留原数据的信息；</li><li><strong>最小重构误差：</strong>使得样本点到超平面的距离足够近，降低重构难度。</li></ul><p>样本点$\pmb x^{(n)}$投影之后的表示为<br>$$<br>z^{(n)}&#x3D;\pmb w^\top \pmb x^{(n)}<br>$$<br>所有样本点投影之后的方差为<br>$$<br>\begin{align}<br>\sigma(\pmb {X};\pmb w)&amp;&#x3D;{1\over N}\sum_{n&#x3D;1}^N(\pmb w^\top\pmb x^{(n)}-\pmb w^\top \pmb {\overline x})^2 \\<br>&amp;&#x3D;{1\over N}\sum_{n&#x3D;1}^N(\pmb w^\top\pmb X-\pmb w^\top\pmb {\overline X})(\pmb w^\top\pmb X-\pmb w^\top\pmb {\overline X})^\top\\<br>&amp;&#x3D;\pmb w^\top \Sigma \pmb w<br>\end{align}<br>$$<br>其中$\Sigma$是原始空间中$X$的协方差矩阵，对上式方差做拉格朗日乘子法，可得目标函数<br>$$<br>\max_\omega\pmb w^\top \Sigma \pmb w+\lambda(1-\pmb w^\top\pmb w)<br>$$<br>对目标函数求导并令导数等于0，可得：<br>$$<br>\Sigma \pmb w&#x3D;\lambda\pmb w<br>$$<br><img src="/post/888256ea/image-20230128010117724.png" alt="image-20230128010117724"></p><p>从这个式子可以看出，PCA的目标函数过程可以视为：给定协方差矩阵$\Sigma$，求特征值$\lambda$及对应的特征向量$\pmb w$，这种求解过程可以推广到多维，如果要通过投影矩阵$\pmb W\in R^{D\times D’}$将样本投到$D’$维空间，投影矩阵满足$\pmb{W^\top W&#x3D;I}$为单位阵，只需要将$\Sigma$的特征值从小到大排列，保留前$D’$个特征向量，其对应的特征向量即是最优的投影矩阵<br>$$<br>\pmb \Sigma\pmb  W&#x3D;\pmb Wdiag(\pmb \lambda)<br>$$<br>提高两类可分性的方法一般为监督学习方法，比如线性判别分析（LDA）。</p><h3 id="稀疏编码"><a href="#稀疏编码" class="headerlink" title="稀疏编码"></a>稀疏编码</h3><h4 id="（线性）编码"><a href="#（线性）编码" class="headerlink" title="（线性）编码"></a>（线性）编码</h4><p>（线性）编码定义为：给定一组基向量$A&#x3D;[\pmb a_1,…,\pmb a_M]$，将输入样本$\pmb x$表示为这些基向量的线性组合。<br>$$<br>\pmb x&#x3D;\sum_{m&#x3D;1}^M z_m \pmb a_m &#x3D; \pmb {Az}<br>$$<br>即，数据可以分为字典$\pmb A$和编码$\pmb z$，通过字典来实现对数据的编码。编码是对$D$维空间中的样本$\pmb x$找到其在$P$​维空间中的表示或投影，其目标通常是编码的各个维度都是统计独立的，并且可以重构出输入样本。编码的优化目标为：<br>$$<br>\sum_{n&#x3D;1}^N||\pmb A\pmb z^{(n)}-\pmb x^{(n)}||<br>$$<br>迭代时，不断重复“固定一个求另外一个”的过程，即交替求解$\pmb A$和$\pmb z$直到收敛。</p><h4 id="完备性"><a href="#完备性" class="headerlink" title="完备性"></a>完备性</h4><p>如果$M$个基向量刚好可以支撑$M$维的欧氏空间，则这$M$个基向量是完备的；如果$M$个基向量可以支撑$D$ 维的欧氏空间，并且$M&gt;D$，则这$M$个基向量是过完备的（overcomplete）、冗余的．<strong>“过完备”</strong>基向量是指基向量个数远远大于其支撑空间维度。因此这些基向量<strong>一般不具备独立、正交等性质</strong>。</p><ul><li>当$M&gt;D$时，能找到多个完备解且其重构方差为0；</li><li>当$M&lt;D$时，不能找到解，欠完备。</li></ul><h4 id="稀疏编码-1"><a href="#稀疏编码-1" class="headerlink" title="稀疏编码"></a>稀疏编码</h4><p>稀疏编码<strong>目标</strong>是找到一组“过完备”的基向量来进行编码，少数基向量就可以表示原问题的所有信息。少数基向量的加权可以构成原始解。</p><p><img src="/post/888256ea/image-20230128174315722.png" alt="image-20230128174315722"></p><p>给定一组$N$个输入向量的$\pmb x^{(1)},…,x^{(n)}$，其稀疏编码的目标函数为：<br>$$<br>\mathcal L(\pmb A,\pmb Z)&#x3D;\sum_{n&#x3D;1}^N(||\pmb x^{(n)}-\pmb A\pmb z^{(n)}||^2+\eta\rho(\pmb z^{(n)}))<br>$$<br>其中，$\rho(·)$是一个稀疏性衡量函数，$\eta$是一个超参数，用于控制稀疏性的强度，目标函数的前半部分表示重构误差，后半部分是稀疏性的衡量，整体其实和梯度下降添加正则化的目标函数比较类似，一部分是具体目标，一部分是防止过度。</p><p>$\rho(·)$的定义有很多种。</p><p><img src="/post/888256ea/image-20230128182647962.png" alt="image-20230128182647962"></p><p>稀疏编码的<strong>训练过程</strong>分为两步，一般采用交替优化的方法进行。</p><ul><li><p>固定基向量$\pmb A$，对每个输入$\pmb x^{(n)}$，计算其对应的最优编码<br>$$<br>\min_{\pmb z^{(n)}}||\pmb x^{(n)}-\pmb {Az}^{(n)}||^2+\eta\rho(\pmb z^{(n)}), \forall n\in[1,N]<br>$$</p></li><li><p>固定上一步得到的编码$\lbrace \pmb z^{(n)} \rbrace_{n&#x3D;1}^N$，计算其最优基向量<br>$$<br>\min_{\pmb A}\sum_{n&#x3D;1}^N(||\pmb x^{(n)}-\pmb {Az}^{(n)}||^2)+\lambda {1\over 2}||\pmb A||^2<br>$$</p></li></ul><p>稀疏编码的每一维都可以被看作一种特征。稀疏编码的优点有：</p><ol><li>计算量：稀疏性带来的最大好处就是可以极大地降低计算量．</li><li>可解释性：因为稀疏编码只有少数的非零元素，相当于将一个输入样本表示为少数几个相关的特征。这样我们可以更好地描述其特征，并易于理解。</li><li>特征选择 稀疏性带来的另外一个好处是可以<strong>实现特征的自动选择</strong>，只选择和输入样本最相关的少数特征，从而更高效地表示输入样本，降低噪声并减轻过拟合。</li></ol><h3 id="自编码器"><a href="#自编码器" class="headerlink" title="自编码器"></a>自编码器</h3><p>自编码器（Auto-Encoder，AE）是通过无监督的方式来学习一组数据的有效编码（或表示），自编码器的组成部分包括编码器（$\mathbb R^D \rightarrow \mathbb R^M$）和解码器（$\mathbb R^M \rightarrow \mathbb R^D$）。自编码器希望实现编码-解码后恢复的结果与原数据一致，因此其目标函数是最小化重构误差<br>$$<br>\mathcal L&#x3D;\sum_{n&#x3D;1}^N||\pmb x^{(n)}-g(f(\pmb x^{(n)}))||^2 &#x3D; \mathcal L&#x3D;\sum_{n&#x3D;1}^N||\pmb x^{(n)}-f\circ g(\pmb x^{(n)})||^2<br>$$<br>比较简单的是二层网络结构的自编码器，更复杂的可以设定编码的稀疏性、$g \circ f$的形式等。</p><p><img src="/post/888256ea/image-20230129181436308.png" alt="image-20230129181436308"></p><h4 id="稀疏自编码器"><a href="#稀疏自编码器" class="headerlink" title="稀疏自编码器"></a>稀疏自编码器</h4><p>自编码器除了可以学习低维编码之外，也能够学习高维的稀疏编码，假设中间隐藏层$\pmb z$的维度大于输入样本$\pmb x$的维度$D$，并让$\pmb z$尽量稀疏，这就是稀疏自编码器。和稀疏编码一样，稀疏自编码器的优点是有很高的可解释性，并同时进行了隐式的特征选择。稀疏自编码器可以学习到数据中的一些有用的结构。</p><p>稀疏自编码器的目标函数为<br>$$<br>\mathcal L&#x3D;\sum_{n&#x3D;1}^N||\pmb x^{(n)}-\pmb x’^{(n)}||^2+\eta\rho(\pmb Z)+\lambda||\pmb W||^2<br>$$<br>其中$\pmb W$表示自编码器中的参数。</p><h4 id="堆叠自编码器"><a href="#堆叠自编码器" class="headerlink" title="堆叠自编码器"></a>堆叠自编码器</h4><p>对于很多数据来说，仅使用两层神经网络的自编码器还不足以获取一种好的数据表示。为了获取更好的数据表示，我们可以使用更深层的神经网络作为自编码器以提取更为抽象的底层特征，叫做堆叠自编码器。</p><h4 id="降噪自编码器"><a href="#降噪自编码器" class="headerlink" title="降噪自编码器"></a>降噪自编码器</h4><p>通过在自编码器中引入噪声，可以增加编码的鲁棒性。对于一个向量$\pmb x$，首先根据一个比例$\mu$随机将$\pmb x$的一些维度的值设置为0，得到一个被损坏的向量$\widetilde{ \pmb x}$，然后将被损坏的向量$\widetilde{ \pmb x}$输入给自编码器得到编码$\pmb z$，并重构出原始的无损输入$\pmb x$。</p><p><img src="/post/888256ea/image-20230129184752526.png" alt="image-20230129184752526"></p><h2 id="概率密度估计"><a href="#概率密度估计" class="headerlink" title="概率密度估计"></a>概率密度估计</h2><p>概率密度估计可以分为参数密度估计和非参数密度估计两种</p><p>非参数密度估计不假设数据服从某种分布，而是通过将样本空间划分为不同的区域，并估计每个区域的概率来近似数据的概率密度函数。</p><h3 id="参数密度估计"><a href="#参数密度估计" class="headerlink" title="参数密度估计"></a>参数密度估计</h3><p>参数密度估计旨在先根据先验知识<strong>假设随机变量服从某种分布</strong>，然后通过训练样本来估计分布。估计方法有最大似然估计，假设这些样本服从一个概率分布函数$p(\pmb x;\theta)$，并寻找参数最优。<br>$$<br>\log p(\mathcal D;\theta)&#x3D;\sum_{n&#x3D;1}^N\log p(\pmb x^{(n)};\theta)<br>$$<br>参数密度估计常用的先验分布有正态分布、多项分布等；</p><h4 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a>正态分布</h4><p>假设样本$\pmb x\in \mathbb R^D$服从正态分布<br>$$<br>\mathcal N(\pmb x|\pmb \mu,\pmb \Sigma)&#x3D;{ 1 \over {(2\pi)^{D&#x2F;2} |\pmb \Sigma|^{1&#x2F;2} } }\exp(-{1\over 2} (\pmb x-\pmb \mu)^\top \pmb \Sigma^{-1} (\pmb x-\pmb \mu) )<br>$$<br>其中$\pmb \mu$和$\pmb \Sigma$分别为正态分布的均值和方差。数据集$\mathcal D&#x3D;\lbrace \pmb x^{(n)} \rbrace_{n&#x3D;1}^N$的对数似然函数为<br>$$<br>\log p(\mathcal D|\pmb \mu,\pmb \Sigma)&#x3D;-{N\over 2}\log((2\pi)^{D} |\pmb \Sigma|)-{1\over 2} \sum_{n&#x3D;1}^N (\pmb x^{(n)}-\pmb \mu)^\top \pmb \Sigma^{-1} (\pmb x^{(n)}-\pmb \mu)<br>$$<br>分别求上式关于$\pmb{\mu,\Sigma}$的偏导数并令其等于0，可得：<br>$$<br>\begin{align}<br>\pmb \mu^{ML} &amp; &#x3D;{1\over N}\sum_{n&#x3D;1}^N \pmb x^{(n)}, \\<br>\pmb \Sigma^{ML} &amp;&#x3D;{1\over N}\sum_{n&#x3D;1}^N (\pmb x^{(n)}-\pmb \mu^{ML})(\pmb x^{(n)}-\pmb \mu^{ML})^\top.<br>\end{align}<br>$$</p><h4 id="多项分布"><a href="#多项分布" class="headerlink" title="多项分布"></a>多项分布</h4><p>假设样本服从$K$个状态的多项分布，令one-hot向量$\pmb x\in \lbrace 0,1 \rbrace ^K$来表示第$k$个状态，即$x_k&#x3D;1$，其余$x_{i,i\ne k}&#x3D;0$，样本$\pmb x$的概率密度函数为<br>$$<br>p(\pmb x|\pmb \mu)&#x3D;\sum_{k&#x3D;1}^K \mu_k^{x_k}<br>$$<br>其中$u_k$为第$k$个状态的概率，并满足$\sum_{k&#x3D;1}^K \mu_k&#x3D;1$。数据集$D&#x3D;{\pmb x^{(n)}}_{n&#x3D;1}^N$的对数似然函数为</p><p>$$<br>\log p(\mathcal D|\pmb \mu)&#x3D;\sum_{n&#x3D;1}^N \sum_{k&#x3D;1}^K x_k^{(n)}\log(\mu_k)<br>$$<br>多项分布的参数估计为约束优化问题，引入拉格朗日乘子$\lambda$，将原问题转换为无约束优化问题。<br>$$<br>max_{\pmb \mu,\lambda} \sum_{n&#x3D;1}^N\sum_{k&#x3D;1}^K x_k^{(n)} \log(\mu_k)+\lambda(\sum_{k&#x3D;1}^K \mu_k-1)<br>$$<br>分别求上式关于$\mu_k,\lambda$的偏导数，并令其等于0可得<br>$$<br>\mu_k^{ML}&#x3D;{ {m_k} \over N}<br>$$<br>其中$m_k&#x3D;\sum_{n&#x3D;1}^N x_k^{(n)}$是数据集中取值为第$k$个状态的样本数量。</p><h4 id="参数估计的短板"><a href="#参数估计的短板" class="headerlink" title="参数估计的短板"></a>参数估计的短板</h4><p>在实际应用中，参数密度估计一般存在以下问题：</p><ol><li>模型选择问题：即<strong>如何选择数据分布的密度函数</strong>。实际数据的分布往往是非常复杂的，而不是简单的正态分布或多项分布。</li><li>不可观测变量问题：即我们用来训练的样本只包含部分的<strong>可观测变量，还有一些非常关键的变量是无法观测</strong>的，这导致我们很难准确估计数据的真实分布。</li><li>维度灾难问题：即<strong>高维数据</strong>的参数估计十分困难。随着维度的增加，估计参数所需要的样本数量指数增加．在样本不足时会出现过拟合。</li></ol><h3 id="非参数密度估计"><a href="#非参数密度估计" class="headerlink" title="非参数密度估计"></a>非参数密度估计</h3><p>非参数密度估计是不假设数据服从某种分布，通过将样本空间划分为不同的区域并估计每个区域的概率来近似数据的概率密度函数。</p><p>对于高维空间中的一个随机向量$\pmb x$，假设其服从一个未知分布$p(\pmb x)$，则$\pmb x$落入空间中的小区域$\mathcal R$的概率为<br>$$<br>P&#x3D;\int_{\mathcal R}p(\pmb x)d\pmb x<br>$$<br>给定$N$个训练样本$\mathcal D&#x3D;\lbrace \pmb x^{(n)}\rbrace _{n&#x3D;1}^N$，落入区域$\mathcal R$的样本数量$K$服从二项分布：<br>$$<br>P_K&#x3D;\begin{pmatrix}N\\K\end{pmatrix}P^K(1-p)^{1-K}<br>$$<br>其中$K&#x2F;N$的期望为$\mathbb E[K&#x2F;N]&#x3D;P$，方差为$var(K&#x2F;N)&#x3D;P(1-P)N$。当$N$非常大时，我们可以近似认为$P\approx {K\over N}$。假设区域$\mathcal R$足够小，<strong>其内部的概率密度是相同的</strong>，则有<br>$$<br>P&#x3D;p(\pmb x)V<br>$$<br>其中$V$​为区域$\mathcal R$​的体积，结合上面的两个公式可得<br>$$<br>p(\pmb x)&#x3D;{K \over {NV} }<br>$$<br>这个公式就是非参估计中的核心。实践中非参数密度估计通常使用两种方式，一是固定区域大小$V$，统计落入不同区域的数量，如直方图法和核方法；二是改变区域大小使得落入每个区域的样本数量为$K$，如$K$近邻。</p><p>非参数密度估计的弱点在于，其大部分方法需要保留整个数据集（直方图方法除外），而参数估计不需要，因此在存储和计算上相对弱；但是非参数估计可以在不设置参数的情况下直接进行密度估计，其操作性会比参数估计高。</p><h4 id="直方图方法"><a href="#直方图方法" class="headerlink" title="直方图方法"></a>直方图方法</h4><p>直方图方法是一种非常直观的估计连续变量密度函数的方法，可以表示为一种柱状图。</p><p>以一维随机变量为例，首先将其取值范围分成$M$个连续的、不重叠的区间（bin），每个区间的宽度为$\triangle_m$。给定$N$个训练样本$D&#x3D;\lbrace x^{(n)}\rbrace_{n&#x3D;1}^N$我们统计这些样本落入每个区间的数量$K_m$，然后将它们归一化为密度函数。<br>$$<br>p_m&#x3D;{K_m \over {N\triangle_m} }, \ \ \ \ 1\le m\le M<br>$$<br>区间宽度$\triangle_m$的取值偏向于先验，过大会导致估计精度降低，过小会导致部分桶中没有数据，估计不精确。</p><p><img src="/post/888256ea/image-20230130003738091.png" alt="image-20230130003738091"></p><p>直方图方法需要的样本数量会随着维度$D$的增加而指数增长，从而导致维度灾难。</p><h4 id="核密度估计（核方法）"><a href="#核密度估计（核方法）" class="headerlink" title="核密度估计（核方法）"></a>核密度估计（核方法）</h4><p>核密度估计是对直方图方法的一种改进，假设$\mathcal R$是$D$维空间中的一个以点$\pmb x$为中心的“超立方体”，边长为$H$，并定义核函数来表示一个样本$\pmb z$是否落入该超立方体中。<br>$$<br>\phi({ {\pmb z-\pmb x}\over H} )&#x3D;<br>\begin{cases}<br>1\ \ \ \ if|z_i-x_i|&lt; {H\over 2},1\le i\le D \\<br>0\ \ \ \ else<br>\end{cases}<br>$$<br>点$\pmb x$的密度估计则为<br>$$<br>p(\pmb x)&#x3D;{K \over {NH^D} } &#x3D; {1 \over {NH^D} } \sum_{n&#x3D;1}^N \phi( { {\pmb x^{(n)} -\pmb x} \over H } )<br>$$<br>其中$\sum_{n&#x3D;1}^N \phi( { {\pmb x^{(n)} -\pmb x} \over H } )$表示落入到$\mathcal R$的样本数量。</p><h4 id="K近邻方法"><a href="#K近邻方法" class="headerlink" title="K近邻方法"></a>K近邻方法</h4><p>核密度估计方法中的核宽度是固定的，因此同一个宽度可能对高密度的区域过大，而对低密度的区域过小．一种更灵活的方式是设置一种<strong>可变宽度的区域</strong>，并使得落入每个区域中样本数量为固定的$K$。</p><p>要估计点$\pmb x$的密度，首先找到一个以$\pmb x$为中心的球体，使得落入球体的样本数量为$K$，然后根据非参估计公式得出点$\pmb x$的密度，因为落入球体的样本也是离$\pmb x$最近的$K$个样本，因此这种方法也称为$K$近邻方法。</p><p><img src="/post/888256ea/image-20230130010846714.png" alt="image-20230130010846714"></p><h3 id="半监督的应用"><a href="#半监督的应用" class="headerlink" title="半监督的应用"></a>半监督的应用</h3><p>半监督学习中，自训练和协同训练十分的典型，也在各个领域中有所应用，如图像任务中的旋转角度预测，文本任务中的掩码语言模型等。</p><p><img src="/post/888256ea/image-20230130011242192.png" alt="image-20230130011242192"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>注意力机制与外部记忆笔记</title>
      <link href="/post/27dfad29.html"/>
      <url>/post/27dfad29.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="注意力机制与外部记忆"><a href="#注意力机制与外部记忆" class="headerlink" title="注意力机制与外部记忆"></a>注意力机制与外部记忆</h1><h2 id="鸡尾酒会效应"><a href="#鸡尾酒会效应" class="headerlink" title="鸡尾酒会效应"></a>鸡尾酒会效应</h2><p>人脑处理过载的输入信息时，会使用注意力和记忆机制来解决。</p><p>鸡尾酒会效应内容是：当一个人在吵闹的鸡尾酒会上和朋友聊天时，尽管周围噪音干扰很多，他还是可以听到朋友的谈话内容，而忽略其他人的声音<strong>（聚焦式注意力）</strong>。同时，如果背景声中有重要的词（比如他的名字），他会马上注意到<strong>（显著性注意力）</strong>。这就体现了注意力机制中十分重要的两种机制：聚焦和显著。</p><ul><li>聚焦式注意力（会聚）：指有预定目的、依赖任务的，主动有意识地聚焦于某一对象的注意力。</li><li>显著性注意力（汇聚）：是由外界刺激驱动的注意，不需要主动干预，也和任务无关，对于重点信息造成的刺激会进行捕捉。</li></ul><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>用$\pmb X&#x3D;[\pmb x_1,…,\pmb x_n]\in \mathbb R^{D\times N}$表示$N$组输入信息，其中$D$维向量$\pmb x_n\in \mathbb R^D,n\in[1,N]$表示一组输入信息，注意力机制的计算可以分为两步：一是在所有输入信息上计算注意力分布，二是根据注意力分布来计算输入信息的加权平均。</p><h3 id="软性注意力"><a href="#软性注意力" class="headerlink" title="软性注意力"></a>软性注意力</h3><p>软性注意力不是0-1分布（选or不选），而是依概率$p$分布，以$p$的概率选择，这样方便求导操作，从而便于搭模型。</p><p>软性注意力机制可以分为以下两步：</p><ol><li><p>计算注意力分布$\alpha$；<br>$$<br>\alpha_n&#x3D;p(z&#x3D;n|\pmb X,\pmb q)&#x3D;softmax(s(\pmb x_n,\pmb q))&#x3D; { { \exp(s(\pmb x_n,\pmb q)) }\over { { \sum_{j&#x3D;1}^N}\exp(s(\pmb x_n,\pmb q)) } }<br>$$<br>其中$s(\pmb x_n,\pmb q)$是打分函数，是用来计算其重要程度，分数越高其重要性越强。打分函数的模型有以下几种：</p><p><img src="/post/27dfad29/image-20230120004646576.png" alt="image-20230120004646576"></p></li><li><p>根据$\alpha$来计算输入信息的加权平均；<br>$$<br>att(\pmb X,\pmb q)&#x3D;\sum_{n&#x3D;1}^N \alpha_n\pmb x_n&#x3D;\mathbb E_{z\sim p(\pmb z|\pmb X,\pmb q)}[\pmb {x_z}]<br>$$</p></li></ol><p>为了能够有效计算，引入查询向量来计算注意力分布。</p><p><img src="/post/27dfad29/image-20230120004913881.png" alt="image-20230120004913881"></p><p>注意力机制在自然语言处理方向中有很多的应用在：</p><p><img src="/post/27dfad29/image-20230120005454592.png" alt="image-20230120005454592"></p><h3 id="硬性注意力"><a href="#硬性注意力" class="headerlink" title="硬性注意力"></a>硬性注意力</h3><p>离散的注意力机制，输出结果是0-1向量，即选或不选。硬性注意力一般会选取最高概率$att(\pmb X,\pmb q)&#x3D;\pmb x_\hat n$​来进行最大采样或在注意力分布上随机采样来实现。这会使得最终的损失函数与注意力分布之间的函数关系不可导，无法使用反向传播算法进行训练。因此，硬性注意力通常需要使用强化学习来进行训练，为了使用反向传播算法，一般使用软性注意力来代替硬性注意力。</p><h3 id="键值对注意力"><a href="#键值对注意力" class="headerlink" title="键值对注意力"></a>键值对注意力</h3><p>更一般地，我们可以用键值对（key-value pair）格式来表示输入信息，其中“键”用来计算注意力分布$\alpha_n$，“值“用来计算聚合信息。<br>$$<br>att( [ \pmb K, \pmb V],\pmb q)&#x3D;\sum_{n&#x3D;1}^N \alpha_ n \pmb v_ n&#x3D; \sum_{n&#x3D;1}^N{ { \exp(s( \pmb k_ n, \pmb q))} \over{ \sum_ j \exp(s( \pmb k_ j, \pmb q))} } \pmb v_ n<br>$$<br>当$\pmb{K&#x3D;V}$时，键值对注意力就会退化成软性注意力。</p><h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h3><p>多头注意力是利用多个查询$\pmb Q&#x3D;[\pmb q_1,…,\pmb q_M]$来并行地从输入信息中选取多组信息，用多个查询并行处理，每个注意力关注输入信息的不同部分。<br>$$<br>att((\pmb K,\pmb V),\pmb Q)&#x3D;att((\pmb K,\pmb V),\pmb q_1)\oplus\ …\  \oplus att((\pmb K,\pmb V),\pmb q_M)<br>$$</p><h3 id="结构化注意力"><a href="#结构化注意力" class="headerlink" title="结构化注意力"></a>结构化注意力</h3><p>设定输入信息<strong>本身具有层次结构</strong>，比如文本可以分为词、句子、段落、篇章等不同粒度的层次，彼此之间有着关联影响，我们可以使用层次化的注意力来进行更好的信息选择。此外，还可以假设注意力为上下文相关的二项分布，然后就可以用一种图模型来构建更为复杂的结构化注意力分布。</p><h3 id="指针网络"><a href="#指针网络" class="headerlink" title="指针网络"></a>指针网络</h3><p>我们可以只利用注意力机制中的第一步，将注意力分布作为一个软性的指针来指出相关信息的位置。</p><p><img src="/post/27dfad29/image-20230124170855923.png" alt="image-20230124170855923"></p><p>如上图所示，收到’&gt;’信息之后，模型开始计算注意力分布，$h_4$的隐状态输出和$h_1$的相似度最大，因此取$h_5$输入20，$h_5$注意力计算出隐状态和$h_3$相似度最大，故$h_6$输入10，以此类推，实现文本数据从大到小排序。$h_i$的隐状态可以通过RNN实现。</p><h2 id="自注意力机制"><a href="#自注意力机制" class="headerlink" title="自注意力机制"></a>自注意力机制</h2><p>当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列。</p><p><img src="/post/27dfad29/image-20230125003358328.png" alt="image-20230125003358328"></p><p>如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：一种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互；另一种方法是使用全连接网络。全连接网络是一种非常直接的建模远距离依赖的模型，但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的。这时我们就可以利用注意力机制来<strong>“动态”地生成不同连接的权重</strong>，这就是自注意力模型。</p><p>为了提高模型能力，自注意力模型经常采用查询-键-值（Query-Key-Value，QKV）模式：</p><p><img src="/post/27dfad29/image-20230126004631128.png" alt="image-20230126004631128"></p><p>自注意力的计算方式如下：假设输入序列为$\pmb X&#x3D;[\pmb x_1,…,\pmb x_N]\in \mathbb R^{D_x\times N}$，输出序列为$\pmb H&#x3D;[\pmb h_1,…,h_N]\in \mathbb R^{D_v \times N}$；自注意力首先会分别与三个权重矩阵相乘，生成$\pmb {Q,K,V}$三个向量序列，分别代表查询向量、键向量和值向量：<br>$$<br>\begin{align}<br>\pmb Q&#x3D;\pmb W_q\pmb \in \mathbb R^{D_k\times N} \\<br>\pmb K&#x3D;\pmb W_k\pmb \in \mathbb R^{D_k\times N} \\<br>\pmb V&#x3D;\pmb W_v\pmb \in \mathbb R^{D_v\times N}<br>\end{align}<br>$$<br>通过$\pmb {K,V}$来进行键值对注意力机制求解，获得隐状态$h_n$<br>$$<br>\pmb h_n &#x3D; att((\pmb K,\pmb V),\pmb q_n)&#x3D;\sum_{j&#x3D;1}^N\alpha_{nj}\pmb v_j&#x3D;\sum_{j&#x3D;1}^N softmax(s(\pmb k_j,\pmb q_n))\pmb v_j<br>$$<br>其中$n,j\in[1,N]$为输出和输入向量序列的位置，$\alpha_{nj}$表示第$n$个输出关注到第$j$个输入的权重。自注意力的”自”就来源于查询向量$\pmb Q$是自有的。</p><p><img src="/post/27dfad29/image-20230126004743312.png" alt="image-20230126004743312"></p><p>如果使用缩放点积来作为注意力打分函数，输出向量序列可以简写为<br>$$<br>\pmb H &#x3D; \pmb V softmax({ {\pmb K^\top \pmb Q} \over {\sqrt{D_k}} })<br>$$<br><img src="/post/27dfad29/image-20230126003515471.png" alt="image-20230126003515471"></p><p>其中实线表示可学习的权重，虚线表示动态生成的权重．由于自注意力模型的权重是<strong>动态生成的</strong>，因此可以处理变长的信息序列。</p><h3 id="多头自注意力"><a href="#多头自注意力" class="headerlink" title="多头自注意力"></a>多头自注意力</h3><p>自注意力也可以扩展到多头形式，将一个向量矩阵通过不同参数矩阵形成不同的结果，然后拼接起来。</p><p><img src="/post/27dfad29/image-20230126004358890.png" alt="image-20230126004358890"></p><h3 id="Transformer编码器"><a href="#Transformer编码器" class="headerlink" title="Transformer编码器"></a>Transformer编码器</h3><p>自注意力机制通过动态生成权重，解决了变长问题，但丢失了位置信息，因此Transformer引入了位置编码与更多的操作来实现动态权重和位置信息的统一，从而大幅度提高了模型的处理能力。</p><ul><li>位置编码：在编码前加入位置编码向量，表示位置的相对信息</li><li>层归一化：防止梯度爆炸</li><li>直连边：方便线性逼近，提高运行效率</li><li>逐位FNN：先升维后降维</li></ul><p><img src="/post/27dfad29/image-20230126004959194.png" alt="image-20230126004959194"></p><p>Transformer的过程同样是从一个点位出发，建模与其他点位的关系，且上面的一个编码器可以累加，提高模型的表达能力与处理能力。</p><p><img src="/post/27dfad29/image-20230126010019708.png" alt="image-20230126010019708"></p><p><img src="/2020063017264495.gif" alt="img"></p><p>目前，基于Transformer的模型非常非常多，这个还需要以后专门钻研学习。</p><p><img src="/post/27dfad29/image-20230126013456670.png" alt="image-20230126013456670"></p><h2 id="人脑中的记忆"><a href="#人脑中的记忆" class="headerlink" title="人脑中的记忆"></a>人脑中的记忆</h2><p>记忆是人脑对于外界信息做出的内部存储，人脑的记忆过程可以分为工作记忆（短期记忆）、情景记忆、结构记忆（长期记忆），且人脑的记忆特点是联想记忆，即是指一种学习和记住不同对象之间关系的能力。联想记忆是指一种可以通过内容匹配的方法进行寻址的信息存储方式，也称为<strong>基于内容寻址的存储</strong>。</p><p><img src="/post/27dfad29/image-20230126012419701.png" alt="image-20230126012419701"></p><h2 id="结构化外部记忆"><a href="#结构化外部记忆" class="headerlink" title="结构化外部记忆"></a>结构化外部记忆</h2><p>为了增强网络容量，我们可以引入辅助记忆单元，将一些和任务相关的信息保存在辅助记忆中，在需要时再进行读取，这样可以有效地增加网络容量．这个引入的辅助记忆单元一般称为<strong>外部记忆</strong>。</p><p>外部记忆主要由主网络、外部记忆和读写操作三部分构成，</p><p><img src="/post/27dfad29/image-20230126013436851.png" alt="image-20230126013436851"></p><p>外部记忆定义为矩阵$M\in R^{d\times k}$，其中$k$是记忆片段的数量，$d$是每个记忆片段的大小；外部记忆的类型可以分为：</p><ul><li>只读<ul><li>记忆网络：专用于存储的网络结构</li><li>RNN中的$h_t$：用循环网络的隐状态存储部分信息</li></ul></li><li>可读写<ul><li>NTM</li></ul></li></ul><p>要实现类似于人脑神经网络的联想记忆能力，就需要<strong>按内容寻址的方式</strong>进行定位，然后进行读取或写入操作．按内容寻址通常使用<strong>注意力机制</strong>来进行。通过引入外部记忆，可以将神经网络的参数和记忆容量“分离”，即在少量增加网络参数的条件下可以大幅增加网络容量．因此，我们可以将注意力机制看作一个接口，将信息的存储与计算分离。</p><h3 id="记忆网络"><a href="#记忆网络" class="headerlink" title="记忆网络"></a>记忆网络</h3><p>端到端记忆网络采用一种可微的网络结构，可以多次从外部记忆中读取信息。在端到端记忆网络中，外部记忆单元是只读的。为了实现更复杂的计算，我们可以让主网络和外部记忆进行多轮交互。</p><p>给定一组需要存储的信息$m_{1,N}$，外部记忆首先将其分成寻址片段$A&#x3D;[\pmb a_1,…,\pmb a_n]$和输出片段$C&#x3D;[\pmb c_1,…,\pmb c_n]$。开始阶段，主网络接收输入$\pmb x$并通过预测函数$f(·)$，生成输入$\pmb q_1&#x3D;f(\pmb x)$，然后执行下面的步骤：</p><ol><li><p>使用键值对注意力从外部记忆读取相关信息<br>$$<br>\pmb r^{(k)}&#x3D;\pmb C^{(k)}softmax(\pmb A^{(k)}\pmb q^{(k)})<br>$$</p></li><li><p>整合注意力读取的结果，生成新的查询向量<br>$$<br>\pmb q^{(k+1)}&#x3D;\pmb r^{(k)}+\pmb q^{(k)}<br>$$</p></li></ol><p>以上过程可以迭代$K$次，以实现复杂计算。</p><p><img src="/post/27dfad29/image-20230126145731384.png" alt="image-20230126145731384"></p><h3 id="神经图灵机"><a href="#神经图灵机" class="headerlink" title="神经图灵机"></a>神经图灵机</h3><p>图灵机是一种抽象数学模型，主要用来模拟任何可计算问题，通过控制器从无限长的纸带上读写记忆。</p><p><img src="/post/27dfad29/image-20230126154355402.png" alt="image-20230126154355402"></p><p>图灵机由控制器、符号表、读写头、寄存器和纸带五个部分组成，其中可以看出当今计算机架构的一些影子，而神经图灵机基于此，主要由两个部件构成：控制器和外部记忆。其中控制器通过读写操作完成对外部记忆的处理和读取，外部记忆定义为矩阵$M\in \mathbb R^{D\times N}$，其中$D$表示记忆片段大小，$N$表示记忆片段数量；控制器为一个前馈或循环神经网络。整个架构是可微分的。</p><p><img src="/post/27dfad29/ntm1.jpeg" alt="img"></p><p>神经图灵机在每个时刻$t$，控制器接受当前时刻的输入$\pmb x_t$、上一时刻的输出$\pmb h_{t-1}$和上一时刻从外部记忆中读取的信息$\pmb r_{t-1}$，并产生输出$\pmb h_t$，同时生成和读写外部记忆相关的三个向量：查询向量$\pmb q_t$、删除向量$\pmb e_t$和增加向量$\pmb a_t$．然后对外部记忆$\pmb M_t$进行读写操作，生成读向量$\pmb r_t$和新的外部记忆$\pmb M_{t+1}$。</p><h4 id="读操作"><a href="#读操作" class="headerlink" title="读操作"></a>读操作</h4><p>在时刻$t$，外部记忆的内容为$\pmb M_t&#x3D;[\pmb m_{t,1},…,\pmb m_{t,N}]$，读操作为从外部记忆$\pmb M_t$中读取信息$\pmb r_t\in\mathbb R^D$，首先通过注意力机制来进行基于内容的寻址，即<br>$$<br>\alpha_{t,n}&#x3D;softmax(s(\pmb m_{t,n},\pmb q_t))<br>$$<br>根据注意力分布$\alpha_t$，可以计算读向量$\pmb r_t$，作为下一个时刻控制器的输入：<br>$$<br>\pmb r_t&#x3D;\sum_{n&#x3D;1}^N \alpha_n \pmb m_{t,n}<br>$$</p><h4 id="写操作"><a href="#写操作" class="headerlink" title="写操作"></a>写操作</h4><p>外部记忆的写操作可以分解为两个子操作：删除和增加，分别基于删除向量和增加向量按比例操作。通过写操作得到下一时刻的外部记忆$\pmb M_{t+1}$<br>$$<br>\pmb m_{t+1,n}&#x3D;\pmb m_{t,n}(1-\alpha_{t,n}\pmb e_t)+\alpha_{t,n}\pmb a_{t,n}\ \ \ \   \forall n\in[1,N]<br>$$<br><img src="/post/27dfad29/image-20230126154945547.png" alt="image-20230126154945547"></p><h2 id="基于神经动力学的联想学习"><a href="#基于神经动力学的联想学习" class="headerlink" title="基于神经动力学的联想学习"></a>基于神经动力学的联想学习</h2><p>结构化的外部记忆更多是受现代计算机架构的启发，将计算和存储功能进行分离，这些外部记忆的结构也缺乏生物学的解释性。为了具有更好的生物学解释性，还可以将基于神经动力学（Neurodynamics）的联想记忆模型引入到神经网络以增加网络容量。</p><h3 id="自联想和异联想"><a href="#自联想和异联想" class="headerlink" title="自联想和异联想"></a>自联想和异联想</h3><p>联想记忆模型（Associative Memory Model）主要是通过神经网络的动态演化来进行联想，有两种应用场景．</p><ol><li>输入的模式和输出的模式在<strong>同一空间</strong>，这种模型叫作自联想模型（Auto Associative Model）。自联想模型可以通过前馈神经网络或者循环神经网络来实现，也常称为自编码器（Auto-Encoder，AE）。</li><li>输入的模式和输出的模式<strong>不在同一空间</strong>，这种模型叫作异联想模型（Hetero-Associative Model）．从广义上讲，<strong>大部分机器学习问题都可以被看作异联想</strong>，因此异联想模型可以作为分类器使用。</li></ol><p><img src="/post/27dfad29/image-20230126174322446.png" alt="image-20230126174322446"></p><h3 id="Hopfield网络"><a href="#Hopfield网络" class="headerlink" title="Hopfield网络"></a>Hopfield网络</h3><p>hopfield网络将神经网络作为记忆的存储和检索模型，是一种循环神经网络模型，由一组互相连接的神经元组成。Hopfield 网络也可以认为是所有神经元都互相连接的不分层的神经网络。<strong>每个神经元既是输入单元，又是输出单元，没有隐藏神经元。</strong>一个神经元和自身没有反馈相连，不同神经元之间连接权重是对称的。</p><p><img src="/post/27dfad29/image-20230126174923319.png" alt="image-20230126174923319"></p><h4 id="更新规则"><a href="#更新规则" class="headerlink" title="更新规则"></a>更新规则</h4><p>假设一个Hopfield网络有$M$个神经元，第$i$个神经元的更新规则为：<br>$$<br>s_i&#x3D;\begin{cases} +1 \ \ \ \ if \sum_{j&#x3D;1}^Mw_{ij}s_j+b_i\ge0 \\ -1 \ \ \ \ otherwise, \end{cases}<br>$$<br>其中<br>$$<br>\begin{cases}w_ii&#x3D;0 \ \ \ \ \forall i\in[1,M] \\ w_{ij}&#x3D;w_{ji} \ \ \ \ \forall i,j\in[1,M]\end{cases}<br>$$<br>从$s_i$的更新方式中可以看出Hopfield网络的规则是使用相邻单元的权重进行更新，每次做线性加和。更新方式中，异步更新每次更新一个神经元，其可以随机或者固定；同步更新一次性更新所有神经元，更新时满足非线性阶跃函数$f(·)$<br>$$<br>\pmb s_t&#x3D;f(\pmb W\pmb s_{t-1}+\pmb b)<br>$$<br>其中$\pmb s_0&#x3D;\pmb x,\pmb W&#x3D;[ w_{ij} ]_ { M\times M }$为连接权重，$\pmb b&#x3D;[b_i]_ {M \times 1}$为偏置向量。</p><h4 id="能量函数"><a href="#能量函数" class="headerlink" title="能量函数"></a>能量函数</h4><p>在Hopfield网络中，我们给每个不同的网络状态定义一个标量属性，称为“能量”。<br>$$<br>E&#x3D;-{1\over 2}\sum_{i,j}w_{ij}s_is_j-\sum_ib_is_i &#x3D; -{1\over 2}\pmb{s^\top Ws-b^\top s}<br>$$<br>符合物理规律，Hopfield网络应该是稳定的，即能量函数经过多次迭代后会达到收敛状态。<strong>权重对称</strong>是一个重要特征，因为它保证了能量函数在神经元激活时单调递减，而不对称的权重可能导致周期性振荡或者混乱。</p><h4 id="检索过程（联想记忆）"><a href="#检索过程（联想记忆）" class="headerlink" title="检索过程（联想记忆）"></a>检索过程（联想记忆）</h4><p>给定一个外部输入，网络经过演化会达到某个稳定状态。Hopfield网络存在有限的吸引点（Attractor），即能量函数的局部最小点。</p><p>检索过程如下：</p><ul><li>每个吸引点$\pmb u$都对应一个“管辖”区域$\mathcal R_{\pmb u}$．若输入向量$\pmb x$落入这个区域，网络最终会收敛到$\pmb u$。</li><li>吸引点可以看作是网络中存储的信息。</li></ul><p><img src="/post/27dfad29/image-20230126234400661.png" alt="image-20230126234400661"></p><h4 id="存储过程"><a href="#存储过程" class="headerlink" title="存储过程"></a>存储过程</h4><p>信息存储是指将一组向量$\pmb x_1,…,\pmb x_N$存储在网络中的过程，存储过程主要是调整神经元之间的连接权重，因此可以看作是一种学习过程。</p><p>神经元$i$和$j$之间的连接权重为：<br>$$<br>w_{ij}&#x3D;{1\over N}\sum_{n&#x3D;1}^N x_i^{(n)}x_j^{(n)}<br>$$<br>其中$x_i^{(n)}$是第$i$个神经元的第$n$个分量。这个公式表达了这样的意思：如果两个神经元经常同时激活，则它们之间的连接会加强；否则则会消失。这种学习方式和人脑的赫布法则十分的类似。权重的更新公式可以使得权重更新时所得到的能量相对最低，其证明可以将能量式在权重$w_{ij}$上求导。</p><p>这种通过神经元之间权重来存储的方式，其存储容量相对有限，模型容量一般与网络结构和学习方式有关。Hopfield 网络的最大容量为$0.14M$，玻尔兹曼机的容量为$0.6m$， 但是其学习效率比较低，需要非常长时间的演化才能达到均衡状态。</p><p><img src="/post/27dfad29/image-20230127000447421.png" alt="image-20230127000447421"></p><h3 id="联想记忆增加网络容量"><a href="#联想记忆增加网络容量" class="headerlink" title="联想记忆增加网络容量"></a>联想记忆增加网络容量</h3><p>既然联想记忆具有存储和检索功能，我们可以利用联想记忆来增加网络容量。和结构化的外部记忆相比，联想记忆具有更好的生物学解释性。比如，我们可以将一个联想记忆模型作为部件引入 LSTM 网络中，从而在不引入额外参数的情况下增加网络容量。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo+Github+Netlify+CMS实现全站访问加速与在线编辑</title>
      <link href="/post/1b20cec9.html"/>
      <url>/post/1b20cec9.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hexo-Github-Netlify-CMS实现全站访问加速与在线编辑"><a href="#Hexo-Github-Netlify-CMS实现全站访问加速与在线编辑" class="headerlink" title="Hexo+Github+Netlify+CMS实现全站访问加速与在线编辑"></a>Hexo+Github+Netlify+CMS实现全站访问加速与在线编辑</h1><p>Github Pages因为一些众所周知的原因，在国内访问速度实在是太慢，因为决定采用Github和Netlify双线部署的方式，Github Pages保留，同时将域名接到Netlify Pages，通过Netlify加速博客访问；同时，Netlify CMS提供了在线编辑博客的功能，这可以在本地环境有所变化的时候更方便的编写博客与部署，因此这里一并实现部署。</p><h2 id="域名申请"><a href="#域名申请" class="headerlink" title="域名申请"></a>域名申请</h2><p>这里就不再赘述，大家自行申请域名即可，我这里使用的是万网的域名<code>paopao0226.site</code></p><h2 id="Netlify-Pages配置"><a href="#Netlify-Pages配置" class="headerlink" title="Netlify Pages配置"></a>Netlify Pages配置</h2><p>注册过程不再赘述，需要使用Github进行登录。注册完成之后，点击<code>Add new site</code>或相似含义按钮，创建新的网站。</p><p><img src="/post/1b20cec9/image-20230125141932964.png" alt="image-20230125141932964"></p><p>选择Github仓库</p><p><img src="/post/1b20cec9/image-20230124154707813.png" alt="image-20230124154707813"></p><p>第一次使用Github时需要手动登录一下，之后就可以直接访问所有仓库项目了，选择博客所对应的仓库。</p><p><img src="/post/1b20cec9/image-20230124154747680.png" alt="image-20230124154747680"></p><p>继续下一步，页面会显示当前仓库信息和分支，如果博客在其他分支则自行手动改一下，其他配置可以不用动。</p><p><img src="/post/1b20cec9/image-20230124155325503.png" alt="image-20230124155325503"></p><p>三步完成，网站就建完了，十分简便</p><p><img src="/post/1b20cec9/image-20230124155348868.png" alt="image-20230124155348868"></p><p>为了方便，可以先改一下网页的二级地址前缀，选择<code>site settings-&gt;General-&gt;site details-&gt;site information-&gt;change site name</code></p><p><img src="/post/1b20cec9/image-20230125142528292.png" alt="image-20230125142528292"></p><p>之后点击<code>site overview-&gt;production deploy</code>或在顶栏选择<code>deploys</code>，进行第一次部署，等待成功部署。</p><p><img src="/post/1b20cec9/image-20230124155405191.png" alt="image-20230124155405191"></p><p>部署成功之后，点击<code>preview</code>，或者退回到<code>site overview</code>，找到主页地址，均可以预览网站效果。</p><p><img src="/post/1b20cec9/image-20230124155417519.png" alt="image-20230124155417519"></p><p>（这个图只是指路，截图里是我已经配置了域名的入口）</p><p><img src="/post/1b20cec9/image-20230125143041898.png" alt="image-20230125143041898"></p><p><img src="/image-20230125143152997.png" alt="image-20230125143152997"></p><h2 id="域名配置与解析"><a href="#域名配置与解析" class="headerlink" title="域名配置与解析"></a>域名配置与解析</h2><p>完成建站后，点击<code>site settings-&gt;Domain management-&gt;Domains-&gt;Custom domains</code>，点击<code>Add custom domain</code>，将域名输入进去。</p><p><img src="/post/1b20cec9/image-20230124155802252.png" alt="image-20230124155802252"></p><p><img src="/post/1b20cec9/image-20230124155814286.png" alt="image-20230124155814286"></p><p>之后我们会看到域名已经配置到网站中，但是DNS未成功配置。</p><p><img src="/post/1b20cec9/image-20230124155902591.png" alt="image-20230124155902591"></p><p>这时需要先复制好网站原二级域名，即红框中的网址。</p><p><img src="/post/1b20cec9/image-20230124160135151.png" alt="image-20230124160135151"></p><p>然后来到域名的解析页面，我这里是阿里云万网的解析页面，按照下图的配置信息将域名解析，其中TTL可以设置短一些，境外配置可以去掉。</p><p><img src="/post/1b20cec9/image-20230124160707161.png" alt="image-20230124160707161"></p><p>静待一段时间，回到网站<code>Domains</code>设置界面，就能够看到DNS检查提醒消失，这就代表域名解析已经完成。</p><p><img src="/post/1b20cec9/image-20230124160801239.png" alt="image-20230124160801239"></p><p>Netlify自带了Https证书申请机制，以实现网站的Https强制设置与访问，在<code>Domains</code>设置界面向下滑，可以找到<code>SSL/TLS证书</code>界面，如果没有自动申请，则自行点击<code>Let&#39;s Encrypt...</code>按钮，等到出现以下界面即可。</p><p><img src="/post/1b20cec9/image-20230124161601051.png" alt="image-20230124161601051"></p><h2 id="Netlify-CDN配置"><a href="#Netlify-CDN配置" class="headerlink" title="Netlify CDN配置"></a>Netlify CDN配置</h2><p>回到刚才的<code>Domains-&gt;Custom Domain</code>，点击自定义域名的<code>Options</code>按钮，选择<code>Add Domain</code>，一直下一步，将CDN设置好，直至出现以下界面。</p><p><img src="/post/1b20cec9/image-20230125144423564.png" alt="image-20230125144423564"></p><p>其中Netlify提供的他们自己的DNS地址，即页面中的<code>Name Servers</code>，粘贴其中的两个，将其配置到域名管理中的<code>DNS管理 </code>中，我这里是万网的页面。</p><p><img src="/post/1b20cec9/image-20230125144658980.png" alt="image-20230125144658980"></p><p>修改了DNS之后，默认的生效时间是48小时，这个会和服务商当前时间的忙碌程度有关，我这里大概是在一天之后生效的。</p><p><img src="/post/1b20cec9/image-20230125144904386.png" alt="image-20230125144904386"></p><p>在本地cmd中输入<code>nslookup 域名地址</code>，如果出现了多个IP地址，则代表CDN已经生效了。</p><p><img src="/post/1b20cec9/image-20230125145729986.png" alt="image-20230125145729986"></p><p>这样就完成了网站的全部配置啦！</p><h2 id="CMS配置"><a href="#CMS配置" class="headerlink" title="CMS配置"></a>CMS配置</h2><p>在hexo的本地文件夹中左键打开<code>git bash</code>界面，安装CMS</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm i hexo-netlify-cms --save<br>// or yarn<br>yarn add hexo-netlify-cms<br></code></pre></td></tr></table></figure><p><img src="/post/1b20cec9/image-20230125150624747.png" alt="image-20230125150624747"></p><p><img src="/post/1b20cec9/image-20230125150730536.png" alt="image-20230125150730536"></p><p><img src="/post/1b20cec9/image-20230125150912833.png" alt="image-20230125150912833"></p><p><img src="/post/1b20cec9/image-20230125151202948.png" alt="image-20230125151202948"></p><p><img src="/post/1b20cec9/image-20230125151302911.png" alt="image-20230125151302911"></p><p><img src="/post/1b20cec9/image-20230125151402818.png" alt="image-20230125151402818"></p><p><img src="/post/1b20cec9/image-20230125151510523.png" alt="image-20230125151510523"></p><p><img src="/post/1b20cec9/image-20230125151855112.png" alt="image-20230125151855112"></p><p><img src="/post/1b20cec9/image-20230125151935147.png" alt="image-20230125151935147"></p><p><img src="/post/1b20cec9/image-20230125155309385.png" alt="image-20230125155309385"></p><p><img src="/post/1b20cec9/image-20230125155326131.png" alt="image-20230125155326131"></p><p><img src="/post/1b20cec9/image-20230125155441176.png" alt="image-20230125155441176"></p>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> github </tag>
            
            <tag> netlify </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo Fluid博客魔改笔记</title>
      <link href="/post/cf99d62e.html"/>
      <url>/post/cf99d62e.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hexo-Fluid博客魔改笔记"><a href="#Hexo-Fluid博客魔改笔记" class="headerlink" title="Hexo Fluid博客魔改笔记"></a>Hexo Fluid博客魔改笔记</h1><h3 id="鸣谢："><a href="#鸣谢：" class="headerlink" title="鸣谢："></a>鸣谢：</h3><ul><li>第2-8条修改方案参考自：<a href="http://lzqlearn.com/blog/6560820db005/">hexo博客fluid主题美化 - Gilgamesh’s Blog (lzqlearn.com)</a></li><li></li></ul><h2 id="公式渲染bug修改"><a href="#公式渲染bug修改" class="headerlink" title="公式渲染bug修改"></a>公式渲染bug修改</h2><p>见：<a href="/post/f44e7011.html">Hexo-Fluid的Mathjax数学公式渲染错误问题 - Ywj226</a></p><h2 id="Mac风格代码块"><a href="#Mac风格代码块" class="headerlink" title="Mac风格代码块"></a>Mac风格代码块</h2><p>在自定义CSS文件中加入以下代码：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">figure</span><span class="hljs-selector-class">.highlight</span> &#123;<br>    <span class="hljs-comment">/*</span><br><span class="hljs-comment">    background: #21252b;</span><br><span class="hljs-comment">    background: rgb(246,248,250);</span><br><span class="hljs-comment">    */</span><br>    <span class="hljs-attribute">border-radius</span>: <span class="hljs-number">8px</span>;<br>    <span class="hljs-attribute">box-shadow</span>: <span class="hljs-number">1px</span> <span class="hljs-number">2px</span> <span class="hljs-number">22px</span> <span class="hljs-number">1px</span> <span class="hljs-built_in">rgba</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, .<span class="hljs-number">3</span>);<br>    <span class="hljs-attribute">padding-top</span>: <span class="hljs-number">25px</span>;<br>    <span class="hljs-attribute">z-index</span>:<span class="hljs-number">998</span>;<br>&#125;<br><br><span class="hljs-selector-tag">figure</span><span class="hljs-selector-class">.highlight</span><span class="hljs-selector-pseudo">::before</span> &#123;<br>    <span class="hljs-attribute">background</span>: <span class="hljs-number">#fc625d</span>;<br>    <span class="hljs-attribute">border-radius</span>: <span class="hljs-number">50%</span>;<br>    <span class="hljs-attribute">box-shadow</span>: <span class="hljs-number">20px</span> <span class="hljs-number">0</span> <span class="hljs-number">#fdbc40</span>, <span class="hljs-number">40px</span> <span class="hljs-number">0</span> <span class="hljs-number">#35cd4b</span>;<br>    <span class="hljs-attribute">content</span>: <span class="hljs-string">&#x27; &#x27;</span>;<br>    <span class="hljs-attribute">height</span>: <span class="hljs-number">12px</span>;<br>    <span class="hljs-attribute">left</span>: <span class="hljs-number">12px</span>;<br>    <span class="hljs-attribute">margin-top</span>: -<span class="hljs-number">15px</span>;<br>    <span class="hljs-attribute">position</span>: absolute;<br>    <span class="hljs-attribute">width</span>: <span class="hljs-number">12px</span>;<br>    <span class="hljs-attribute">z-index</span>:<span class="hljs-number">999</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="标题样式修改"><a href="#标题样式修改" class="headerlink" title="标题样式修改"></a>标题样式修改</h2><p>在自定义CSS文件中加入以下代码：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h1</span>,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h2</span>,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h3</span>,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h4</span>,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h5</span>,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h6</span> &#123;<br>    <span class="hljs-attribute">position</span>: relative;<br>    <span class="hljs-attribute">margin-top</span>: <span class="hljs-number">1rem</span>;<br>    <span class="hljs-attribute">margin-bottom</span>: <span class="hljs-number">1rem</span>;<br>    <span class="hljs-attribute">font-weight</span>: bold;<br>    <span class="hljs-attribute">line-height</span>: <span class="hljs-number">1.4</span>;<br>    <span class="hljs-attribute">cursor</span>: text;<br>    <span class="hljs-attribute">overflow</span>: hidden;<br>&#125;<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h1</span><span class="hljs-selector-pseudo">:hover</span> <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.anchor</span>,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h2</span><span class="hljs-selector-pseudo">:hover</span> <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.anchor</span>,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h3</span><span class="hljs-selector-pseudo">:hover</span> <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.anchor</span>,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h4</span><span class="hljs-selector-pseudo">:hover</span> <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.anchor</span>,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h5</span><span class="hljs-selector-pseudo">:hover</span> <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.anchor</span>,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h6</span><span class="hljs-selector-pseudo">:hover</span> <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.anchor</span> &#123;<br>    <span class="hljs-attribute">text-decoration</span>: none;<br>&#125;<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h1</span> tt,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h1</span> <span class="hljs-selector-tag">code</span> &#123;<br>    <span class="hljs-attribute">font-size</span>: inherit;<br>&#125;<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h2</span> tt,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h2</span> <span class="hljs-selector-tag">code</span> &#123;<br>    <span class="hljs-attribute">font-size</span>: inherit;<br>&#125;<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h3</span> tt,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h3</span> <span class="hljs-selector-tag">code</span> &#123;<br>    <span class="hljs-attribute">font-size</span>: inherit;<br>&#125;<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h4</span> tt,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h4</span> <span class="hljs-selector-tag">code</span> &#123;<br>    <span class="hljs-attribute">font-size</span>: inherit;<br>&#125;<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h5</span> tt,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h5</span> <span class="hljs-selector-tag">code</span> &#123;<br>    <span class="hljs-attribute">font-size</span>: inherit;<br>&#125;<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h6</span> tt,<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h6</span> <span class="hljs-selector-tag">code</span> &#123;<br>    <span class="hljs-attribute">font-size</span>: inherit;<br>&#125;<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h1</span> &#123;<br>    <span class="hljs-attribute">font-size</span>: <span class="hljs-number">2.25em</span>;<br>    <span class="hljs-attribute">line-height</span>: <span class="hljs-number">1.2</span>;<br>    <span class="hljs-attribute">border-bottom</span>: <span class="hljs-number">1px</span> solid <span class="hljs-number">#eee</span>;<br>&#125;<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h2</span> &#123;<br>    <span class="hljs-attribute">font-size</span>: <span class="hljs-number">1.75em</span>  ;<br>    <span class="hljs-attribute">line-height</span>: <span class="hljs-number">1.1</span>;<br>    <span class="hljs-comment">/*</span><br><span class="hljs-comment">    border-bottom: 0px solid #eee;</span><br><span class="hljs-comment">    */</span><br>    <span class="hljs-attribute">color</span>: <span class="hljs-number">#158956</span>;<br>&#125;<br><br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h3</span> &#123;<br>    <span class="hljs-attribute">font-size</span>: <span class="hljs-number">1.5em</span>;<br>    <span class="hljs-attribute">line-height</span>: <span class="hljs-number">1.1</span>;<br>    <span class="hljs-attribute">color</span>: <span class="hljs-built_in">RGB</span>(<span class="hljs-number">47</span>,<span class="hljs-number">85</span>,<span class="hljs-number">151</span>);<br>&#125;<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h4</span> &#123;<br>    <span class="hljs-attribute">font-size</span>: <span class="hljs-number">1.25em</span>;<br>&#125;<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h5</span> &#123;<br>    <span class="hljs-attribute">font-size</span>: <span class="hljs-number">1em</span>;<br>&#125;<br><span class="hljs-selector-class">.markdown-body</span> &gt; <span class="hljs-selector-tag">h6</span> &#123;<br>   <span class="hljs-attribute">font-size</span>: <span class="hljs-number">1em</span>;<br>    <span class="hljs-attribute">color</span>: <span class="hljs-number">#777</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="首行缩进"><a href="#首行缩进" class="headerlink" title="首行缩进"></a>首行缩进</h2><p>也可以使用<code>&amp;emsp;&amp;emsp;</code>来手动缩进。</p><p>在自定义CSS文件中加入以下代码：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">p</span> &#123;<br>    <span class="hljs-attribute">text-indent</span>: <span class="hljs-number">2em</span>; <span class="hljs-comment">/*首行缩进*/</span><br>&#125;<br><span class="hljs-selector-tag">li</span> &gt; <span class="hljs-selector-tag">p</span> &#123;<br>    <span class="hljs-attribute">text-indent</span>: <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="修改光标"><a href="#修改光标" class="headerlink" title="修改光标"></a>修改光标</h2><p>注意进行此项美化会导致<strong>大量的网络请求</strong>（虽然不会有大数据传输，但大量请求仍然很慢），其直观表现就是<strong>网页加载变慢</strong>。</p><p>在自定义CSS文件中加入以下代码：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">body</span>, <span class="hljs-selector-tag">header</span>, <span class="hljs-selector-tag">div</span><span class="hljs-selector-class">.mask</span><span class="hljs-selector-class">.flex-center</span>, <span class="hljs-selector-tag">div</span><span class="hljs-selector-class">.scroll-down-bar</span>, <span class="hljs-selector-id">#banner</span><span class="hljs-selector-class">.banner</span>, <span class="hljs-selector-tag">h1</span>,<span class="hljs-selector-tag">h2</span>,<span class="hljs-selector-tag">h3</span>,<span class="hljs-selector-tag">h4</span>,<span class="hljs-selector-tag">h5</span>,<span class="hljs-selector-tag">h6</span> &#123;<br>    <span class="hljs-attribute">cursor</span>: <span class="hljs-built_in">url</span>(<span class="hljs-string">https://typora-gilgamesh.oss-cn-shanghai.aliyuncs.com/img1/a11.cur</span>), auto;<br>&#125;<br><br><span class="hljs-selector-tag">a</span><span class="hljs-selector-pseudo">:hover</span>, <span class="hljs-selector-tag">button</span><span class="hljs-selector-pseudo">:hover</span>, <span class="hljs-selector-tag">i</span><span class="hljs-selector-pseudo">:hover</span> &#123;<br>    <span class="hljs-attribute">cursor</span>: <span class="hljs-built_in">url</span>(<span class="hljs-string">https://typora-gilgamesh.oss-cn-shanghai.aliyuncs.com/img1/a22.cur</span>), auto;<br>&#125;<br><br><span class="hljs-selector-id">#rightside</span>&gt;<span class="hljs-selector-tag">div</span>&gt;<span class="hljs-selector-tag">button</span>,<br><span class="hljs-selector-id">#rightside</span>&gt;<span class="hljs-selector-tag">div</span>&gt;<span class="hljs-selector-tag">a</span> &#123;<br>    <span class="hljs-attribute">display</span>: block;<br>    <span class="hljs-attribute">margin-bottom</span>: <span class="hljs-number">2px</span>;<br>    <span class="hljs-attribute">width</span>: <span class="hljs-number">30px</span>;<br>    <span class="hljs-attribute">height</span>: <span class="hljs-number">30px</span>;<br>    <span class="hljs-attribute">color</span>: <span class="hljs-built_in">var</span>(--btn-color);<br>    <span class="hljs-attribute">text-align</span>: center;<br>    <span class="hljs-attribute">font-size</span>: <span class="hljs-number">16px</span>;<br>    <span class="hljs-attribute">cursor</span>: <span class="hljs-built_in">url</span>(<span class="hljs-string">https://typora-gilgamesh.oss-cn-shanghai.aliyuncs.com/img1/a22.cur</span>), auto;<br>&#125;<br><br><span class="hljs-comment">/* 文章标签 */</span><br><span class="hljs-selector-class">.layout_post</span> <span class="hljs-selector-class">.tag_share</span> <span class="hljs-selector-class">.post-meta__tags</span> &#123;<br>    <span class="hljs-attribute">cursor</span>: <span class="hljs-built_in">url</span>(<span class="hljs-string">https://typora-gilgamesh.oss-cn-shanghai.aliyuncs.com/img1/a22.cur</span>), auto;<br>&#125;<br><span class="hljs-comment">/* 评论区 */</span><br><span class="hljs-selector-class">.vcol</span> * &#123;<br>    <span class="hljs-attribute">cursor</span>: <span class="hljs-built_in">url</span>(<span class="hljs-string">https://typora-gilgamesh.oss-cn-shanghai.aliyuncs.com/img1/a22.cur</span>), auto;<br>&#125;<br><span class="hljs-selector-class">.v</span><span class="hljs-selector-attr">[data-class=v]</span> <span class="hljs-selector-class">.vicon</span> &#123;<br>    <span class="hljs-attribute">cursor</span>: <span class="hljs-built_in">url</span>(<span class="hljs-string">https://typora-gilgamesh.oss-cn-shanghai.aliyuncs.com/img1/a22.cur</span>), auto;<br>&#125;<br><span class="hljs-selector-class">.v</span><span class="hljs-selector-attr">[data-class=v]</span> <span class="hljs-selector-class">.vbtn</span> &#123;<br>    <span class="hljs-attribute">cursor</span>: <span class="hljs-built_in">url</span>(<span class="hljs-string">https://typora-gilgamesh.oss-cn-shanghai.aliyuncs.com/img1/a22.cur</span>), auto;<br>&#125;<br><span class="hljs-comment">/* 回复 */</span><br><span class="hljs-selector-class">.vat</span> &#123;<br>    <span class="hljs-attribute">cursor</span>: <span class="hljs-built_in">url</span>(<span class="hljs-string">https://typora-gilgamesh.oss-cn-shanghai.aliyuncs.com/img1/a22.cur</span>), auto;<br>&#125;<br><span class="hljs-comment">/*分页器*/</span><br><span class="hljs-selector-id">#pagination</span> <span class="hljs-selector-class">.page-number</span><span class="hljs-selector-pseudo">:hover</span> &#123;<br>    <span class="hljs-attribute">cursor</span>: <span class="hljs-built_in">url</span>(<span class="hljs-string">https://typora-gilgamesh.oss-cn-shanghai.aliyuncs.com/img1/a22.cur</span>), auto;<br>&#125;<br><span class="hljs-comment">/* 分页器的三个点(...) */</span><br><span class="hljs-selector-class">.space</span> &#123;<br>    <span class="hljs-attribute">color</span>: <span class="hljs-number">#00c4b6</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="文章显示框加宽"><a href="#文章显示框加宽" class="headerlink" title="文章显示框加宽"></a>文章显示框加宽</h2><p>针对文章主体的Box进行修改，在自定义CSS文件中加入如下代码：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-class">.col-lg-8</span><span class="hljs-selector-class">.nopadding-x-md</span> &gt; <span class="hljs-selector-class">.container</span><span class="hljs-selector-class">.nopadding-x-md</span> &gt; <span class="hljs-selector-id">#board</span> &#123;<br>    <span class="hljs-attribute">width</span>: <span class="hljs-number">123%</span>;<br>    <span class="hljs-attribute">left</span>: -<span class="hljs-number">23%</span>; <br>&#125;<br><span class="hljs-keyword">@media</span> (<span class="hljs-attribute">max-width</span>: <span class="hljs-number">767px</span>) &#123;<br>    <span class="hljs-selector-class">.col-lg-8</span><span class="hljs-selector-class">.nopadding-x-md</span> &gt; <span class="hljs-selector-class">.container</span><span class="hljs-selector-class">.nopadding-x-md</span> &gt; <span class="hljs-selector-id">#board</span> &#123;<br>        <span class="hljs-attribute">width</span>: <span class="hljs-number">100%</span>;<br>        <span class="hljs-attribute">left</span>: <span class="hljs-number">0</span>; <br>    &#125;<br>&#125;<br><span class="hljs-keyword">@media</span> (<span class="hljs-attribute">max-width</span>: <span class="hljs-number">424px</span>) &#123;<br>    <span class="hljs-selector-class">.col-lg-8</span><span class="hljs-selector-class">.nopadding-x-md</span> &gt; <span class="hljs-selector-class">.container</span><span class="hljs-selector-class">.nopadding-x-md</span> &gt; <span class="hljs-selector-id">#board</span> &#123;<br>        <span class="hljs-attribute">width</span>: <span class="hljs-number">100%</span>;<br>        <span class="hljs-attribute">left</span>: <span class="hljs-number">0</span>; <br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="文章目录样式修改"><a href="#文章目录样式修改" class="headerlink" title="文章目录样式修改"></a>文章目录样式修改</h2><p>在目录中加入选中时的底色与字体，在自定义CSS文件中加入以下代码：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-class">.tocbot-active-link</span> &#123;<br>    <span class="hljs-attribute">font-weight</span>: bold;<br>    <span class="hljs-comment">/*color var(--link-hover-color)*/</span><br>    <span class="hljs-attribute">color</span>: <span class="hljs-number">#158956</span>;<br>    <span class="hljs-attribute">background</span>: <span class="hljs-number">#00c4b520</span>;<br>    <span class="hljs-attribute">border-radius</span>: <span class="hljs-number">5px</span>;<br>&#125;<br><span class="hljs-selector-class">.toc-list-item</span><span class="hljs-selector-pseudo">::before</span> &#123;<br>    <span class="hljs-attribute">background</span>: <span class="hljs-number">#158956</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure><h2 id="减少文字页边距"><a href="#减少文字页边距" class="headerlink" title="减少文字页边距"></a>减少文字页边距</h2><p>fluid主题本来中间文字框已经不大了，再加上边距，实际文字占宽度一半不到。这或许美观，但不实用。可以将页边距从10%改为5%，视觉上内容会充实许多。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs css">// hexo-theme-fluid/source/css/_pages/_post/post<span class="hljs-selector-class">.styl</span><br><span class="hljs-selector-class">.post-content</span>, post-custom<br>  <span class="hljs-attribute">box-sizing</span> <span class="hljs-attribute">border</span>-box<br>  <span class="hljs-attribute">padding-left</span> <span class="hljs-number">5%</span><br>  <span class="hljs-attribute">padding-right</span> <span class="hljs-number">5%</span><br></code></pre></td></tr></table></figure><h2 id="右下角看板娘"><a href="#右下角看板娘" class="headerlink" title="右下角看板娘"></a>右下角看板娘</h2><p>使用了<code>hexo-helper-live2d</code>中的模型</p><p>首先下载插件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">npm install --save hexo-helper-live2d <br></code></pre></td></tr></table></figure><p>其次在站点配置文件<code>blog/_config.yml</code>中加入以下代码</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">live2d:</span><br>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">scriptFrom:</span> <span class="hljs-string">local</span><br>    <span class="hljs-attr">model:</span> <br>        <span class="hljs-attr">use:</span> <span class="hljs-string">live2d-widget-model-hijiki</span> <span class="hljs-comment">#模型选择</span><br>    <span class="hljs-attr">display:</span> <br>        <span class="hljs-attr">position:</span> <span class="hljs-string">left</span> <span class="hljs-comment">#模型位置</span><br>        <span class="hljs-attr">width:</span> <span class="hljs-number">225</span>      <span class="hljs-comment">#模型宽度</span><br>        <span class="hljs-attr">height:</span> <span class="hljs-number">350</span>      <span class="hljs-comment">#模型高度</span><br>    <span class="hljs-attr">mobile:</span> <br>        <span class="hljs-attr">show:</span> <span class="hljs-literal">false</span>      <span class="hljs-comment">#是否在手机端显示</span><br></code></pre></td></tr></table></figure><p>再次渲染即可看到看板娘。更多模型参见：<a href="https://github.com/xiazeyu/live2d-widget-models">xiazeyu&#x2F;live2d-widget-models: The model library for live2d-widget.js (github.com)</a></p><h2 id="标签样式修改"><a href="#标签样式修改" class="headerlink" title="标签样式修改"></a>标签样式修改</h2><p>找到fluid文件夹下的tag的CSS目录，修改内容</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs css">// hexo-theme-fluid/source/css/_tag/tags<span class="hljs-selector-class">.styl</span><br><span class="hljs-selector-class">.tagcloud</span><br>  <span class="hljs-attribute">padding</span> .<span class="hljs-number">5rem</span> <span class="hljs-number">2%</span><br> <br>  <span class="hljs-selector-tag">a</span><span class="hljs-selector-pseudo">:before</span><br>    <span class="hljs-attribute">content</span> &quot;🔖&quot;<br>  <span class="hljs-selector-tag">a</span><br>    <span class="hljs-attribute">display</span> inline-block<br>    <span class="hljs-attribute">vertical-align</span> middle<br>    <span class="hljs-attribute">padding</span> .<span class="hljs-number">2rem</span><br>    <span class="hljs-attribute">box-shadow</span> <span class="hljs-number">0px</span> <span class="hljs-number">15px</span> <span class="hljs-number">42px</span> -<span class="hljs-number">3px</span> rgba(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0.1</span>)<br>    <span class="hljs-attribute">border-radius</span> <span class="hljs-number">5px</span><br>  <br>    &amp;<span class="hljs-selector-pseudo">:hover</span><br>      <span class="hljs-attribute">color</span> <span class="hljs-selector-tag">var</span>(<span class="hljs-attr">--link-hover-color</span>) !important<br><br></code></pre></td></tr></table></figure><p>同时修改主题配置文件中tag的颜色和文字配置，这个因人而异</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">//</span> <span class="hljs-string">hexo-theme-fluid/_config.yml</span><br><span class="hljs-comment">#---------------------------</span><br><span class="hljs-comment"># 标签页</span><br><span class="hljs-comment"># Tag Page</span><br><span class="hljs-comment">#---------------------------</span><br><span class="hljs-attr">tag:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">banner_img:</span> <span class="hljs-string">/img/wallpaper1.jpg</span><br>  <span class="hljs-attr">banner_img_height:</span> <span class="hljs-number">80</span><br>  <span class="hljs-attr">banner_mask_alpha:</span> <span class="hljs-number">0.3</span><br>  <span class="hljs-attr">tagcloud:</span><br>    <span class="hljs-attr">min_font:</span> <span class="hljs-number">22</span><br>    <span class="hljs-attr">max_font:</span> <span class="hljs-number">24</span><br>    <span class="hljs-attr">unit:</span> <span class="hljs-string">px</span><br>    <span class="hljs-attr">start_color:</span> <span class="hljs-string">&quot;#923daa&quot;</span><br>    <span class="hljs-attr">end_color:</span> <span class="hljs-string">&quot;#00c4e9&quot;</span><br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fluid </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型独立的学习方式笔记</title>
      <link href="/post/d8c30796.html"/>
      <url>/post/d8c30796.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="模型独立的学习方式"><a href="#模型独立的学习方式" class="headerlink" title="模型独立的学习方式"></a>模型独立的学习方式</h1><h2 id="不同的学习方式"><a href="#不同的学习方式" class="headerlink" title="不同的学习方式"></a>不同的学习方式</h2><p>这些学习方式不限于具体的模型，但一种学习方式会对具有某种特质的模型更加青睐。</p><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>集成学习的目的在于通过某种策略将多个模型集成起来，通过群体决策来提高决策准确率。</p><h3 id="直接平均（投票）"><a href="#直接平均（投票）" class="headerlink" title="直接平均（投票）"></a>直接平均（投票）</h3><p><strong>定理10-1：</strong>对于$M$个不同的模型$f_1(\pmb x),…,f_M(\pmb x)$，其平均期望错误为$\overline {\mathcal R}(f)$，基于简单投票机制的集成模型$f^{(c)}(\pmb x)&#x3D;{1 \over M}\sum_{m&#x3D;1}^Mf_m(\pmb x)$的期望错误理论在${1\over M}\overline {\mathcal R}(f)$和$\overline {R}(f)$之间。</p><p><strong>证明：</strong><br>$$<br>\begin{align}<br>\mathcal R(f) &amp; &#x3D; \mathbb E_x[({1\over M}\sum_{m&#x3D;1}^ Mf_m(\pmb x)-h(\pmb x))^2]  \\<br>&amp; &#x3D; \mathbb E_x[{1\over M^2}\sum_{m&#x3D;1}^ M(f_m(\pmb x)-h(\pmb x))^2] \\<br>&amp; &#x3D; {1\over M^2}\mathbb E_x[\sum_{m&#x3D;1}^ M\epsilon_m(\pmb x)^2]\\<br>&amp; &#x3D; {1\over M^2}\sum_{m&#x3D;1}^ M\sum_{n&#x3D;1}^ N\mathbb E_x[\epsilon_m(\pmb x)\epsilon_n(\pmb x)]\\<br>\end{align}<br>$$<br>其中$\mathbb E_x[\epsilon_m(\pmb x)\epsilon_n(\pmb x)]$是模型错误的相关性，若模型不相关，则有$\forall m\ne n，\mathbb E_x[\epsilon_m(\pmb x)\epsilon_n(\pmb x)]&#x3D;0$；若模型完全相同相关，则有$\forall m\ne n,\epsilon_m(\pmb x)&#x3D;\epsilon_n(\pmb x)$，那么则有：$\overline {\mathcal R}(f)\ge \mathcal R(f)\ge{1\over M}\overline {\mathcal R}(f)$</p><p>证毕。</p><p>根据证明过程可知，基分类器应当差异越大越好。</p><h3 id="Bagging类"><a href="#Bagging类" class="headerlink" title="Bagging类"></a>Bagging类</h3><p>通过<strong>不同模型的训练数据集的独立性</strong>来提高不同模型之间的独立性。实现方式是在原数据集上进行有放回的随机采样，得到$M$个比较小的训练集并训练$M$个模型，然后通过简单投票进行集成。</p><h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>在Bagging基础上引入随机特征，进一步提高每个基模型间的独立性。</p><h3 id="Boosting类"><a href="#Boosting类" class="headerlink" title="Boosting类"></a>Boosting类</h3><p>按照一定的顺序先后训练不同的模型，前序模型指导后续模型的训练，每次训练时根据前序模型的结果调整权重。</p><p><img src="/post/d8c30796/image-20230116233154025.png" alt="image-20230116233154025"></p><h4 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h4><p>Boosting类集成模型目标是学习一个加性模型：<br>$$<br>F(\pmb x)&#x3D;\sum_{m&#x3D;1}^M \alpha_mf_m(\pmb x)<br>$$<br>其中$f_m(\pmb x)$是基分类器，$\alpha_m$是权重，Adaboost算法的过程是这样的：假设已经训练了第$m$个基分类器，在训练第$m+1$个基分类器的时候提高分错样本的权重，使得第$m+1$个基分类器更关注分错的样本。以二分类为例：</p><p><img src="/post/d8c30796/image-20230117000508078.png" alt="image-20230117000508078"></p><p>其中，加权错误率$\epsilon_m$的计算在此不再叙述，自行查阅相关文献；权重更新时是通过损失最小来进行更新。</p><h2 id="自训练和协同训练"><a href="#自训练和协同训练" class="headerlink" title="自训练和协同训练"></a>自训练和协同训练</h2><p>自训练和协同训练一般用在半监督学习中，使用少量标注数据和大量无标注数据来训练模型。</p><h3 id="自训练"><a href="#自训练" class="headerlink" title="自训练"></a>自训练</h3><p>自训练是首先使用标注数据来训练一个模型，并使用这个模型来预测无标注样本的标签，把预测置信度比较高的样本及其预测的<strong>伪标签</strong>加入训练集，然后重新训练新的模型，并不断重复这个过程。</p><p><img src="/post/d8c30796/image-20230117005733623.png" alt="image-20230117005733623"></p><h3 id="协同训练"><a href="#协同训练" class="headerlink" title="协同训练"></a>协同训练</h3><p>协同训练是在自训练基础上的改进，通过两个<strong>基于不同视角</strong>的分类器来互相促进。比如互联网上的每个网页都由两种视角组成：文字内容（text）和指向其他网页的链接，如果要确定一个网页的类别，既可以根据文字内容来判断，也可根据网页之间的链接关系来判断。</p><p><img src="/post/d8c30796/image-20230117010748950.png" alt="image-20230117010748950"></p><p>与自训练不同的是，协同训练是两个互补模型一起标注无标签数据并加入。</p><p><img src="/post/d8c30796/image-20230117010827497.png" alt="image-20230117010827497"></p><p>由于不同视角的条件独立性，在不同视角上训练出来的模型就相当于从不同视角来理解问题，具有一定的互补性．协同训练就是利用这种互补性来进行自训练的一种方法。</p><h2 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h2><p>多任务学习（Multi-task Learning）是指同时学习多个相关任务，让这些任务在学习过程中共享知识，利用多个任务之间的相关性来改进模型在每个任务上的性能和泛化能力．多任务学习可以看作一种归纳迁移学习（Inductive Transfer Learning），即通过利用包含在相关任务中的信息作为归纳偏置（Inductive Bias）来提高泛化能力。</p><p>（归纳偏置：关于<a href="https://so.csdn.net/so/search?q=%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0&spm=1001.2101.3001.7020">目标函数</a>的必要假设）</p><p>多任务学习有以下几种的学习模式：</p><ul><li>硬共享模式：底层共享，高层特用；</li><li>软共享模式：任务间共享信息，如隐状态、注意力等；</li><li>层次共享模式：底层提取低级局部特征，高层提取抽象语义特征；</li><li>共享-私有模式：共享模块与特定（私有）模块分开提取特征；</li></ul><p><img src="/post/d8c30796/image-20230118004610008.png" alt="image-20230118004610008"></p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>多任务学习的损失函数是所有任务的损失函数的线性加权：<br>$$<br>\mathcal L(\theta)&#x3D;\sum_{m&#x3D;1}^M\sum_{n&#x3D;1}^N\eta_m\mathcal L_m(f_m(x^{(m,n)};\theta),y^{(m,n)})<br>$$<br>其学习流程分为联合训练和单任务精调两个阶段，分别对应于泛化任务和特化任务。</p><h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>迁移学习的目标在于从已有的大规模的与目标任务分布不同的训练数据中学习到普遍使用的可泛化知识，将其迁移到目标任务上。</p><p>假设一个机器学习任务$\mathcal T$的样本空间为$\mathcal X\times \mathcal Y$，其中$\mathcal X$是输入空间，$\mathcal Y$是输出空间，概率密度函数为$p(\pmb x,y)$，一个样本空间及其分布可以称作一个领域$\mathcal D&#x3D;(\mathcal X,\mathcal Y,p(\pmb x,y))$；一个机器学习任务的定义是在一个领域$\mathcal D$上的条件概率$p(\pmb x|y)$的建模问题。</p><p>迁移学习的本质是不同领域的知识迁移过程，利用源领域$\mathcal D_S$中学到的知识来帮助目标领域$\mathcal D_T$上的学习任务，源领域的训练样本数量一般远大于目标领域。</p><p><img src="/post/d8c30796/image-20230119001031796.png" alt="image-20230119001031796"></p><p><img src="/post/d8c30796/image-20230119001058906.png" alt="image-20230119001058906"></p><p>迁移学习根据不同的迁移方式分成两种类型：</p><ul><li>归纳迁移学习：对应于归纳学习，指在源领域和任务上学习出一般的规律，然后将这个规律迁移到目标领域和任务上。（基于特征或精调）<ul><li>与多任务学习的不同：有先后次序，先源领域学习，后目标领域迁移。</li></ul></li><li>转导迁移学习：是一种从样本到样本的迁移，直接利用源领域和目标领域的样本进行迁移学习。<ul><li>转导迁移学习的一个常见子问题是领域适应，一般假设源领域和目标领域有相同的样本空间，但是数据分布不同，即$p_s(\pmb x,y)\ne p_t(\pmb x,y)$。</li><li>三种情况导致分布不同：协变量偏移（$p(\pmb x)$）&#x2F;概念偏移（$p(y|\pmb x)$）&#x2F;先验偏移（$p(y)$）</li><li>协变量偏移：一般指输入在训练集和测试集上的分布不同．这样，在训练集上学习到的模型在测试集上的表现会比较差。</li></ul></li></ul><h2 id="元学习"><a href="#元学习" class="headerlink" title="元学习"></a>元学习</h2><p>元学习指的是能够通过动态调整模型超参或动态选择模型来适应不同任务的特点，又可称为“学习的学习”，和元学习比较相关的一个机器学习问题是小样本学习，即在小样本上的快速学习能力。</p><p><img src="/post/d8c30796/image-20230119154720420.png" alt="image-20230119154720420"></p><ul><li><p>基于优化器的元学习</p><ul><li>用一个模型预测梯度下降时的梯度差值$\triangle \delta_t&#x3D;\delta_t-\delta_{t-1}$，其优化器为$g_t(·)$，则第$t$步的更新规则就可以写成$\delta_t+1&#x3D;\delta_t+g_t(\nabla\mathcal L(\delta_t);\phi)$。其目标函数为：</li><li><img src="/post/d8c30796/image-20230119011609365.png" alt="image-20230119011609365">，其中$LSTM$网络可以看作一个更为高阶的学习模型，学习的是原模型的学习方式。</li><li><img src="/post/d8c30796/image-20230119010620443.png" alt="image-20230119010620443"></li></ul></li><li><p>模型无关的元学习</p><ul><li><p>是一个简单的模型无关、任务无关的元学习算法，表示为在任务空间中的所有任务上学习一种通用的表示。MAML方法提倡采用一种折中的方式，即通过将多个任务的损失函数进行平均，而后用平均损失函数对模型参数求导，得到参数的梯度方向。</p></li><li><p>$$<br>&#x3D;\min_{\theta}\sum_{\mathcal F_m~p(\mathcal F)}\mathcal L_{\mathcal F_m}(f(\underbrace{\theta-\alpha\nabla_\theta\mathcal L_{\mathcal F_m}(f_\theta))}_{\theta’_m})<br>$$</p></li></ul></li></ul><h2 id="终身学习"><a href="#终身学习" class="headerlink" title="终身学习"></a>终身学习</h2><p>人类的学习是一直持续的，人脑可以通过记忆不断地累积学习到的知识，这些知识累积可以在不同的任务中持续进行。和归纳迁移学习一样，终身学习也是在前$m$个任务中学习，不过终身学习关注于对前$m$个任务的知识的积累过程。</p><p>在终身学习中，一个关键的问题是如何避免“灾难性遗忘”，即按照一定顺序学习多个任务时，在学习新任务的同时不忘记先前学会的历史任务。</p><h3 id="弹性权重巩固"><a href="#弹性权重巩固" class="headerlink" title="弹性权重巩固"></a>弹性权重巩固</h3><p>以两个任务的持续学习为例，假设任务$\mathcal T_A$和任务$\mathcal T_B$的数据集分别为$\mathcal D_A$和$\mathcal D_B$，从贝叶斯角度看，将模型参数$\theta$看作随机变量，给定两个任务时$\theta$的后验分布为：<br>$$<br>\log p(\theta|\mathcal D) &#x3D; \log p(\mathcal D|\theta)+\log p(\theta)-\log p(\mathcal D), 其中\mathcal D &#x3D; \mathcal D_A \cup \mathcal D_B<br>$$<br>根据独立同分布假设，上式可以写为：<br>$$<br>\begin{align}<br>\log p(\theta|\mathcal D) &amp; &#x3D; \underline{\log p(\mathcal D_A|\theta)}+\log p(\mathcal D_B|\theta)+\underline{\log p(\theta)}-\underline{\log p(\mathcal D_A)}-\log p(\mathcal D_B) \\<br>&amp; &#x3D; \log p(\mathcal D_B|\theta)+\underline{\log p(\theta|\mathcal D_A)}-\log p(\mathcal D_B)<br>\end{align}<br>$$<br>其中$\log p(\theta|\mathcal D_A)$包含所有在任务$\mathcal T_A$上学到的信息，当顺序的学习任务$\mathcal T_B$时，参数在两个任务上的后验分布和其在任务$\mathcal T_A$的后验分布有关。</p><p><img src="/post/d8c30796/image-20230119180525416.png" alt="image-20230119180525416"></p><p>由于后验分布难以建模，因此可以用精度矩阵（协方差矩阵的逆）来<strong>近似</strong>：<br>$$<br>p(\theta|\mathcal D_A)&#x3D;\mathcal N(\theta_A^*,F^{-1})<br>$$<br>其中$F$是$Fisher$信息矩阵，为了提高计算效率，$F$可以简化为对角阵。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git push一直提示Everything up-to-date</title>
      <link href="/post/a895b120.html"/>
      <url>/post/a895b120.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="git-push一直提示Everything-up-to-date"><a href="#git-push一直提示Everything-up-to-date" class="headerlink" title="git push一直提示Everything up-to-date"></a>git push一直提示Everything up-to-date</h1><p>参考自<a href="https://blog.csdn.net/zhuhongyang_/article/details/118521264"> git提交代码时出现Everything up-to-date的解决办法_zhuhongyang_的博客-CSDN博客_everything is up-to-date</a></p><p>主要目的在于将push的状态从up-to-date的错误状态中出来，这里采用的是“曲线救国”的一种方法</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">创建新分支</span><br>git branch new<br><span class="hljs-meta prompt_"># </span><span class="language-bash">查看分支是否成功创建</span><br>git branch<br><span class="hljs-meta prompt_"># </span><span class="language-bash">切换到此分支</span><br>git checkout new<br><span class="hljs-meta prompt_"># </span><span class="language-bash">代码推送到新分支</span><br>git push origin new(若不成功则试git push --set-upstream origin newbranch)<br><span class="hljs-meta prompt_"># </span><span class="language-bash">切换到原分支</span><br>git checkout master<br><span class="hljs-meta prompt_"># </span><span class="language-bash">合并新分支内容到原分支</span><br>git merge new(若有文件冲突则git diff手动处理)<br><span class="hljs-meta prompt_"># </span><span class="language-bash">将原分支提交到远程</span><br>git push origin master<br><span class="hljs-meta prompt_"># </span><span class="language-bash">删除本地新分支</span><br>git branch -D new<br><span class="hljs-meta prompt_"># </span><span class="language-bash">删除远程新分支</span><br>git push origin --delete new<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo-Fluid的Mathjax数学公式渲染错误问题</title>
      <link href="/post/f44e7011.html"/>
      <url>/post/f44e7011.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hexo-Fluid的Mathjax数学公式渲染错误问题"><a href="#Hexo-Fluid的Mathjax数学公式渲染错误问题" class="headerlink" title="Hexo-Fluid的Mathjax数学公式渲染错误问题"></a>Hexo-Fluid的Mathjax数学公式渲染错误问题</h1><p>参考自<a href="https://cloud.tencent.com/developer/article/2067298">Fluid -19- 解决 Fluid 1.9+ mathjax 公式渲染错误的问题 - 腾讯云开发者社区-腾讯云 (tencent.com)</a></p><p>Fluid 更新到 1.9+ 版本后，mathjax 渲染的数学公式在电脑端浏览器无法正常显示，但在手机端显示是没有问题的，其根本原因在于PC端网页的懒加载，当公式进行懒加载后就难以重新嵌入到网页中，因此需要将公式的懒加载去除。</p><p>具体方法是删除<code>fluid\layout\_partials\plugins\math.ejs</code>中的三行代码</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs ejs">- loader : &#123;<br>-   $&#123; lazy ? &#x27;load: \[\&#x27;ui/lazy\&#x27;\]&#x27; : &#x27;&#x27; &#125;<br>- &#125;,<br></code></pre></td></tr></table></figure><p>这样就解决了数学公式的渲染问题。</p>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mathjax </tag>
            
            <tag> Fluid </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Github Action博客自动部署搭建</title>
      <link href="/post/4b70d5e.html"/>
      <url>/post/4b70d5e.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Github-Action博客自动部署搭建"><a href="#Github-Action博客自动部署搭建" class="headerlink" title="Github Action博客自动部署搭建"></a>Github Action博客自动部署搭建</h1><p> 本文参考自<a href="https://dslwind.github.io/2021-04-20-github-action-hexo/">使用 GitHub Action 自动部署 Hexo 博客 | 水木风的个人博客 (dslwind.github.io)</a>、<a href="https://blog.csdn.net/qq_41426117/article/details/108703295">GitHub Action + Hexo实现在线写作_磊少1999的博客-CSDN博客_hexo在线编辑</a>，部分有所改动。</p><h2 id="Github-Action"><a href="#Github-Action" class="headerlink" title="Github Action"></a>Github Action</h2><p>Github Actions 可以很方便实现 CI&#x2F;CD 工作流，来帮我们完成一些工作，比如实现自动化测试、打包、部署等操作。Github Action是Github提供的一站式服务，通过设置workflow工作队列，将需要进行的工作部署到Github上自动实现。在hexo博客搭建时，创建博客、hexo clean &amp;&amp; hexo g &amp;&amp; hexo d三步骤等都十分的繁琐，有时候为了简单博文内容，但手头的设备上没有 hexo 的环境，这时候就可以借助 GitHub Actions 来实现云端部署。因此这里将这些步骤分包到Github Action自动完成；同时，之后若有进一步的动作，如gitee文件同步、更新等操作，就可以同样用Github Action来完成了。</p><p>以下是自动部署的步骤：</p><h2 id="创建仓库或分支"><a href="#创建仓库或分支" class="headerlink" title="创建仓库或分支"></a>创建仓库或分支</h2><p>完成Github Action首先需要规定一个动作，在这里可以指对源码仓库或源码分支的更新，因此需要创建一个新的仓库或者源码来存储现有的源码。即</p><ul><li>创建SourceCode仓库存储Hexo的源码文件和markdown文本文件；</li><li>继续使用原有的xxx.github.io作为站点仓库。</li></ul><h2 id="生成远程密钥"><a href="#生成远程密钥" class="headerlink" title="生成远程密钥"></a>生成远程密钥</h2><p>首先在博客blog目录下设置一个新的ssh-key用于部署。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-keygen -f github-deploy-key<br></code></pre></td></tr></table></figure><p><img src="/post/4b70d5e/image-20230116011611023.png" alt="image-20230116011611023"></p><p>当前目录将生成 <code>github-deploy-key</code> 和 <code>github-deploy-key.pub</code> 两个文件。</p><h2 id="Github在线部署"><a href="#Github在线部署" class="headerlink" title="Github在线部署"></a>Github在线部署</h2><p><strong>下面的步骤请注意设置的仓库分别是哪个</strong></p><p>1、复制<code>github-deploy-key</code>文件中的内容，在<strong>源码仓库（SourceCode）</strong>中<code>Settings-&gt;Secrets and variables-&gt;Actions-&gt;New repository secret</code>设置私钥：</p><ul><li>在Name输入框中填写<code>HEXO_DEPLOY_PRI</code></li><li>在 <code>Value</code> 输入框填写 <code>github-deploy-key</code> 文件内容。</li></ul><p><img src="/post/4b70d5e/image-20230116163126081.png" alt="image-20230116163126081"></p><p>2、复制 <code>github-deploy-key.pub</code> 文件内容，在 <strong>站点仓库<code>username.github.io</code></strong> 仓库 <code>Settings -&gt; Deploy keys -&gt; Add deploy key</code> 页面上添加。</p><ul><li>在 <code>Title</code> 输入框填写 <code>HEXO_DEPLOY_PUB</code>。</li><li>在 <code>Key</code> 输入框填写 <code>github-deploy-key.pub</code> 文件内容。</li><li>勾选 <code>Allow write access</code> 选项。</li></ul><p><img src="/post/4b70d5e/image-20230116163036531.png" alt="image-20230116163036531"></p><p>这样就完成了Github Action的密钥设置。</p><h2 id="编写Action脚本"><a href="#编写Action脚本" class="headerlink" title="编写Action脚本"></a>编写Action脚本</h2><p>Action脚本采用的是Workflow工作流，其目标在于记录工作的步骤和目的，这里参考了<a href="https://blog.csdn.net/qq_41426117/article/details/108703295">GitHub Action + Hexo实现在线写作_磊少1999的博客-CSDN博客_hexo在线编辑</a>的Action代码，这里我只有自动部署的需求，更多需求可以自行查找Action代码并替换。</p><p>在源码目录的根目录中创建<code>.github/workflows/deploy.yml</code> 文件，并将下面的代码粘进去。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs json"># workflow name<br>name<span class="hljs-punctuation">:</span> Hexo Blog CI<br><br># master branch on push<span class="hljs-punctuation">,</span> auto run<br>on<span class="hljs-punctuation">:</span> <br>  push<span class="hljs-punctuation">:</span><br>    branches<span class="hljs-punctuation">:</span><br>  #注意这里更改为自己源码的分支<br>      - master<br>      <br>jobs<span class="hljs-punctuation">:</span><br>  build<span class="hljs-punctuation">:</span> <br>    runs-on<span class="hljs-punctuation">:</span> ubuntu-latest <br>        <br>    steps<span class="hljs-punctuation">:</span><br>    # check it to your workflow can access it<br>    # from<span class="hljs-punctuation">:</span> https<span class="hljs-punctuation">:</span><span class="hljs-comment">//github.com/actions/checkout</span><br>    - name<span class="hljs-punctuation">:</span> Checkout Repository master branch<br>      uses<span class="hljs-punctuation">:</span> actions/checkout@master <br>      <br>    # from<span class="hljs-punctuation">:</span> https<span class="hljs-punctuation">:</span><span class="hljs-comment">//github.com/actions/setup-node  </span><br>    - name<span class="hljs-punctuation">:</span> Setup Node.js <span class="hljs-number">12.</span>x <br>      uses<span class="hljs-punctuation">:</span> actions/setup-node@master<br>      with<span class="hljs-punctuation">:</span><br>        node-version<span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;12.x&quot;</span><br>    <br>    - name<span class="hljs-punctuation">:</span> Setup Hexo Dependencies<br>      run<span class="hljs-punctuation">:</span> |<br>        npm install hexo-cli -g<br>        npm install<br>    <br>    - name<span class="hljs-punctuation">:</span> Setup Deploy Private Key<br>      env<span class="hljs-punctuation">:</span><br>        HEXO_DEPLOY_PRIVATE_KEY<span class="hljs-punctuation">:</span> $<span class="hljs-punctuation">&#123;</span><span class="hljs-punctuation">&#123;</span> secrets.HEXO_DEPLOY_PRIVATE_KEY <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">&#125;</span><br>      run<span class="hljs-punctuation">:</span> |<br>        mkdir -p ~/.ssh/<br>        echo <span class="hljs-string">&quot;$HEXO_DEPLOY_PRIVATE_KEY&quot;</span> &gt; ~/.ssh/id_rsa <br>        chmod <span class="hljs-number">600</span> ~/.ssh/id_rsa<br>        ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts<br>        <br>    - name<span class="hljs-punctuation">:</span> Setup Git Infomation<br>      run<span class="hljs-punctuation">:</span> | <br># 更改成自己的信息<br>        git config --global user.name &#x27;&lt;你的用户名&gt;&#x27; <br>        git config --global user.email &#x27;&lt;你登录的email&gt;&#x27;<br>    - name<span class="hljs-punctuation">:</span> Deploy Hexo <br>      run<span class="hljs-punctuation">:</span> |<br>        hexo clean<br>        hexo generate <br>        hexo deploy<br></code></pre></td></tr></table></figure><h2 id="开启Action"><a href="#开启Action" class="headerlink" title="开启Action"></a>开启Action</h2><p>开启Action的条件是监听到master的push需求，因此我们需要将源码push到源码仓库的master分支中（自定义）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">初始化git</span><br>git init<br><span class="hljs-meta prompt_"># </span><span class="language-bash">设置远程仓库</span><br>git remote add origin git@github.com:paopao0226/SourceCode.git<br><span class="hljs-meta prompt_"># </span><span class="language-bash">添加文件</span><br>git add .<br><span class="hljs-meta prompt_"># </span><span class="language-bash">提交commit</span><br>git commit -m &quot;提交&quot;<br><span class="hljs-meta prompt_"># </span><span class="language-bash">推送到仓库中</span><br>git push origin master<br></code></pre></td></tr></table></figure><p>这里可能会遇到如下问题：</p><ol><li>master无法推送：更改当前分支为master分支</li><li><code>! [rejected] master -&gt; master (fetch first)</code>：先pull后push，如果和仓库中的冲突过多可以<code>git push -f origin master</code>暴力覆盖（不推荐）</li><li>git push Everything up-to-date：参考博文：</li></ol><p>若有其他问题请补充。</p><p>推送到仓库之后，Github会自动监听master的推送，并开启Action操作。</p><p><img src="/post/4b70d5e/image-20230116165235345.png" alt="image-20230116165235345"></p><p>这里可能会出现Action的<code>Deploy Hexo</code>出错，这里可能是因为站点仓库受保护的原因，其他原因可以自行点开日志阅读理解。</p><p>完成自动部署之后，我们就会看到自己的站点成功更新。</p>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo博客配置github的免密推流</title>
      <link href="/post/da6dfdb5.html"/>
      <url>/post/da6dfdb5.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英语口语考试备稿</title>
      <link href="/post/77bfcf4c.html"/>
      <url>/post/77bfcf4c.html</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">  <script id="hbeData" type="hbeData" data-hmacdigest="9a789a58517215846582dde4d2259178dc0b6211efbbb79b43a788db5ab91f1e">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 英语 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络优化与正则化笔记</title>
      <link href="/post/cbfc2cb0.html"/>
      <url>/post/cbfc2cb0.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="网络优化"><a href="#网络优化" class="headerlink" title="网络优化"></a>网络优化</h1><h2 id="网络优化的难点"><a href="#网络优化的难点" class="headerlink" title="网络优化的难点"></a>网络优化的难点</h2><h3 id="结构差异大"><a href="#结构差异大" class="headerlink" title="结构差异大"></a>结构差异大</h3><ul><li>没有通用的优化算法</li><li>超参数多</li></ul><h3 id="非凸优化问题"><a href="#非凸优化问题" class="headerlink" title="非凸优化问题"></a>非凸优化问题</h3><ul><li><p>鞍点：梯度为0的点，在不同维度的性质不同，如下图，在不同的维度里分别是局部最高点和局部最低点</p><p><img src="/post/cbfc2cb0/image-20221115180044115.png" alt="image-20221115180044115"></p></li><li><p>平坦最小值：邻域内的值都接近于局部最小值，所有点对应的训练损失都比较接近。大部分的局部最小解都是平坦最小值，而局部最小解对应的训练损失都可能非常接近于全局最小解对应的训练损失。</p><p><img src="/post/cbfc2cb0/image-20221115180106978.png" alt="image-20221115180106978"></p></li></ul><h2 id="网络优化的改善方法"><a href="#网络优化的改善方法" class="headerlink" title="网络优化的改善方法"></a>网络优化的改善方法</h2><ul><li><p>更有效的优化算法来提高优化方法的效率和稳定性</p><ul><li>动态学习率调整</li><li>梯度估计修正</li></ul></li><li><p>更好的参数初始化方法、数据预处理方法来提高优化效率</p></li><li><p>修改网络结构来得到更好的优化地形</p><p>​<img src="/post/cbfc2cb0/image-20221115181154238.png" alt="image-20221115181154238"></p><ul><li>ReLU激活函数</li><li>残差连接</li><li>逐层归一化</li></ul></li><li><p>使用更好的超参数优化方法</p></li></ul><h1 id="优化算法本身的改进"><a href="#优化算法本身的改进" class="headerlink" title="优化算法本身的改进"></a>优化算法本身的改进</h1><h2 id="小批量随机梯度下降（MiniBatch）"><a href="#小批量随机梯度下降（MiniBatch）" class="headerlink" title="小批量随机梯度下降（MiniBatch）"></a>小批量随机梯度下降（MiniBatch）</h2><p>选取$K$个训练样本$\lbrace x^ { (k)},y^ { (k) } \rbrace_{ k&#x3D;1} ^ K$，计算偏导数<br>$$<br>g_t(\theta) &#x3D; {1 \over K} \sum_{(\pmb x,\pmb y) \in \mathcal S_t} { {\partial \mathcal L(\pmb y,f(\pmb x;\theta))} \over \partial \theta}<br>$$<br>定义梯度为前一个阶段参数$\theta_{t-1}$的函数<br>$$<br>g_t &#x3D; g_t(\theta_{t-1})<br>$$<br>更新参数为梯度的反方向，其中$\alpha&gt;0$为网络的学习率<br>$$<br>\theta_t &#x3D; \theta_{t-1}-\alpha g_t<br>$$<br>影响Minibatch的因素有很多，但关键在于批量大小、梯度和学习率三个参数因素。</p><h2 id="批量大小"><a href="#批量大小" class="headerlink" title="批量大小"></a>批量大小</h2><p>批量大小不影响随机梯度的期望，但会影响随机梯度的<strong>方向</strong>。批量越大，随机梯度的方差越小，引入的噪声相对越少，训练也会越稳定，就可以采用更高的学习率来提高学习效率；反之若批量越小，则需要设置小学习率防止不收敛。</p><p>按batch更新一次为单位，批量越大，学习率越高，其效果和效率也越好；按整个训练集迭代一遍为单位，批量越小，学习率越低，走的步次越多，其学习的准确度越高。<br>$$<br>1回合(Epoch) &#x3D; ({训练样本的数量N\over 批量大小K}) \times 迭代(Iteration)<br>$$<br><img src="/post/cbfc2cb0/image-20221115232605096.png" alt="image-20221115232605096"></p><h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><p><img src="/post/cbfc2cb0/image-20221115232849962.png" alt="image-20221115232849962"></p><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><p>越到学习的中后期，其学习的步长理应越小，以接近最优点，防止过拟合情况出现，因此学习率衰减技巧十分重要。</p><p><b>逆时衰减：</b>按照迭代次数衰减<br>$$<br>\alpha_t &#x3D; \alpha_0 {1 \over {1+\beta \times t}}<br>$$<br><b>指数衰减：</b>按照衰减系数对于迭代次数的指数衰减<br>$$<br>\alpha_t &#x3D; \alpha_0 \beta^t ,\ \ \beta&lt;1<br>$$<br><b>自然指数衰减：</b>$\alpha_t &#x3D; \alpha_0·exp(-\beta \times t)$</p><p><b>余弦衰减：</b>采用余弦函数中(0,1)段的递减性衰减<br>$$<br>\alpha_t &#x3D; {1\over 2}\alpha_0(1+cos({t\pi \over T}))<br>$$<br>几个方法的比较：</p><p><img src="/post/cbfc2cb0/image-20221116001027995.png" alt="image-20221116001027995"></p><h3 id="周期学习率"><a href="#周期学习率" class="headerlink" title="周期学习率"></a>周期学习率</h3><p>周期学习率旨在体现整体衰减但部分跳跃的学习率，可以<strong>帮助跳出局部最优，收敛于更为平坦的局部解。</strong></p><p><b>三角循环学习率：</b>假设每个循环周期的长度相等都为$2\triangle𝑇$，其中前$\triangle T$步为学习率线性增大阶段，后$\triangle T$步为学习率线性缩小阶段。其中在第t次迭代时有<br>$$<br>\begin{align}<br>m &amp; &#x3D; \lfloor 1+{t \over 2\triangle T} \rfloor \\<br>b &amp; &#x3D; |{t \over \triangle T} - 2m+1| \\<br>\alpha_t &amp; &#x3D; \alpha_{min}^m +(\alpha_{max}^m - \alpha_{min}^m)(max(0,1-b))<br>\end{align}<br>$$<br><b>带热重启的随机梯度下降：</b>假设在梯度下降过程中重启$M$次，第$𝑚$次重启在上次重启开始第个回合后进行，称为$T_m$重启周期．在第$m$次重启之前，采用余弦衰减来降低学习率<br>$$<br>\alpha_t &#x3D; \alpha_{min}^m + {1 \over 2}(\alpha_{max}^m - \alpha_{min}^m)(1+cos({T_{cur} \over T_m} \pi))<br>$$<br>两种周期学习率调整方法的示意图如图</p><p><img src="/post/cbfc2cb0/image-20221116001332954.png" alt="image-20221116001332954"></p><h3 id="自适应学习率"><a href="#自适应学习率" class="headerlink" title="自适应学习率"></a>自适应学习率</h3><h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p>借鉴了$l_2$正则化的思想，在更新梯度时结合之前梯度平方和的影响<br>$$<br>\triangle \theta_t &#x3D; - {\alpha \over {\sqrt{G_t+\epsilon}}} \odot \pmb g_t，其中G_t &#x3D; \sum_{\tau&#x3D;1} ^t{\pmb g_t \odot \pmb g_t}<br>$$</p><h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><p>RMSprop可以在有些情况下避免AdaGrad算法中学习率不断单调下降以至于过早衰减的缺点。<br>$$<br>\triangle \theta_t &#x3D; - {\alpha \over {\sqrt{G_t+\epsilon}}} \odot \pmb g_t，其中G_t &#x3D; \beta G_{t-1}+(1-\beta){\pmb g_t \odot \pmb g_t}&#x3D; (1-\beta) \sum_{\tau&#x3D;1}^t \beta^{t-\tau}\pmb g_\tau \odot g_\tau<br>$$<br>从上式可以看出，$RMSProp$算法和$AdaGrad$算法的区别在于$G_t$的计算由累积方式变成了指数衰减移动平均．在迭代过程中，每个参数的学习率并不是呈衰减趋势，既可以变小也可以变大。</p><h4 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h4><p>加入方向梯度参数进行考量，当某个方向的梯度一直很大时，代表这个方向还没有训练完全，应该多学一下。<br>$$<br>\triangle \theta_t &#x3D; - { {\sqrt{\triangle X_{t-1} ^2 + \epsilon} } \over {\sqrt{G_t+\epsilon} } } \odot \pmb g_t，其中G_t &#x3D; \sum_{\tau&#x3D;1} ^t{\pmb g_t \odot \pmb g_t},\triangle X_{t-1}^2 &#x3D; \beta_1\triangle X_{t-2}^2+(1-\beta_1)\triangle \theta_{t-1} \odot \triangle \theta_{t-1}<br>$$<br>$AdaDelta$算法还引入了每次参数更新差值$\triangle \theta$的平方的指数衰减权移动平均，一定程度上减小了学习率$\alpha$的波动。</p><h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><h3 id="梯度方向优化"><a href="#梯度方向优化" class="headerlink" title="梯度方向优化"></a>梯度方向优化</h3><h4 id="Momentum（动量法）"><a href="#Momentum（动量法）" class="headerlink" title="Momentum（动量法）"></a>Momentum（动量法）</h4><p>用之前累计动量来替代真正的梯度进行更新，每次迭代的梯度可以看作是加速度。在第$t$次迭代，计算负梯度的”加权移动平均“作为参数更新的方向。<br>$$<br>\triangle \theta_t &#x3D; \rho \triangle \theta_{t-1} -\alpha \pmb g_t &#x3D; -\alpha\sum_{\tau&#x3D;1}^t \rho^{t-\tau}\pmb g_\tau<br>$$<br>其中$\rho$是动量因子，整体是动量因子和梯度的乘积的加权移动平均，每个$\rho^{t-i}\pmb g_t$是单位的动量。这样每个参数的实际更新差值取决于最近一段时间内梯度的加权平均值。当某个参数在最近一段时间内的梯度方向不一致时，其真实的参数更新幅度变小；相反，当在最近一段时间内的梯度方向都一致时，其真实的参数更新幅度变大，起到加速作用。从而起到抵消相反方向，加速相同方向的效果。</p><p><img src="/post/cbfc2cb0/image-20221118163828905.png" alt="image-20221118163828905"></p><p>当前梯度与之前梯度的加权移动平均可以近似看作二阶梯度，体现梯度的变化方向。</p><h4 id="Nesterov加速梯度"><a href="#Nesterov加速梯度" class="headerlink" title="Nesterov加速梯度"></a>Nesterov加速梯度</h4><p>对于动量法的公式，可以将其看作两步，第一步是加上之前$\triangle \theta_{t-1}$的参数，得到中间参数$\triangle \hat \theta$；第二步是加上梯度反方向，得到新的参数$\triangle \theta_t$。其中，第二步的更新本应从中间参数的位置进行更新，这样才能用到第一步的中间更新结果，因此就可以得到$Nesterov$方法的更新公式：<br>$$<br>\triangle \theta_t &#x3D; \rho \triangle \theta_{t-1} -\alpha \pmb g_t(\theta_{t-1}+\rho \triangle \theta_{t-1})<br>$$<br>其中括号中的参数加和就体现了参数本身的中间值更新。</p><p><img src="/post/cbfc2cb0/image-20221118164634719.png" alt="image-20221118164634719"></p><h4 id="Adam算法：梯度方向优化-自适应学习率"><a href="#Adam算法：梯度方向优化-自适应学习率" class="headerlink" title="Adam算法：梯度方向优化+自适应学习率"></a>Adam算法：梯度方向优化+自适应学习率</h4><p>计算梯度平方$g_t^2$的指数加权平均（RMSprop）和梯度$g_t$的指数加权平均（动量法）<br>$$<br>\begin{align}<br>M_t &amp; &#x3D; \beta_1M_{t-1}+(1-\beta_1)\pmb g_t \\<br>G_t &amp; &#x3D; \beta_2G_{t-1}+(1-\beta_2)\pmb g_t \odot \pmb g_t<br>\end{align}<br>$$<br>取其中一个加权移动平均，并看平均式子的第一项：$M_1 &#x3D; \beta_1M_0+(1-\beta_1)g_1$，若前面的$\beta_1M_0&#x3D;0$，则有$M_1&#x3D;(1-\beta_1)g_1$，而梯度更新的单位不能变，即本身应该$M_1&#x3D;g_1$，因此需要修正掉这个系数的偏差，扩展到$M_t$和$G_t$则有<br>$$<br>\hat M_t &#x3D; {M_t \over {1-\beta_1^t}},\hat G_t &#x3D; {G_t \over {1-\beta_2^t}}<br>$$<br>最后，按照自适应学习率的方法更新参数即可<br>$$<br>\triangle \theta_t &#x3D; - {\alpha \over \sqrt{\hat G_t+\epsilon}}\hat M_t<br>$$</p><h3 id="梯度截断"><a href="#梯度截断" class="headerlink" title="梯度截断"></a>梯度截断</h3><p>梯度截断通过将梯度的模限制在一个区间，从而解决了<strong>梯度爆炸</strong>的问题，防止梯度过大。梯度截断的的方法相对会比较暴力，有按值截断和按模截断两种方法。</p><p><strong>按值截断：</strong>$\pmb g_t &#x3D; max(min(\pmb g_t,b),a)$ ——&gt;限制在$[a,b]$之间</p><p><strong>按模截断：</strong>$\pmb g_t &#x3D; { b \over ||\pmb g_t||}\pmb g_t$</p><h2 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h2><p><b>增大批量大小：</b>批量越大，相对需要的学习率就越大，而单纯增大批量，效果可以视为变相降低学习率</p><p><font color="red"><b>学习率预热：</b></font>在一开始梯度较乱的时候使用小学习率提高稳定性，到一定epoch后开始正式学习，学习率预热目前已成为业界公认常规使用的一种方法。</p><p><img src="/post/cbfc2cb0/image-20221116003230617.png" alt="image-20221116003230617"></p><h2 id="优化算法小结"><a href="#优化算法小结" class="headerlink" title="优化算法小结"></a>优化算法小结</h2><h3 id="统一公式"><a href="#统一公式" class="headerlink" title="统一公式"></a>统一公式</h3><p>$$<br>\triangle \theta_t &#x3D; - {\alpha_t \over {\sqrt{G_t+\epsilon} } }M_t,\ 其中G_t &#x3D; \phi(\pmb g_1.\pmb g_2,…,\pmb g_n)，\ M_t &#x3D; \psi(\pmb g_1.\pmb g_2,…,\pmb g_n)<br>$$</p><p><img src="/post/cbfc2cb0/image-20221120005619057.png" alt="image-20221120005619057"></p><h1 id="参数和数据"><a href="#参数和数据" class="headerlink" title="参数和数据"></a>参数和数据</h1><h2 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h2><p>神经网络的参数初始化不可以像一般规模的参数一样全部初始化为0，这样会出现所有神经元的输出结果一样，即进入了<strong>对称态</strong>，这样我们的网络就不能学到更多的特征了，因此需要规避这种可能性。</p><p><img src="/post/cbfc2cb0/1199293-20220321215442891-1712051133.png" alt="一个简单的神经网络"></p><p>初始化的方法有预训练初始化（Backbone）、随机初始化和固定值初始化等方法。</p><h3 id="预训练初始化"><a href="#预训练初始化" class="headerlink" title="预训练初始化"></a>预训练初始化</h3><p>在训练模型时，我们常常需要利用预训练的baseline模型对所设计网络的backbone或部分layer进行初始化，给网络训练提供一个较好的起点，同时减少训练的时间成本。</p><h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>随机初始化是通过采用一定的随机化方法，将参数进行随机赋值来完成初始化。</p><p><strong>高斯分布初始化：</strong>参数从一个固定均值和固定方差的高斯分布进行随机初始化（如$X \sim \mathcal N(0,0.01^2)$</p><p><strong>均匀分布初始化：</strong>参数可以在区间$[-r,r]$内采用均匀分布进行初始化。</p><h3 id="基于方差缩放的参数初始化"><a href="#基于方差缩放的参数初始化" class="headerlink" title="基于方差缩放的参数初始化"></a>基于方差缩放的参数初始化</h3><p>在一个神经网络中，第$l$层的神经元要接收第$l-1$层的输入，即有<br>$$<br>a^{(l)} &#x3D; f(\sum_{i&#x3D;1}^{M_{l-1}} w_i^{(l)}a_i^{(l-1)})<br>$$<br>其中$f(·)$是激活函数，不妨在这里设为简单的恒等函数$f(x)&#x3D;x$。假设$w_i^{(l)}$和$a_i^{(l-1)}$的均值均为0且<strong>相互独立</strong>，则有<br>$$<br>\mathbb E[a^{(l)}]&#x3D;\mathbb E[\sum_{i&#x3D;1}^{M_{l-1}} w_i^{(l)}a_i^{(l-1)}]&#x3D;\sum_{i&#x3D;1}^{M_{l-1}}\mathbb E[w_i^{(l)}]\mathbb E[a_i^{(l-1)}] &#x3D; 0<br>$$<br>而$a^{(l)}$的方差为：<br>$$<br>\begin{align}<br>var(a^{(l)})&amp;&#x3D;var(\sum_{i&#x3D;1}^{M_{l-1}} w_i^{(l)}a_i^{(l-1)})\\<br>&amp;&#x3D;\sum_{i&#x3D;1}^{M_{l-1}}\ var(w_i^{(l)})var(a_i^{(l-1)}) \\<br>&amp;&#x3D;M_{l-1}\ var(w_i^{(l)})var(a_i^{(l-1)})<br>\end{align}<br>$$<br>也就是说通过一次神经元计算，方差从$l-1$层到$l$层变化了$M_{l-1}\ var(w_i^{(l)})$倍，为了使得在经过多层网络后，信号不被过分放大或过分减弱，我们尽可能保持每个神经元的输入和输出的方差一致，则有<br>$$<br>M_{l-1}\ var(w_i^{(l)})&#x3D;1,\ \ \<br>var(w_i^{(l)})&#x3D;{1\over M_{l-1}}<br>$$<br>同理，对于反向传播的结果也需要保证方差一致，需要将$w_i^{(l)}$保持方差为$var(w_i^{(l)})&#x3D;{1\over M_{l}}$</p><p>因为，取两个式子的折中结果为<br>$$<br>var(w_i^{(l)})&#x3D;{2\over {M_{l-1} + M_l} }<br>$$<br>这个就是方差缩放初始化的基本思想，通过将参数的方差限制到前后一致的程度，从而减少因参数变化导致的模型动荡的问题。</p><p>假设初始化方法服从均匀分布，根据方差计算公式，可得服从$[a,b]$的均匀分布的期望为$\mathbb E[X]&#x3D;{1\over 2}(a+b)$，所以有<br>$$<br>\begin{align}<br>D[X]&#x3D;E[X^2]-[E[X]]^2 &amp; &#x3D; \int_a^b x^2f(x)dx - ({a+b\over2})^2 \\<br>&amp; &#x3D; \int_a^b x^2{1\over {b-a} } dx- ({a+b\over2})^2 \\<br>&amp; &#x3D; {1\over {b-a} }·{1\over 3}x^3 \vert <em>a^b  - ({a+b\over2})^2 \\<br>&amp; &#x3D; {(b-a)^2 \over 12}<br>\end{align}<br>$$<br>当服从$[-r,r]$的均匀分布时，代入到方差式子中，令$var(x)&#x3D;\sigma^2$，可得$r&#x3D;\sqrt{3\sigma^2}$，代入上面的方差结果可以得到<br>$$<br>r &#x3D; \sqrt{6\over{M</em>{l-1}+M_l}}<br>$$<br>这个就是$Xavier$初始化的基本思想，将这个思想扩展可以得到其他变种的初始化方法。</p><p><img src="/post/cbfc2cb0/image-20221120154335050.png" alt="image-20221120154335050"></p><h3 id="正交初始化"><a href="#正交初始化" class="headerlink" title="正交初始化"></a>正交初始化</h3><p>一个$M$层的等宽线性网络定义如下：<br>$$<br>\pmb y &#x3D; \pmb W^{(L)}\pmb W^{(L-1)}…\pmb W^{(1)}\pmb x<br>$$<br>网络的反向传播公式为$\delta^{(l)}&#x3D;\pmb (W^{(l)})^\top \delta^{(l-1)}$为了避免梯度消失或梯度爆炸问题，希望误差项应该是相等的，即：<br>$$<br>||\delta^{(l-1)}||^2 &#x3D; ||\delta^{(l)}||^2 &#x3D; ||(\pmb W^{(l)})^\top\delta^{(l-1)}||^2<br>$$<br>因此，我们希望反向传播的参数相乘结果保持为单位阵，即$\pmb W^{(l)} ·(\pmb W^{(l)})^\top &#x3D; \mathbb I$，这种性质我们称之为<strong>范数保持性</strong></p><p>为了能够保持这一性质，可以直接采用正交初始化的方法，将参数矩阵设置为正交矩阵，正交矩阵可以满足相乘为单位阵的特点。正交初始化的方法可以按照下面步骤来</p><ol><li>用均值为0，方差为1的高斯分布初始化得到一个矩阵</li><li>将这个矩阵通过奇异值分解得到两个正交矩阵，并使用其中一个作为权重矩阵。</li></ol><h2 id="数据归一化"><a href="#数据归一化" class="headerlink" title="数据归一化"></a>数据归一化</h2><p>不同机器学习模型对数据特征尺度的敏感程度不一样。如果一个机器学习算法在缩放全部或部分特征后不影响它的学习和预测，我们就称该算法具有<strong>尺度不变性（Scale Invariance）</strong>。对于尺度敏感的模型，必须先对样本进行预处理，将各个维度的特征转换到相同的取值区间，并且消除不同特征之间的相关性，才能获得比较理想的结果．</p><p>数据归一化操作通过使数据特征范围限制到特定范围内，从而对训练起到积极作用，具体来说，数据归一化可以使梯度更新方向与最优的指向方向一致，加速模型的训练速度。</p><p><img src="/post/cbfc2cb0/image-20221120160927380.png" alt="image-20221120160927380"></p><h3 id="简单归一化"><a href="#简单归一化" class="headerlink" title="简单归一化"></a>简单归一化</h3><p>简单归一化直接对数据本身做归一化处理，其包括以下方法：</p><p><strong>最小最大值归一化：</strong>通过缩放将每一个特征的取值范围归一到$[0, 1]$或$[−1, 1]$之间，即：<br>$$<br>\hat x^{(n)} &#x3D; { {x^{(n)}-min_nx^{(n)} } \over {max_nx^{(n)}-min_nx^{(n)} } }<br>$$<br><strong>Z值标准化：</strong>对每一维的特征进行调整，使之均值为0，方差为1，首先计算特征的均值和方差：<br>$$<br>\begin{align}<br>\mu&#x3D;{1\over N}\sum_{n&#x3D;1}^Nx^{(n)},\ \ \sigma^2{1\over N}\sum_{n&#x3D;1}^N(x^{(n)}-\mu)^2<br>\end{align}<br>$$<br>然后将特征减去均值，除以标准差，得到均值为0，方差为1的分布。<br>$$<br>\hat x^{(n)} &#x3D; { {x^{(n)}-\mu} \over \sigma }<br>$$<br><strong>白化（PCA降维）：</strong>输入数据经过白化处理后，特征之间相关性较低，并且所有特征具有相同的方差，从而降低输入数据的冗余性，白化的一个重要实现方式是主成分分析（PCA）</p><p>几种方法的比较如下：</p><p><img src="/post/cbfc2cb0/image-20221120162957808.png" alt="image-20221120162957808"></p><h2 id="逐层归一化"><a href="#逐层归一化" class="headerlink" title="逐层归一化"></a>逐层归一化</h2><p>每层或每隔几层做一次层归一化，以达到层与层之间<strong>更好的尺度不变性</strong>，减少内部协变量偏移（内部协变量偏移：每层活性值的分布都有所变化，随着层的增多其变化导致的偏移会越来越明显）；可以达到<strong>平滑地形</strong>的效果，优化地形使梯度更容易收敛。</p><p>逐层归一化的方法有批量归一化、层归一化、权重归一化、局部响应归一化等</p><h3 id="批量归一化（Batch-Normalization-BN）"><a href="#批量归一化（Batch-Normalization-BN）" class="headerlink" title="批量归一化（Batch Normalization,BN）"></a>批量归一化（Batch Normalization,BN）</h3><p>对于一个深层神经网络，有<br>$$<br>\pmb a^{(l)}&#x3D;f(\pmb z^{(l)}) &#x3D; f(W\pmb a^{(l-1)}+\pmb b)<br>$$<br>其中$f(·)$使激活函数，W和b是可学习参数，批量归一化对深层网络的净输入$\pmb z^{(l)}$进行归一化，应用于仿射变换之后，激活函数之前，具体的，对于净输入应用z值标准化：<br>$$<br>\hat z^{(l)} &#x3D; { {\pmb z^{(l)} - \mathbb E[\pmb z^{(l)}]} \over {\sqrt{var(\pmb z^{(l)})+\epsilon}} }<br>$$<br>其中期望和方差都针对于$\pmb z^{(l)}$在整个训练集上的结果，而由于训练时更多的是应用批量化的随机梯度下降，因此在整个数据集上算是不现实的，因此使用单个批量上的均值和方差来拟合整体的期望和方差，有<br>$$<br>\mu_\mathcal B &#x3D; {1\over K}\sum_{k&#x3D;1}^K\pmb z^{(k,l)},\ \ \sigma_\mathcal B^2&#x3D;{1\over K}\sum_{k&#x3D;1}^K(\pmb z^{(k,l)}-\mu_\mathcal B)\odot (\pmb z^{(k,l)}-\mu_\mathcal B)<br>$$<br>最后采用批量上的均值和方差代替整体的期望和方差。<br>$$<br>\hat {\pmb z}^{(l)} &#x3D; { {\pmb z^{(l)} - \mu_\mathcal B} \over \sqrt{\sigma_\mathcal B^2+\epsilon} } \odot \gamma + \beta \Leftrightarrow BN_{\gamma,\beta}(\pmb z^{(l)})<br>$$<br>其中$\gamma$表示缩放因子，$\beta$表示平移因子，当$\gamma&#x3D;\sigma_\mathcal B,\beta&#x3D;\mu_\mathcal B$时，调整后的$\pmb z$可以学习回到原有的$\pmb z$，这样也提高了本身的灵活性。但由于在RNN中，同一个神经元在不同时刻的值有所不同，无法确定在同一个神经元中的固定结果，维度不同从而导致$BN$很难应用进去。</p><h3 id="层归一化（Layer-Normalization-LN）"><a href="#层归一化（Layer-Normalization-LN）" class="headerlink" title="层归一化（Layer Normalization,LN）"></a>层归一化（Layer Normalization,LN）</h3><p>与批量归一化不同，层归一化针对的是每一层的所有样本，而不是基于一个batch。设第$l$层的神经元净输入为$z^{(l)}$，则有期望和方差为<br>$$<br>\mu^{(l)} &#x3D; {1\over n^l}\sum_{i&#x3D;1}^{n^l} z_i^{(l)}, \ {\sigma^{(l)} }^2 &#x3D; {1 \over n^l}\sum_{i&#x3D;1}^{n^l} (z_i^{(l)} - \mu^{(l)})^2, \ \ 其中n^{l}为l层的神经元数量<br>$$<br>求出层上的均值和方差之后，归一化的方法和批量归一化一样都是移动中心点。<br>$$<br>\hat {\pmb z}^{(l)} &#x3D;{ {\pmb z^{(l)} - \mu^{(l)}} \over \sqrt{ {\sigma^{(l)} } ^2+\epsilon} } \odot \gamma + \beta \Leftrightarrow LN_{\gamma,\beta}(\pmb z^{(l)})<br>$$<br>批量归一化和层归一化的对比可以看下图：</p><p><img src="/post/cbfc2cb0/image-20221122153002506.png" alt="image-20221122153002506"></p><p>可以看到，第一张图中，蓝色的区域表示在第$i$个深度中的所有样本$N$，第二个部分是第$i$个样本在所有神经元中训练，第三个和第四个也是逐层归一化的一种方法，分别对应一个实体的归一化和一个组的归一化。</p><p>层归一化和批量归一化整体上是十分类似的，差别在于归一化的方法不同。对于$K$个样本的一个小批量集合$Z^{(l)} &#x3D; [Z_1^{(l)},Z_2^{(l)},…,Z_m^{(l)}]$，层归一化是对矩阵$Z^{(l)}$的每一列进行归一化，而批量归一化是对每一行进行归一化。一般而言，批量归一化是一种更好的选择。当小批量样本数量比较小时，可以选择层归一化。此外，归一化方法中还有基于权重的归一化（$WN$）和局部区域的响应归一化（$LRN$），这里不再详述。</p><h2 id="超参数优化"><a href="#超参数优化" class="headerlink" title="超参数优化"></a>超参数优化</h2><p>超参数的类型众多，且范围不尽相同，因此针对于超参数的搜索和优化难度相对较高，需要找到一种相对合理的赋值或搜索的方法。</p><h3 id="超参数的种类"><a href="#超参数的种类" class="headerlink" title="超参数的种类"></a>超参数的种类</h3><p>常见的超参数包含以下三种：</p><ol><li>网络结构，包括神经元之间的连接关系、层数、每层的神经元数量、激活函数的类型等．</li><li>优化参数，包括优化方法、学习率、小批量的样本数量等．</li><li>正则化系数．</li></ol><p>超参数优化（Hyperparameter Optimization）是一个组合优化问题，无法像一般参数那样通过梯度下降方法来优化，也没有一种通用有效的优化方法；同时，评估一组超参数配置（Configuration）一般需要完整运行整套机制，因此时间成本很高，需要从参数的选择本身进行优化。</p><h3 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h3><p>是一种通过尝试所有超参数的组合来寻找合适一组超参数配置的方法。假设共有$K$个超参数，其中第$k$个超参数可以取到$m_k$个值；如果参数是连续的，可以选择几个“经验”值作为整体，一般这个经验值非等距；进一步的，为了能够使每一维上的值尽可能多的覆盖，可以采用随机搜索的方法，用不同取值的组合来模拟随机性。</p><p><img src="/post/cbfc2cb0/image-20221122160614242.png" alt="image-20221122160614242"></p><h3 id="贝叶斯优化"><a href="#贝叶斯优化" class="headerlink" title="贝叶斯优化"></a>贝叶斯优化</h3><p>通过当前已经尝试过的超参数组合来预测下一步的最优组合，每个组合的优良与否采用概率的方式给出，即对当前的$K$种已尝试的方案建模。</p><p>具体的，比较常用的贝叶斯优化方法是时序模型优化：假设超参数优化的函数$f(x)$服从高斯分布，贝叶斯过程通过根据$N$组已经产生结果的数据$\mathcal H&#x3D;{\pmb x^{(n)},y^{(n)} } <em>{i&#x3D;1} ^N$建模后验分布$\mathcal p</em>{\mathcal {GP} }(f(x)|x)$。而要使后验分布尽可能去接近真实分布，其需要对样本空间进行足够多的采样，为了使尽可能少的样本产生尽可能好的效果，需要对样本设置一个收益函数，比较常用的是期望改善。具体的设置方法不再详述。</p><p><img src="/post/cbfc2cb0/image-20221122162638043.png" alt="image-20221122162638043"></p><h3 id="动态资源分配"><a href="#动态资源分配" class="headerlink" title="动态资源分配"></a>动态资源分配</h3><p>动态资源分配的关键是将有限的资源分配给更有可能带来收益的超参数组合．一种有效方法是逐次减半（Successive Halving）方法，将超参数优化看作一种非随机的最优臂问题，即在给定有限的机会次数下，如何玩这些赌博机并找到收益最大的臂。和多臂赌博机问题类似，最优臂问题也是在利用和探索之间找到最佳的平衡。</p><h3 id="神经架构搜索"><a href="#神经架构搜索" class="headerlink" title="神经架构搜索"></a>神经架构搜索</h3><p>神经架构搜索的基本思想是通过神经网络来实现网络架构的设计，利用元学习的思想，神经架构搜索利用一个控制器来生成另一个子网络的架构描述。控制器可以由一个循环神经网络来实现。控制器的训练可以通过强化学习来完成，其奖励信号为生成的子网络在开发集上的准确率。</p><h2 id="网络正则化"><a href="#网络正则化" class="headerlink" title="网络正则化"></a>网络正则化</h2><p>神经网络会优先记住具有一般性的规律，之后才会考虑到噪声，且由于通用近似定理的存在，神经网络的拟合能力也十分突出。但神经网络所用到的参数规模十分庞大，一般会超过样本的规模，因此很容易出现过拟合的问题，故需要对网络采用正则化的技术，限制其拟合能力到正常范围。正则化的方法原理是损害优化，其可以通过增加优化约束（L1&#x2F;L2约束、数据增强）、干扰优化过程（权重衰减、随机梯度下降、早停法）等方法实现。</p><h3 id="l-1-和-l-2-正则化（Normalization）"><a href="#l-1-和-l-2-正则化（Normalization）" class="headerlink" title="$l_1$和$l_2$正则化（Normalization）"></a>$l_1$和$l_2$正则化（Normalization）</h3><p>$$<br>\theta^* &#x3D; \mathop{argmin}_\theta {1 \over N}\mathcal L(y^{(n)},f(\pmb x^{(n)},\theta)) + \lambda \mathscr l_p(\theta)<br>$$</p><p>其中，$\mathscr l_p$是范数函数，$p$的取值一般取${1,2}$，$\mathscr l_1&#x3D;\mathop {\sum_i} |\theta_i| \le 1$，$\mathscr l_2&#x3D;\sum_i(\theta_i)^2 \le 1$；$\lambda$为正则化系数。</p><p><img src="/post/cbfc2cb0/image-20221123133816828.png" alt="image-20221123133816828"></p><h3 id="早停法"><a href="#早停法" class="headerlink" title="早停法"></a>早停法</h3><p>使用验证集来测试每一次迭代的参数在验证集上是否是最优，如果在验证集上的错误率不再下降就停止迭代，防止过拟合。</p><h3 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h3><p>在每次参数更新时引入一个衰减系数$w$：<br>$$<br>\theta_t \leftarrow (1-w)\theta_{t-1}-\alpha\pmb g_t<br>$$<br>对于$l_2$正则化，其公式可以表达为：<br>$$<br>\theta_t &#x3D; \theta_{t-1}-\alpha(\pmb g_t+\lambda\theta_{t-1}) &#x3D; (1-\alpha\lambda)\theta_{t-1}-\alpha\pmb g_t<br>$$<br>当$w&#x3D;\alpha\lambda$时，可以看出在随机梯度下降的背景下，权重衰减的效果与$l_2$正则化是完全一致的。</p><h3 id="暂退法（Dropout）"><a href="#暂退法（Dropout）" class="headerlink" title="暂退法（Dropout）"></a>暂退法（Dropout）</h3><p>在神经网络中，多个神经元的行为一致，可能会出现协同效应，这会降低模型的鲁棒性和表达能力，而当所有神经元的行为一致，就会退化为对称效应。为了避免协同效应，可以采用暂退法，对于一个神经层$y&#x3D;f(Wx+b)$，引入一个丢弃函数$d(·)$使得$y&#x3D;f(Wd(x)+b)$。</p><p>在训练时，$d(\pmb x) &#x3D; \pmb m \odot \pmb x$，其中$\pmb m \in {0,1}^d$是丢弃掩码，可以通过依概率为$p$的伯努利分布生成，在训练时，$1-p$部分的神经元会被丢弃掉，这个过程是随机的，因此具有很强的多样性，其表达能力也会因为随机性而破坏程度降低。</p><p><img src="/post/cbfc2cb0/image-20221123134031472.png" alt="image-20221123134031472"></p><p>在测试时，由于训练时神经元是依概率为$p$的伯努利分布随机$dropout$的，因此仅会有$p·n^l$​的数量的神经元保留，而在测试时，所有的神经元都是可以激活的，这会造成训练和测试时网络的输出不一致，因此需要对输入限制，即$d(\pmb x) &#x3D; p\pmb x$，相当于对不同的神经元做平均。</p><p>对于暂退法，可以有以下两种形式的理解：</p><p><strong>集成学习的解释：</strong>每做一次$Dropout$是对原有的完整网络做了一次子网络提取，若一个神经网络有$n$个神经元，则可以提取出$2^n$个子网络，这些子网络共享模型参数，相当于每次采用的是不同的网络训练，这会有效提高模型的鲁棒性。</p><p><strong>贝叶斯学习的解释：</strong></p><p>假设参数服从先验分布$q(\theta)$，则有贝叶斯加和为<br>$$<br>E_{q(\theta)}[y] &#x3D; \int_qf(\pmb x,\theta)q(\theta)d\theta \approx {1\over M}\sum_{m&#x3D;1}^Mf(\pmb x,\theta_m)<br>$$<br>其中$f(\pmb x,\theta_m)$是第$m$次$dropout$后的网络，多个子网络分布的综合可以看作对先验分布的期望。</p><p>在循环神经网络中，每次采样的参数需要在每个时刻保持不变，因此在对循环神经网络上使用$dropout$时需要对参数矩阵的每个元素随机丢弃，并在所有时刻都使用相同的丢弃掩码。这种丢弃方式称之为变分丢弃法。</p><p><img src="/post/cbfc2cb0/image-20221123143310503.png" alt="image-20221123143310503"></p><h2 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h2><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>在数据中引入噪声或加入变换来增加数据的多样性和数量。</p><p><strong>图像增强：</strong>旋转、平移、翻转、缩放、加噪声等</p><p><strong>文本增强：</strong>词汇替换、回译、随机编辑噪声（增删改查、句子乱序）</p><h3 id="标签平滑"><a href="#标签平滑" class="headerlink" title="标签平滑"></a>标签平滑</h3><p>在样本的输出标签中加入噪声来避免模型过拟合。若一个样本$x$的标签用独热编码表示：$\pmb y&#x3D;[0,…,0,1,0,…,0]\top$，则加入一个很小的噪声$\epsilon$，即<br>$$<br>\hat {\pmb y} &#x3D; [{\epsilon \over {K-1} } ,,,,,{\epsilon \over {K-1} },1-\epsilon,{\epsilon \over {K-1} } ,…,{\epsilon \over {K-1} }  ]\top<br>$$<br>若要在标签中继续加入语义相关性，可以采用知识蒸馏的方法，比如先训练另外一个更复杂（一般为多个网络的集成）的教师网络（Teacher</p><p>Network），并使用大网络的输出作为软目标来训练学生网络（Student Network）。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云计算课计算重点笔记</title>
      <link href="/post/475f1eb0.html"/>
      <url>/post/475f1eb0.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>先想输出长什么样子，然后反推</p><h1 id="批量计算"><a href="#批量计算" class="headerlink" title="批量计算"></a>批量计算</h1><h2 id="倒排索引"><a href="#倒排索引" class="headerlink" title="倒排索引"></a>倒排索引</h2><h3 id="初次思考"><a href="#初次思考" class="headerlink" title="初次思考"></a>初次思考</h3><p>按照文件的数量分成多个$batch$，对每个$batch$进行$Map$过程，$Map$过程为列出$&lt;词,[文件名,]&gt;$的键值对，$Reduce$过程合并所有键值对中键相同的值列表，即按照词进行文件名的合并，最后输出。</p><h3 id="讲解"><a href="#讲解" class="headerlink" title="讲解"></a>讲解</h3><p>按照文件的数量分成多个$batch$，对每个$batch$进行$Map$过程，$Map$过程为列出<span class="label label-$<<词,文件名>,出现次数>$ green %>的键值对 green"></span>，$Reduce$过程 <span class="label label-按照词的类型进行合并,将文件名和出现次数的对应关系进行字符串拼接 blue"></span>，即按照词进行文件名的合并，最后输出。</p><h2 id="相似度比较"><a href="#相似度比较" class="headerlink" title="相似度比较"></a>相似度比较</h2><h3 id="初次思考-1"><a href="#初次思考-1" class="headerlink" title="初次思考"></a>初次思考</h3><p>没思考出来。。</p><h3 id="讲解-1"><a href="#讲解-1" class="headerlink" title="讲解"></a>讲解</h3><span class="label label-数据不变，每部分的工作互不影响，因此是批量计算问题 red"></span>。键值对中的键是文本对$<文本1,文本2>$，相似性是按照词的重复次数来计算。<p>首先先倒排索引，将文件处理成词语-文件-次数的数据，根据词将倒排索引的结果进行$batch$，然后根据词语进行$Map$，$Map$过程根据词语，将有相同词语的文件合并，$Map$的结果为$&lt;&lt;文件1,文件2&gt; | 重复次数&gt;$，其中重复次数需要设定统计方式，中间过程和$Reduce$过程将每个$batch$中键相同（即文件对相同）的进行合并，重复次数相加，最后输出为重复次数超过阈值（阈值自定义）的结果。</p><h2 id="关系的自然连接"><a href="#关系的自然连接" class="headerlink" title="关系的自然连接"></a>关系的自然连接</h2><p>键值对：需要加上标记位。</p><p>$Map$过程加入<span class="label label-表的标记位 pink"></span>，结果为$&lt;order | &lt;表名,[值1,值2,…]&gt;&gt;$，$Reduce$过程按照键$order$和标志位$表名$，进行两两连接。</p><h2 id="大矩阵乘法（去年考了）"><a href="#大矩阵乘法（去年考了）" class="headerlink" title="大矩阵乘法（去年考了）"></a>大矩阵乘法（去年考了）</h2><p>存储方式：[行标 列标 值]</p><p>键值对：$&lt;&lt;行标,列标&gt; | 值&gt;$</p><p>思路是先定义输出结果，然后回推，看计算的最终结果需要的是哪个部分。</p><p><img src="/post/475f1eb0/20130609192256515.jpeg" alt="img"></p><h2 id="电商购物的订单统计"><a href="#电商购物的订单统计" class="headerlink" title="电商购物的订单统计"></a>电商购物的订单统计</h2><h1 id="流式计算"><a href="#流式计算" class="headerlink" title="流式计算"></a>流式计算</h1><h2 id="中小学选班长"><a href="#中小学选班长" class="headerlink" title="中小学选班长"></a>中小学选班长</h2><p><img src="/image-20221111163657462.png" alt="image-20221111163657462"></p><h1 id="图计算"><a href="#图计算" class="headerlink" title="图计算"></a>图计算</h1><h2 id="舆情传播"><a href="#舆情传播" class="headerlink" title="舆情传播"></a>舆情传播</h2><p><strong>迪杰斯特拉算法</strong>（计算每个点的最短路径，模拟时判断当前时间和最短传播时间）</p><p>加概率，免疫传播，</p></文本1,文本2>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop安装指南</title>
      <link href="/post/383a95d9.html"/>
      <url>/post/383a95d9.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hadoop一步完成安装指南"><a href="#Hadoop一步完成安装指南" class="headerlink" title="Hadoop一步完成安装指南"></a>Hadoop一步完成安装指南</h1><blockquote><p>本博客部分参考于<a href="https://blog.csdn.net/tang5615/article/details/120382513"> Hadoop集群安装和搭建（全面超详细的过程）_小汤TYT的博客-CSDN博客_hadoop环境搭建与安装</a>并稍有修改。</p></blockquote><p>Hadoop是一个开源的、可运行与Linux集群上的<a href="https://so.csdn.net/so/search?q=%E5%88%86%E5%B8%83%E5%BC%8F&spm=1001.2101.3001.7020">分布式</a>计算平台，用户可借助Hadoop存有基础环境的配置（虚拟机安装、Linux安装等），Hadoop集群搭建，配置和测试。可以借用Hadoop实现高效化的集群环境的搭建与分布式计算。</p><p>以下是hadoop的安装步骤：</p><h2 id="VMware的安装"><a href="#VMware的安装" class="headerlink" title="VMware的安装"></a>VMware的安装</h2><p>这个不再本博客再赘述了，大家可以参考此博客：</p><p><a href="https://blog.csdn.net/weixin_45912291/article/details/108894737">虚拟机VMware下载与安装教程（详细）</a></p><h2 id="虚拟机安装"><a href="#虚拟机安装" class="headerlink" title="虚拟机安装"></a>虚拟机安装</h2><p>首先到镜像网站将Linux系统的镜像包下载下来，这里使用的是CentOS7（mini）的安装包</p><p><a href="http://mirrors.aliyun.com/centos/7/isos/x86_64/">centos-7-isos-x86_64安装包下载_开源镜像站-阿里云 (aliyun.com)</a></p><h3 id="安装虚拟机"><a href="#安装虚拟机" class="headerlink" title="安装虚拟机"></a>安装虚拟机</h3><p>点击【Vmware-&gt;文件-&gt;新建虚拟机】</p><p><img src="/post/383a95d9/image-20221109125241400.png" alt="image-20221109125241400"></p><h3 id="安装程序光盘映像文件。"><a href="#安装程序光盘映像文件。" class="headerlink" title="安装程序光盘映像文件。"></a>安装程序光盘映像文件。</h3><p>这里需要定向到刚刚下载的iso文件。</p><p><img src="/post/383a95d9/image-20221109125316431.png" alt="image-20221109125316431"></p><p>修改名称与安装位置</p><p><img src="/post/383a95d9/image-20221109125410623.png" alt="image-20221109125410623"></p><h3 id="分配内存。"><a href="#分配内存。" class="headerlink" title="分配内存。"></a>分配内存。</h3><p>一般是默认系统的安装内存(20G)，我的电脑内存剩余太小了，怕炸掉所以用了10G，注意选择<font color="red">将虚拟磁盘拆分成多个文件</font>。</p><p><img src="/post/383a95d9/image-20221109125433164.png" alt="image-20221109125433164"></p><p>完成之后点击配置，即可以看到现有的系统配置，可以选择默认配置，下一步。</p><p><img src="/post/383a95d9/image-20221109125532816.png" alt="image-20221109125532816"></p><p>点击完成就结束了单个虚拟机的硬件安装过程。</p><p><img src="/post/383a95d9/image-20221109125600940.png" alt="image-20221109125600940"></p><h3 id="运行CentOS系统，开始安装系统软件"><a href="#运行CentOS系统，开始安装系统软件" class="headerlink" title="运行CentOS系统，开始安装系统软件"></a>运行CentOS系统，开始安装系统软件</h3><p><img src="/post/383a95d9/image-20221109125656849.png" alt="image-20221109125656849"></p><p>选择时区为上海，语言为中文</p><p><img src="/post/383a95d9/image-20221109125810622.png" alt="image-20221109125810622"></p><h3 id="配置系统网络并连接"><a href="#配置系统网络并连接" class="headerlink" title="配置系统网络并连接"></a>配置系统网络并连接</h3><p>注意设置主机名</p><p><img src="/post/383a95d9/image-20221109125925832.png" alt="image-20221109125925832"></p><p><img src="/post/383a95d9/image-20221109130110416.png" alt="image-20221109130110416"></p><p>点击开始安装，设置用户名和密码，不创建用户（默认为root）。</p><p><img src="/post/383a95d9/image-20221109130134791.png" alt="image-20221109130134791"></p><p><img src="/post/383a95d9/image-20221109130211492.png" alt="image-20221109130211492"></p><p>完成安装，重启。</p><h3 id="使用root账户【用户名：root；密码为自设密码】登录系统"><a href="#使用root账户【用户名：root；密码为自设密码】登录系统" class="headerlink" title="使用root账户【用户名：root；密码为自设密码】登录系统"></a>使用root账户【用户名：root；密码为自设密码】登录系统</h3><p><img src="/post/383a95d9/image-20221109130708748.png" alt="image-20221109130708748"></p><p><img src="/post/383a95d9/image-20221109130743254.png" alt="image-20221109130743254"></p><p>使用ping指令测试一下网络是否联通</p><p><img src="/post/383a95d9/image-20221109130840614.png" alt="image-20221109130840614"></p><h2 id="地址配置"><a href="#地址配置" class="headerlink" title="地址配置"></a>地址配置</h2><h3 id="安装net-tools工具"><a href="#安装net-tools工具" class="headerlink" title="安装net-tools工具"></a>安装net-tools工具</h3><p>net-tools工具可以便于之后操作网络地址信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum upgrade<br>yum install net-tools<br></code></pre></td></tr></table></figure><h3 id="查看MAC物理地址和ip地址范围"><a href="#查看MAC物理地址和ip地址范围" class="headerlink" title="查看MAC物理地址和ip地址范围"></a>查看MAC物理地址和ip地址范围</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ifconfig<br></code></pre></td></tr></table></figure><p>将【enter】后面的mac地址记录下来，之后用到。</p><p><img src="/post/383a95d9/image-20221109131307436.png" alt="image-20221109131307436"></p><p>然后点击当前虚拟机下的网络编辑器，【编辑-&gt;虚拟网络编辑器】，选择NAT模式下的VMnet8网络，点击【DHCP配置】</p><p><img src="/post/383a95d9/image-20221109131354974.png" alt="image-20221109131354974"></p><p>这里的起始和结束地址就是虚拟机的公用地址范围，可以默认将【192.168.XX.131】记下来作为hadoop1的ip地址，如我这里就是【192.168.47.131】。</p><p><img src="/post/383a95d9/image-20221109131415451.png" alt="image-20221109131415451"></p><h3 id="修改网络配置文件"><a href="#修改网络配置文件" class="headerlink" title="修改网络配置文件"></a>修改网络配置文件</h3><p>进入网络配置文件，修改之</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi /etc/sysconfig/network-scripts/ifcfg-ens33<br></code></pre></td></tr></table></figure><p>修改BOOTPROTO的镜像模式为”static”，若ONBOOT为&#x3D;”no”则修改为”yes”，然后在配置文件后加入下面的配置信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">IPV6_PRIVACY=&quot;yes&quot;<br>WADDR=&quot;00:0c:29:XX:XX:XX&quot; #mac地址<br>IPADDR=&quot;192.168.XX.131&quot; #IP地址<br>GATEWAY=&quot;192.168.XX.2&quot; #网关地址，默认为ip最后一段改为2<br>NETMASK=&quot;255.255.255.0&quot; #子网掩码，照抄<br>DNS1=&quot;8.8.8.8&quot; #DNS定位，用谷歌的免费DNS1<br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109132519558.png" alt="image-20221109132519558"></p><p>重启网络和系统，然后查看是否配置成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl restart network<br>reboot<br>ifconfig<br>ping www.baidu.com<br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109134137887.png" alt="image-20221109134137887"></p><h2 id="虚拟机克隆"><a href="#虚拟机克隆" class="headerlink" title="虚拟机克隆"></a>虚拟机克隆</h2><p>完成单个虚拟机的配置之后，将虚拟机克隆两次，并另外命名为hadoop2和hadoop3，作为数据节点。</p><h3 id="克隆"><a href="#克隆" class="headerlink" title="克隆"></a>克隆</h3><p>右键hadoop1虚拟机，【管理-&gt;克隆】，注意<font color="red">创建完整克隆</font></p><p><img src="/post/383a95d9/image-20221109134346446.png" alt="image-20221109134346446"></p><p><img src="/post/383a95d9/image-20221109173253406.png" alt="image-20221109173253406"></p><p><img src="/post/383a95d9/image-20221109173318270.png" alt="image-20221109173318270"></p><p><img src="/post/383a95d9/image-20221109134453256.png" alt="image-20221109134453256"></p><h3 id="二次地址配置"><a href="#二次地址配置" class="headerlink" title="二次地址配置"></a>二次地址配置</h3><p>完成克隆之后，需要按照第一个虚拟机的方法再次进行两次地址配置，这里不再赘述过程，放出来图片供大家参考</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ifconfig<br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109134812016.png" alt="image-20221109134812016"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi /etc/sysconfig/network-scripts/ifcfg-ens33<br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109135849928.png" alt="image-20221109135849928"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">ping www.baidu.com<br>systemctl restart network<br>ifconfig<br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109135902333.png" alt="image-20221109135902333"></p><p><img src="/post/383a95d9/image-20221109140207521.png" alt="image-20221109140207521"></p><h2 id="免密登录"><a href="#免密登录" class="headerlink" title="免密登录"></a>免密登录</h2><p>为了以免之后主机链接客机的时候每次都得填密码（那会累死的），设置一下ssh的免密登录。</p><h3 id="修改hosts文件"><a href="#修改hosts文件" class="headerlink" title="修改hosts文件"></a>修改hosts文件</h3><p>首先修改hosts文件的配置，将主机和客机都加进去</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi /etc/hosts<br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109140636069.png" alt="image-20221109140636069"></p><h3 id="生成密钥文件"><a href="#生成密钥文件" class="headerlink" title="生成密钥文件"></a>生成密钥文件</h3><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">ssh-keygen -t rsa</span><br></code></pre></td></tr></table></figure><p>回车三次即可，如果是已有密钥文件，回车四次。</p><p><img src="/post/383a95d9/image-20221109140938883.png" alt="image-20221109140938883"></p><h3 id="复制密钥"><a href="#复制密钥" class="headerlink" title="复制密钥"></a>复制密钥</h3><p>将本机公钥文件复制到其它虚拟机上（接收方需先开机），在三个主机上都需要分别输入下列复制文件，保证三台主机都能够免密登录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-copy-id hadoop01<br>ssh-copy-id hadoop02<br>ssh-copy-id hadoop03<br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109141259437.png" alt="image-20221109141259437"></p><p><img src="/post/383a95d9/image-20221109141157065.png" alt="image-20221109141157065"></p><p><img src="/post/383a95d9/image-20221109141225064.png" alt="image-20221109141225064"></p><p>使用ssh指令测试一下是否成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">比如在hadoop1主机下输入</span><br>ssh hadoop2<br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109141621838.png" alt="image-20221109141621838"></p><h2 id="安装Hadoop和JDK"><a href="#安装Hadoop和JDK" class="headerlink" title="安装Hadoop和JDK"></a>安装Hadoop和JDK</h2><p>首先在每个主机下创建三个文件夹，data用于存数据、servers用于存储服务器配置、software用于存储软件包等。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p /export/data<br>mkdir -p /export/servers<br>mkdir -p /export/software<br></code></pre></td></tr></table></figure><h3 id="下载安装包"><a href="#下载安装包" class="headerlink" title="下载安装包"></a>下载安装包</h3><p>推荐使用更为稳定的hadoop-2.7.4版本和jdk8。</p><p>hadoop-2.7.4.tar.gz ：<a href="https://hadoop.apache.org/release/2.7.4.html">Apache Hadoop</a></p><p>jdk-8u161-linux-x64.tar.gz ：<a href="https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html">Java Archive Downloads - Java SE 8 (oracle.com)</a></p><h3 id="安装Xshell"><a href="#安装Xshell" class="headerlink" title="安装Xshell"></a>安装Xshell</h3><p>安装Xshell软件，这个软件能够方便的进行多主机协同，相对于虚拟机在UI和功能上更为友好。</p><p><a href="https://www.xshell.com/zh/free-for-home-school/">家庭&#x2F;学校免费 - NetSarang Website (xshell.com)</a>，直接用免费的就好，然后和普通软件一样下一步*n。</p><h3 id="配置Xshell中的节点环境"><a href="#配置Xshell中的节点环境" class="headerlink" title="配置Xshell中的节点环境"></a>配置Xshell中的节点环境</h3><p>打开Xshell后点击文件并选择新建，名称填hadoop1，主机填写hadoop1的IP地址。</p><p><img src="/post/383a95d9/image-20221109155443676.png" alt="image-20221109155443676"></p><p><img src="/post/383a95d9/image-20221109155533585.png" alt="image-20221109155533585"></p><p>再点击用户身份验证，把hadoop1的账号，密码输入，就可以通过Xshell控制虚拟机，方便后续软件的传输。</p><p><img src="/post/383a95d9/image-20221109155613472.png" alt="image-20221109155613472"></p><p>重复步骤新建会话控制hadoop2，hadoop3。</p><p><img src="/post/383a95d9/image-20221109155655906.png" alt="image-20221109155655906"></p><h3 id="上传文件（以hadoop1为例）"><a href="#上传文件（以hadoop1为例）" class="headerlink" title="上传文件（以hadoop1为例）"></a>上传文件（以hadoop1为例）</h3><p>安装一个插件方便上传文件，提供资源管理器的UI</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /export/software<br>yum   -y  install  lrzsz<br>rz<br></code></pre></td></tr></table></figure><p>rz指令之后会进入资源管理器，将hadoop文件和jdk文件上传到系统中</p><p><img src="/post/383a95d9/image-20221109155919795.png" alt="image-20221109155919795"></p><p><img src="/post/383a95d9/image-20221109160145263.png" alt="image-20221109160145263"></p><p>之后解压hadoop和jdk的压缩包到software文件夹下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /export/software<br>tar -zxvf jdk-8u161-linux-x64.tar.gz -C /export/servers/<br>tar -zxvf hadoop-2.7.4.tar.gz -C /export/servers/<br><span class="hljs-meta prompt_">#</span><span class="language-bash">重命名jdk文件夹</span><br>cd /export/servers<br>mv jdk1.8.0_161 jdk<br></code></pre></td></tr></table></figure><h3 id="软件配置（以hadoop1为例）"><a href="#软件配置（以hadoop1为例）" class="headerlink" title="软件配置（以hadoop1为例）"></a>软件配置（以hadoop1为例）</h3><p>将hadoop和jdk的配置信息加到全局配置中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi /etc/profile #进入全局配置文件<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">tip:在配置文件末尾追加</span><br>export JAVA_HOME=/export/servers/jdk<br>export PATH=$PATH:$JAVA_HOME/bin<br>export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar<br><span class="hljs-meta prompt_">#</span><span class="language-bash">tip:在文件末尾追加</span><br>export HADOOP_HOME=/export/servers/hadoop-2.7.4<br>export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin<br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109161459940.png" alt="image-20221109161459940"></p><p>更新配置文件使之生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure><p>这样就完成了两个软件的配置，可以分别使用<code>java -version</code>和<code>hadoop version</code>查看是否成功安装。</p><p><img src="/post/383a95d9/image-20221109161201657.png" alt="image-20221109161201657"></p><p><img src="/post/383a95d9/image-20221109161354682.png" alt="image-20221109161354682"></p><p>上述两个小步骤需要在hadoop2和hadoop3分别完成。</p><h2 id="Hadoop集群配置"><a href="#Hadoop集群配置" class="headerlink" title="Hadoop集群配置"></a>Hadoop集群配置</h2><blockquote><p>（vi编辑器：i为编辑模式，esc为退出编辑模式，:wq为保存并退出）</p></blockquote><p>首先进入主节点hadoop1的配置目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /export/servers/hadoop-2.7.4/etc/hadoop/<br></code></pre></td></tr></table></figure><h3 id="java地址配置"><a href="#java地址配置" class="headerlink" title="java地址配置"></a>java地址配置</h3><p>修改hadoop中对于java环境地址的配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi hadoop-env.sh<br><span class="hljs-meta prompt_">#</span><span class="language-bash">tip:找到相应位置，添加这段话</span><br>export JAVA_HOME=/export/servers/jdk<br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109161744733.png" alt="image-20221109161744733"></p><h3 id="核心配置文件"><a href="#核心配置文件" class="headerlink" title="核心配置文件"></a>核心配置文件</h3><p>修改<strong>core-site.xml</strong>文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi core-site.xml<br></code></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs xml">#tip:下图中乱码部分为注释代码，可以删掉，不影响<br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--配置Hadoop的文件系统，由URI指定--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-comment">&lt;!--配置namenode的地址到hadoop1--&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://hadoop1:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--配置Hadoop的临时目录，默认/tem/hadoop-$&#123;user.name&#125;--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/export/servers/hadoop-2.7.4/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109161949003.png" alt="image-20221109161949003"></p><p>修改<strong>hdfs-site.xml</strong>文件，其中hdfs的每个部分的含义可以参见以下链接。</p><p><a href="https://www.cnblogs.com/jpcflyer/p/9005222.html">hadoop配置文件详解系列（二）-hdfs-site.xml篇 - 程序员姜小白 - 博客园 (cnblogs.com)</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi hdfs-site.xml<br></code></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--指定HDFS的数量，默认不变就可以--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>3<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--secondarynamenode 所在主机的IP和端口--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop2:50090<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109162339041.png" alt="image-20221109162339041"></p><h3 id="附属配置文件"><a href="#附属配置文件" class="headerlink" title="附属配置文件"></a>附属配置文件</h3><p>修改<strong>mapred-site.xml</strong>文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cp mapred-site.xml.template mapred-site.xml<br>vi mapred-site.xml<br></code></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--指定MapReduce运行时的框架，这里指定在YARN上，默认在local--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109163300911.png" alt="image-20221109163300911"></p><p>修改<strong>yarn-site.xml</strong>文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi yarn-site.xml<br></code></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--指定YARN集群的管理者（ResourceManager）的地址--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-comment">&lt;!--指定yarn节点的管理逻辑--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109162829388.png" alt="image-20221109162829388"></p><h3 id="节点配置"><a href="#节点配置" class="headerlink" title="节点配置"></a>节点配置</h3><p>修改<strong>slaves</strong>文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">tip:将文件中的localhost删除，添加主节点和子节点的主机名称</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">tip:如主节点hadoop1，子节点hadoop2和hadoop3</span><br>vi slaves<br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109162752736.png" alt="image-20221109162752736"></p><h3 id="复制配置到子结点"><a href="#复制配置到子结点" class="headerlink" title="复制配置到子结点"></a>复制配置到子结点</h3><p>将主节点中配置好的文件和hadoop目录copy给子节点，其中主节点为hadoop1，子结点为hadoop2和hadoop3。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">scp /etc/profile hadoop02:/etc/profile<br>scp /etc/profile hadoop03:/etc/profile<br>scp -r /export/ hadoop02:/<br>scp -r /export/ hadoop03:/<br></code></pre></td></tr></table></figure><p>去到hadoop2和hadoop3主机中，使新传过来的配置信息生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">tip:返回hadoop2和hadoop3节点执行下面命令</span><br>source /etc/profile<br></code></pre></td></tr></table></figure><h3 id="格式化文件系统"><a href="#格式化文件系统" class="headerlink" title="格式化文件系统"></a>格式化文件系统</h3><p>在主节点格式化文件系统，初始化hadoop的文件系统配置。如图片中的红框，给出【successfully formatted】即代表已经成功格式化文件系统。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs namenode -format<br></code></pre></td></tr></table></figure><p><img src="/post/383a95d9/image-20221109163720042.png" alt="image-20221109163720042"></p><h2 id="集群启动"><a href="#集群启动" class="headerlink" title="集群启动"></a>集群启动</h2><h3 id="启动进程"><a href="#启动进程" class="headerlink" title="启动进程"></a>启动进程</h3><p>在主节点启动dfs和yarn服务进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">start-dfs.sh<br>start-yarn.sh<br>jps<br></code></pre></td></tr></table></figure><p>在主节点会显示ResourceManager（Yarn）、Namenode（主节点）、NodeManager（Hadoop）、在次主节点会显示SecondaryNamenode。</p><p><img src="/post/383a95d9/image-20221109165428445.png" alt="image-20221109165428445"></p><p>关闭进程可以使用stop进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">stop-dfs.sh<br>stop-yarn.sh<br></code></pre></td></tr></table></figure><h3 id="关闭防火墙（所有虚拟机都要操作）"><a href="#关闭防火墙（所有虚拟机都要操作）" class="headerlink" title="关闭防火墙（所有虚拟机都要操作）"></a>关闭防火墙（所有虚拟机都要操作）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl stop firewalld    #关闭防火墙<br>systemctl disable firlewalld  #关闭防火墙开机启动<br></code></pre></td></tr></table></figure><h3 id="本次配置"><a href="#本次配置" class="headerlink" title="本次配置"></a>本次配置</h3><p>打开window下的【C:\Windows\System32\drivers\etc】，以管理员权限打开hosts文件，在文件末添加三行代码：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">192.168.121.134</span> hadoop01<br><span class="hljs-number">192.68.121.135</span> hadoop02<br><span class="hljs-number">192.168.121.136</span> hadoop03<br></code></pre></td></tr></table></figure><p>在windows系统下访问<a href="http://hadoop1:50070，即可查看hdfs集群状态；访问http://hadoop1:8088，查看Yarn集群状态。">http://hadoop1:50070，即可查看hdfs集群状态；访问http://hadoop1:8088，查看Yarn集群状态。</a></p><p>这样就完成了整个hadoop文件系统的配置，个人觉得比较麻烦，需要一步步很细节的去完成。</p>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> 云计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>循环神经网络笔记</title>
      <link href="/post/502e77b9.html"/>
      <url>/post/502e77b9.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="《神经网络与机器学习》第五章笔记"><a href="#《神经网络与机器学习》第五章笔记" class="headerlink" title="《神经网络与机器学习》第五章笔记"></a>《神经网络与机器学习》第五章笔记</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p><img src="/post/502e77b9/image-20221105181433396.png" alt="image-20221105181433396"></p><h3 id="有限自动机"><a href="#有限自动机" class="headerlink" title="有限自动机"></a>有限自动机</h3><p>有限状态机是一种用来进行对象行为建模的工具，其作用主要是描述对象在它的生命周期内所经历的状态序列，以及如何响应来自外界的各种事件。在计算机科学中，有限状态机被广泛用于建模应用行为、硬件电路系统设计、软件工程，编译器、网络协议、和计算与语言的研究。</p><p><img src="/post/502e77b9/image-20221105181553093.png" alt="image-20221105181553093"></p><h3 id="图灵机"><a href="#图灵机" class="headerlink" title="图灵机"></a>图灵机</h3><p>控制器可以存储当前自身的状态；读写头，可以读、写存储带上小方格的数字&#x2F;字母。在工作过程中，图灵机可以根据读写头读到数字&#x2F;字母和程序更改自身的状态。</p><p><img src="/post/502e77b9/image-20221105181741496.png" alt="image-20221105181741496"></p><p>以上的机器中都用到了之前的状态对之后的状态的影响和修改，这要求机器本身需要有记忆能力，即对之前的状态进行保存与修正。循环神经网络，即为在前馈网络等的基础上加入记忆元素的网络。</p><h3 id="延时神经网络"><a href="#延时神经网络" class="headerlink" title="延时神经网络"></a>延时神经网络</h3><p>建立额外的延时单元保存网络的历史信息（包括输入、输出、隐状态等）。<br>$$<br>\pmb h_t ^{(l)} &#x3D; f(\pmb h_t ^{(l-1)},\pmb h_{t-1} ^{(l-1)},…,\pmb h_{t-K} ^{(l-1)})<br>$$<br>其中$K$在不同层数$l$上有所不同。延时单元本身基于前馈神经网络。</p><h3 id="自回归模型"><a href="#自回归模型" class="headerlink" title="自回归模型"></a>自回归模型</h3><p>一种时间序列模型，使用变量$y$的历史信息来预测当前的$y$，即：<br>$$<br>\pmb y_t &#x3D; w_0 + \sum_{k&#x3D;1} ^K w_k \pmb y_{t-k} + \epsilon_t<br>$$<br>其中，$w_0$是模型偏置，可以理解为$b$；$\epsilon_t$是模型噪声，可以使用高斯噪声$\epsilon _t \sim N(0,\sigma^2)$</p><p>有外部输入的非线性模型是在上述模型的基础上进一步引入输入$\pmb x$的影响。<br>$$<br>\pmb y_t &#x3D; f(\pmb x_t ,\pmb x_{t-1},…,\pmb x_{t-K_x},\pmb y_{t-1}, \pmb y_{t-2},…,\pmb y_{t-K_y})<br>$$<br>其中$f(·)$表示非线性函数，其可以是一个前馈网络，$K_x$和$K_y$是超参数。</p><h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><p>循环神经网络在借用了自回归的变量存储思路，使用一个带自反馈的神经元来处理任意长度的时序数据，这相对于之前的延时模型和自回归模型有很大的改进。<br>$$<br>\pmb h_t &#x3D; f(\pmb h_{t-1},\pmb x_t)<br>$$<br>其中$\pmb h_{t-1}$是对前一个活性值状态的记忆，这种记忆的实现可以采用之前提到的延时器等实现，这种基于输入$x$和前一整体状态$\pmb h$的思想与残差思想有所重合。</p><p>循环神经网络比前馈神经网络更加符合生物神经网络的结构，也因此在序列识别任务，如语音、自然语言等任务上有很广泛的应用。</p><p>循环神经网络的时间维度上由于按时间扩展，其深度很深；在每个时间刻度的层次维度上只有一个$\pmb h_t$作为非线性处理输入给出输出，因此深度较浅。</p><p><img src="/post/502e77b9/image-20221105184130797.png" alt="image-20221105184130797"></p><h2 id="循环网络的相关定理"><a href="#循环网络的相关定理" class="headerlink" title="循环网络的相关定理"></a>循环网络的相关定理</h2><h4 id="定理1：循环神经网络的通用近似定理"><a href="#定理1：循环神经网络的通用近似定理" class="headerlink" title="定理1：循环神经网络的通用近似定理"></a>定理1：循环神经网络的通用近似定理</h4><p>如果一个完全连接的循环神经网络有足够数量的$sigmoid$型隐藏神经元，它可以以任意的准确率去近似一个任意的非线性动力系统。<br>$$<br>\begin{align}<br>\pmb s_t &amp; &#x3D; g(\pmb s_{t-1}, \pmb x_t) \\\<br>\pmb y_t &amp; &#x3D; o(\pmb s_t)<br>\end{align}<br>$$<br>其中$\pmb s_t$是每个时刻的隐状态，$\pmb x_t$是外部输入，$g(·)$是可测的状态转换函数，$o(·)$是连续输出函数，且对状态空间的紧致性没有限制。</p><p>这个定理告诉我们，一个完全连接的循环神经网络可以近似任意一个可行程序。</p><h4 id="定理2：图灵完备"><a href="#定理2：图灵完备" class="headerlink" title="定理2：图灵完备"></a>定理2：图灵完备</h4><p>所有的图灵机都可以被一个使用$sigmoid$型激活函数的神经元构成的全连接循环网络来进行模拟，即可以实现图灵机的所有功能，解决所有的可计算问题。</p><h2 id="循环神经网络的应用种类"><a href="#循环神经网络的应用种类" class="headerlink" title="循环神经网络的应用种类"></a>循环神经网络的应用种类</h2><h3 id="序列到类别的模式"><a href="#序列到类别的模式" class="headerlink" title="序列到类别的模式"></a>序列到类别的模式</h3><p>序列到序列的模型，可以理解为输入为一个$\pmb x$的序列，输出为一个表示类别的标量$\hat y$。</p><p><img src="/post/502e77b9/image-20221105191923213.png" alt="image-20221105191923213"></p><p>序列到类别的应用包括情感分类（文本输入到情感类别）等。</p><h3 id="同步的序列到序列的模式"><a href="#同步的序列到序列的模式" class="headerlink" title="同步的序列到序列的模式"></a>同步的序列到序列的模式</h3><p>同步的序列到序列模式，输入为一个$\pmb x$的序列，输出也为一个序列$\pmb {\hat y}&#x3D;[\hat y_1,\hat y_2,…,\hat y_T]$。同步的性质表示为在每个隐状态中，都是一个$\pmb x_i$经处理到输出结果$\hat y_i$。</p><p><img src="/post/502e77b9/image-20221105192348310.png" alt="image-20221105192348310"></p><p>同步序列到序列的模式应用包括中文分词（文本单词输入到每个部分的位置类型输出）、信息抽取（文本单词输入到每个部分的信息类型输出）、语音识别（语音序列输入到每个子段的识别结果）等。</p><p><img src="/post/502e77b9/image-20221105192636241.png" alt="image-20221105192636241"></p><p><img src="/post/502e77b9/image-20221105192648785.png" alt="image-20221105192648785"></p><h3 id="异步的序列到序列的模式"><a href="#异步的序列到序列的模式" class="headerlink" title="异步的序列到序列的模式"></a>异步的序列到序列的模式</h3><p>异步的序列到序列模式，其输入输出也均为序列，与同步的模型不同的是，异步模型的输入与输出不在同一个隐状态函数中处理，大部分都是先输入一段信息，然后开始处理，进行输出，因此这种模型一般都可以分为$Encoder$与$Decoder$部分，其中$Encoder$负责对输入的信息进行编码，形成网络可识别的数据类型；$Decoder$负责完成网络逻辑并进行输出。</p><p><img src="/post/502e77b9/image-20221105193115078.png" alt="image-20221105193115078"></p><p>异步序列到序列的模式应用包括机器翻译（先输入文本序列，经编码后处理输出为另一段文本序列）。</p><p><img src="/post/502e77b9/image-20221105193252275.png" alt="image-20221105193252275"></p><h2 id="参数学习与梯度"><a href="#参数学习与梯度" class="headerlink" title="参数学习与梯度"></a>参数学习与梯度</h2><p>给定训练样本$\pmb{(x,y)}$，其中$\pmb x&#x3D;{x_1,x_2,…,x_T}$是输入序列，$\pmb y&#x3D; y_1,y_2,…,y_T$是标签序列，循环神经网络在时刻$t$的瞬时损失函数可以表示为：<br>$$<br>\begin{align}<br>\mathcal L_t &amp; &#x3D; \mathcal L(\pmb y_t,g(\pmb h_t)) \\\\<br>\mathcal L &amp; &#x3D; \sum_{t&#x3D;1} ^T \mathcal L_t<br>\end{align}<br>$$<br>对梯度的计算可以参考反向传播算法，针对每个时刻的梯度求解，如下图：</p><p><img src="/post/502e77b9/image-20221105203351536.png" alt="image-20221105203351536"></p><p>对于简易循环神经网络，其状态转移方法如下：<br>$$<br>\pmb h_{t+1} &#x3D; f(\pmb z_{t+1}) &#x3D; f(U\pmb h_t + W \pmb x_{t+1} + b)<br>$$<br>其中$U,W$为超参数，每个层可能有所不同。在反向传播的过程中，$\delta_{t,k}$为第$t$时刻的损失对第$k$步隐藏神经元的净输入$z_k$的导数，即<br>$$<br>\begin{align}<br>\delta_{t,k} &amp; &#x3D; {\partial \mathcal L_t \over \partial z_k} \\\\<br>&amp; &#x3D; {\partial h_k \over \partial z_k}·{\partial z_{k+1} \over \partial h_k}·{\partial \mathcal L_t \over \partial z_{k+1}}\\\\<br>&amp; &#x3D; diag(f^ \prime(z_k)) U^\top\delta_{t,k+1}  \\\\<br>&amp; &#x3D; \prod_ {\tau&#x3D;k} ^{t-1}(diag(f^ \prime(z_\tau)) U^\top)\delta_{t,t}<br>\end{align}<br>$$<br>对于整个网络的损失梯度可以表示为：<br>$$<br>\begin{align}<br>\partial \mathcal L\over \partial U &amp; &#x3D; \sum_ {t&#x3D;1} ^T {\partial \mathcal L_t \over \partial U} \\\\<br>&amp; &#x3D; \sum_ {t&#x3D;1} ^T {\sum_ {k&#x3D;1} ^t} {\partial \mathcal L_t \over \partial U^{(k)}} \\\\<br>&amp; &#x3D; \sum_ {t&#x3D;1} ^T {\sum_ {k&#x3D;1} ^t} {\partial \mathcal L_t \over \partial z_k}·{\partial z_k \over \partial U^{(k)}}\\\\<br>&amp; &#x3D; \sum_ {t&#x3D;1} ^T {\sum_ {k&#x3D;1} ^t} \ \delta_ {t,k}\pmb h_{k-1}^T<br>\end{align}<br>$$<br>对于梯度公式$\delta_{t,k} &#x3D; \prod_ {\tau&#x3D;k} ^{t-1}(diag(f^ \prime(z_\tau)) U^\top)\delta_{t,t}$，不妨设其中$\lambda  &#x3D; diag(f^ \prime(z_\tau)) U^\top$，则$\delta_{t,k} \cong \lambda^{t-k}  \delta_{t,t}$。当$\lambda&gt;1,t-k\rightarrow +\infty$时，梯度趋于无穷大，可能引发梯度爆炸问题；当$\lambda&lt;1,t-k\rightarrow +\infty$时，梯度趋于0，可能引发梯度消失问题；由于梯度爆炸或消失问题，实际上循环神经网络只能学习到短周期的依赖关系，即长程依赖问题。</p><p><img src="/post/502e77b9/image-20221107165707431.png" alt="image-20221107165707431"></p><p>因此，要保证$\lambda$尽可能等于1。更多的，解决长程依赖问题的方法可以通过线性依赖关系、门控网络等方法解决。</p><h3 id="线性依赖关系"><a href="#线性依赖关系" class="headerlink" title="线性依赖关系"></a>线性依赖关系</h3><p>将循环关系从非线性关系中摘出来。<br>$$<br>h_t &#x3D; h_{t-1} + g(x_t;\theta)<br>$$<br>但这种完全摘出来的方法可能会对循环本身造成伤害，故可以增加非线性<br>$$<br>h_t&#x3D;h_{t-1}+g(x_t,h_{t-1};\theta)<br>$$<br>这种表示方式就可以联想到残差网络的两路处理方法。</p><h2 id="GRU-Gated-Recurrent-Unit"><a href="#GRU-Gated-Recurrent-Unit" class="headerlink" title="GRU(Gated Recurrent Unit)"></a>GRU(Gated Recurrent Unit)</h2><p>针对于前文提到的加入非线性的线性依赖，可以继续加入门控网络，控制线性和非线性部分的比重，提高灵活性<br>$$<br>h_t&#x3D; z_t\odot h_{t-1} + (1-z_t)\odot g(x_t,h_{t-1};\theta),\ z_t\in(0,1)^d,\ d&#x3D;Dim(h_t)<br>$$<br>其中$z_t$是门控部分，若$z_t&#x3D;1$则遗忘$h_{t-1}$，取非线性部分；若$z_t&#x3D;0$则遗忘非线性部分，取$h_t &#x3D; h_{t-1}$，当然上述是加入门控的一种思路，实现比这要复杂许多。</p><p>更多的，由于线性依赖中的非线性关系$g(x_t,h_{t-1};\theta)$是包含记忆单元$h_{t-1}$的，因此可以类似于$z_t$的思路将$h_{t-1}$前面加上一个门控$r_t$，即可得到$GRU$门控循环网络的两个主要门控。<br>$$<br>\begin{align}<br>\pmb {\hat h_t} &amp; &#x3D; tanh( W_c \pmb x_t+ U(\pmb r_t \odot \pmb h_{t-1})+\pmb b_n) \\\\<br>\pmb h_t &amp; &#x3D; \pmb z_t\odot \pmb h_{t-1} + (1-\pmb z_t)\odot \pmb {\hat h_t}<br>\end{align}<br>$$<br>对于两个门控，本身也可以用线性组合过非线性函数来实现。<br>$$<br>\begin{align}<br>重置门: \pmb r_t &#x3D; \sigma(W_r \pmb x_t+ U_r \pmb h_{t-1}+ \pmb b_r) \\\\<br>更新门: \pmb z_t &#x3D; \sigma(W_z\pmb  x_t+ U_z\pmb h_{t-1}+ \pmb b_z)<br>\end{align}<br>$$<br><img src="/post/502e77b9/image-20221108002209549.png" alt="image-20221108002209549"></p><h2 id="LSTM-Long-Short-Term-Memory"><a href="#LSTM-Long-Short-Term-Memory" class="headerlink" title="LSTM(Long Short-Term Memory)"></a>LSTM(Long Short-Term Memory)</h2><p>基于GRU的门控思路，可以在最后输出时再加入一个输出门，用于控制输出与否和输出的强度，即网络表示为：<br>$$<br>\begin{align}<br>\pmb {\hat c_t} &amp; &#x3D; tanh(W_c \pmb x_t+ U_c\pmb h_{t-1}+\pmb b_c) \\\\<br>\pmb c_t &amp; &#x3D; \pmb f_t\odot \pmb c_{t-1} + \pmb i_t\odot \pmb {\hat h_t} \\\\<br>\pmb h_t &amp; &#x3D; \pmb o_t \odot tanh(\pmb c_t)<br>\end{align}<br>$$<br>其中每个门也都可以使用输入$x_i$和历史记忆$h_{i-1}$表示。<br>$$<br>\begin{align}<br>输入门: \pmb i_t &amp; &#x3D; \sigma(W_i\pmb x_t+ U_i\pmb h_{t-1}+ \pmb b_i) \\\\<br>遗忘门: \pmb f_t &amp; &#x3D; \sigma(W_f\pmb x_t+ U_f\pmb h_{t-1}+ \pmb b_f) \\\\<br>输出门: \pmb o_t &amp; &#x3D; \sigma(W_o\pmb x_t+ U_o\pmb h_{t-1}+ \pmb b_o)<br>\end{align}<br>$$<br><img src="/post/502e77b9/image-20221108010138710.png" alt="image-20221108010138710"></p><p>LSTM根据使用的门控不同，其也有许多派生的种类，如没有遗忘门：$\pmb c_t &#x3D; \pmb c_{t-1}+ \pmb i_t \odot \pmb {\hat c_t}$；耦合输入门和遗忘门：$\pmb f_t+\pmb i_t &#x3D; 1$等。</p><h2 id="深层模型（非时间维度上的加深）"><a href="#深层模型（非时间维度上的加深）" class="headerlink" title="深层模型（非时间维度上的加深）"></a>深层模型（非时间维度上的加深）</h2><h3 id="堆叠神经网络"><a href="#堆叠神经网络" class="headerlink" title="堆叠神经网络"></a>堆叠神经网络</h3><p>在非时间维度上堆叠多层$h$非线性层。</p><p><img src="/post/502e77b9/image-20221108144940826.png" alt="image-20221108144940826"></p><h3 id="双向神经网络"><a href="#双向神经网络" class="headerlink" title="双向神经网络"></a>双向神经网络</h3><p>使用双向的隐含状态</p><p><img src="/post/502e77b9/image-20221108145549521.png" alt="image-20221108145549521"></p><h2 id="非序列模型"><a href="#非序列模型" class="headerlink" title="非序列模型"></a>非序列模型</h2><h3 id="递归神经网络"><a href="#递归神经网络" class="headerlink" title="递归神经网络"></a>递归神经网络</h3><p>被指向结点使用指向节点的多个状态信息。</p><p><img src="/post/502e77b9/image-20221108145650940.png" alt="image-20221108145650940"></p><h3 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h3><p><img src="/post/502e77b9/image-20221108145952075.png" alt="image-20221108145952075"></p><p>对于一个任意的图结构$G(V,E)$，可以得</p><p>更新函数：<br>$$<br>\begin{align}<br>\pmb m_t ^{(v)} &#x3D; \sum_{u\in N(v)} f(\pmb h_{t-1} ^{(v)},\pmb h_{t-1} ^ {(v)}, \pmb e^{(u,v)}) \\\\<br>\pmb h_t ^{(v)} &#x3D; g(\pmb h_{t-1} ^{(v)},\pmb m_t ^{(v)})<br>\end{align}<br>$$<br>读出函数<br>$$<br>\pmb y_t &#x3D; g({h_T ^{(v)} | v \in \mathcal V})<br>$$</p><h2 id="其他应用"><a href="#其他应用" class="headerlink" title="其他应用"></a>其他应用</h2><p>语言生成模型</p><p><img src="/post/502e77b9/image-20221108151013889.png" alt="image-20221108151013889"></p><p>序列到序列的机器翻译</p><p><img src="/post/502e77b9/image-20221108151057898.png" alt="image-20221108151057898"></p><p>对话系统</p><p><img src="/post/502e77b9/image-20221108151123921.png" alt="image-20221108151123921"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>引入记忆环节，提高时序的利用效率</li><li>图灵完备，可拟合任意一个程序单元</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>长程依赖问题$\rightarrow GRU&#x2F;LSTM$</li><li>记忆容量问题$\rightarrow 遗忘门$</li><li>并行能力，时间依赖</li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python课程作业四——maillog分析</title>
      <link href="/post/2d2ab2fb.html"/>
      <url>/post/2d2ab2fb.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h1><p>分析附件中的邮件日志maillog，把id、状态、IP地址和目标邮箱提取出来。并统计出现错误最多的是那种邮箱。</p><p><a href="/download/maillog.txt">点击下载：maillog.txt</a></p><h1 id="代码与注释"><a href="#代码与注释" class="headerlink" title="代码与注释"></a>代码与注释</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">出现错误最多的邮箱有:</span><br><span class="hljs-string"> [&#x27;online.sh.cn&#x27;]</span><br><span class="hljs-string">其出错次数为:</span><br><span class="hljs-string"> 6</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> re<br><span class="hljs-comment">#正则表达式</span><br>pat_delivery = <span class="hljs-string">&quot;delivery \d&#123;0,&#125;&quot;</span><br>pat_id = <span class="hljs-string">&quot;\d+&quot;</span><br>pat_start = <span class="hljs-string">&quot;starting&quot;</span><br>pat_mail = <span class="hljs-string">&quot;\w+@\S+\.(com|cn)&quot;</span><br>pat_status = <span class="hljs-string">&quot;(success|deferral|failure)&quot;</span><br>pat_ip= <span class="hljs-string">&quot;\d&#123;1,3&#125;\.\d&#123;1,3&#125;\.\d&#123;1,3&#125;\.\d&#123;1,3&#125;&quot;</span><br><span class="hljs-comment">#记录空白信息</span><br>mail_error = <span class="hljs-string">&quot;mail not found&quot;</span><br>ip_error = <span class="hljs-string">&quot;ip not found&quot;</span><br><span class="hljs-comment">#暂存邮箱地址的dict</span><br>id_mail_dict = &#123;&#125;<br><span class="hljs-comment">#存储所有结果的list</span><br>info_list = []<br><span class="hljs-comment">#统计错误邮箱的dict</span><br>error_count = &#123;&#125;<br><span class="hljs-comment">#统计错误邮箱数最大的list</span><br>error_max_list = []<br><span class="hljs-keyword">try</span>:<br>    maillog = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;maillog.txt&quot;</span>,<span class="hljs-string">&quot;r&quot;</span>)<br>    <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> maillog:<br>        <span class="hljs-comment">#通过delivery的匹配判断是否是需要的语句</span><br>        ret = re.search(pat_delivery,each)<br>        <span class="hljs-keyword">if</span> ret <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment">#判断是否是请求发出的日志</span><br>            isStart = re.search(pat_start,each)<br>            <span class="hljs-built_in">id</span> = re.search(pat_id, <span class="hljs-built_in">str</span>(ret.group())).group()<br>            <span class="hljs-comment">#如果是请求发出日志，截取id和mail</span><br>            <span class="hljs-keyword">if</span> isStart <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                mail = re.search(pat_mail,each).group()<br>                id_mail_dict[<span class="hljs-built_in">id</span>] = mail<br>            <span class="hljs-comment">#否则，截取status和ip</span><br>            <span class="hljs-keyword">else</span>:<br>                status = re.search(pat_status,each).group()<br>                ip = re.search(pat_ip,each)<br>                <span class="hljs-comment">#try_catch用来判断存mail的环节中是否有当前的id，如果没有，记录&quot;mail not found&quot;</span><br>                <span class="hljs-keyword">try</span>:<br>                    <span class="hljs-comment">#if-else判断ip是否读取成功，如果没有，记录&quot;ip not found&quot;</span><br>                    <span class="hljs-keyword">if</span> ip <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                        ip = ip_error<br>                    <span class="hljs-keyword">else</span>: ip = ip.group()<br>                    <span class="hljs-comment">#存储一组信息到list中，格式:(id,mail,status,ip)</span><br>                    info_list.append((<span class="hljs-built_in">id</span>,id_mail_dict[<span class="hljs-built_in">id</span>],status,ip))<br>                <span class="hljs-keyword">except</span>:<br>                    info_list.append((<span class="hljs-built_in">id</span>,mail_error,status,ip))<br>    <span class="hljs-comment">#将读取到的结果存储到文件中</span><br>    <span class="hljs-keyword">try</span>:<br>        info_file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;result.txt&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>)<br>        <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> info_list:<br>            <span class="hljs-built_in">id</span>,mail,status,ip = each[<span class="hljs-number">0</span>],each[<span class="hljs-number">1</span>],each[<span class="hljs-number">2</span>],each[<span class="hljs-number">3</span>]<br>            <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;id:&quot;</span>+<span class="hljs-built_in">id</span>+<span class="hljs-string">&quot;\tmail:&quot;</span>+mail+<span class="hljs-string">&quot;\tstatus:&quot;</span>+status+<span class="hljs-string">&quot;\tip:&quot;</span>+ip<br>            <span class="hljs-comment"># print(each)</span><br>            info_file.write(<span class="hljs-built_in">str</span>+<span class="hljs-string">&quot;\n&quot;</span>)<br>            <span class="hljs-comment">#统计出现错误最多的邮箱，其中deferral与failure均作为错误处理</span><br>            <span class="hljs-keyword">if</span> status == <span class="hljs-string">&quot;deferral&quot;</span> <span class="hljs-keyword">or</span> status == <span class="hljs-string">&quot;failure&quot;</span>:<br>                <span class="hljs-keyword">if</span> mail != mail_error:<br>                    mail_kind = mail.split(<span class="hljs-string">&quot;@&quot;</span>)[<span class="hljs-number">1</span>]<br>                    <span class="hljs-keyword">if</span> mail_kind <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> error_count.keys():<br>                        error_count[mail_kind] = <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">else</span>:<br>                        error_count[mail_kind] += <span class="hljs-number">1</span><br>        <span class="hljs-comment">#因为错误数量最大的邮箱可能有多个，所以选择排序后判断的方法</span><br>        <span class="hljs-comment">#按值排序</span><br>        sorted_count = <span class="hljs-built_in">sorted</span>(error_count.items(),key= <span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>],reverse=<span class="hljs-literal">True</span>)<br>        max_count = sorted_count[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]<br>        <span class="hljs-built_in">print</span>(sorted_count)<br>        <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> sorted_count:<br>            <span class="hljs-keyword">if</span> each[<span class="hljs-number">1</span>] == max_count:<br>                error_max_list.append(each[<span class="hljs-number">0</span>])<br>            <span class="hljs-keyword">else</span>: <span class="hljs-keyword">break</span><br>        <span class="hljs-comment">#输出最多错误的邮箱种类</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;出现错误最多的邮箱有:\n&quot;</span>,error_max_list)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;其出错次数为:\n&quot;</span>,max_count)<br><br>    <span class="hljs-keyword">except</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件创建有误，请检查您的操作是否正确&quot;</span>)<br>    maillog.close()<br><span class="hljs-keyword">except</span> FileNotFoundError:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件不存在，请检查您的日志文件是否有效&quot;</span>)<br><br><br></code></pre></td></tr></table></figure><h1 id="结果文件"><a href="#结果文件" class="headerlink" title="结果文件"></a>结果文件</h1><p><a href="/download/result.txt">点击下载：result.txt</a></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络笔记</title>
      <link href="/post/9c21e22e.html"/>
      <url>/post/9c21e22e.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="《神经网络与机器学习》第五章笔记"><a href="#《神经网络与机器学习》第五章笔记" class="headerlink" title="《神经网络与机器学习》第五章笔记"></a>《神经网络与机器学习》第五章笔记</h1><p><a href="echarts.min.js">点击下载</a></p><h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><ol><li>全连接神经网络的权重矩阵参数非常多，容易受到干扰。</li><li>自然图像中的物体都具有局部不变性特征。</li></ol><h2 id="卷积神经网络的特性"><a href="#卷积神经网络的特性" class="headerlink" title="卷积神经网络的特性"></a>卷积神经网络的特性</h2><ol><li>局部连接</li><li>权重共享（卷积核）</li><li>空间或时间上的次采样</li></ol><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>卷积经常用在信号处理的过程中，用于计算信号的<b>延迟累积</b></p><p>假设信号发生器每个时刻$t$产生一个信号$x_t$，信号的衰减系数分别为$[w_1,w_2,…,w_K]$，则有收到的信号$y_t$为<br>$$<br>y_t &#x3D; \displaystyle \sum_{k&#x3D;1} ^K w_k x_{t-k+1}<br>$$<br>注意是，衰减系数$w$和信号$x$的下标是逆序的，这样，通过$K$个衰减系数可知，下一层的参数只与上一层的$K$个参数相关，这个过程大大降低了参数的量。</p><p><img src="/post/9c21e22e/image-20221028160049793.png" alt="image-20221028160049793"></p><h3 id="特定卷积核-x2F-特定滤波器"><a href="#特定卷积核-x2F-特定滤波器" class="headerlink" title="特定卷积核&#x2F;特定滤波器"></a>特定卷积核&#x2F;特定滤波器</h3><p>在这个问题中，衰减系数$w$又可以叫做<b>卷积核或滤波器</b>，其中，对于滤波器的不同值，其也有不同的作用</p><p>滤波器和作用的对应关系如下：</p><ul><li><p>$w &#x3D; [{1\over 2}, 0, -{1 \over 2}]$，近似于信号序列的一阶微分。</p><p>$x^ \prime (t) &#x3D; {x(t+1) - x(t-1) \over 2}$</p></li><li><p>$w &#x3D; [1,-2,1]$，近似于信号序列的二阶微分，二阶微分及更为高阶的微分可以用于提取高频信息（高频变化信号）。</p><p>$x^{\prime \prime}(t)&#x3D;{x^ \prime(t+1) - x ^\prime (t-1) \over 2}$</p></li><li><p>$w&#x3D;[{1\over 3},{1\over 3},{1\over 3}]$，均值信号，可以用于提取信号的低频信息（低频变化信号）。</p></li></ul><h3 id="步长S与零填充P"><a href="#步长S与零填充P" class="headerlink" title="步长S与零填充P"></a>步长S与零填充P</h3><p>进一步的，引入滤波器的滑动步长$S$和零填充$P$。</p><p><img src="/post/9c21e22e/image-20221028163459422.png" alt="image-20221028163459422"></p><h3 id="卷积类型（K为卷积核size）"><a href="#卷积类型（K为卷积核size）" class="headerlink" title="卷积类型（K为卷积核size）"></a>卷积类型（K为卷积核size）</h3><ol><li>窄卷积：步长$S&#x3D;1$，零填充$P&#x3D;0$，卷积后输出长度$L&#x3D;M-K+1$</li><li>宽卷积：步长$S&#x3D;1$，零填充$P &#x3D; K-1$，卷积后输出长度$L&#x3D;M-K+1+2\times(K-1) &#x3D; M +K -1$</li><li>等宽卷积：步长$S&#x3D;1$，零填充$P &#x3D; (K-1)&#x2F;2$，卷积后输出长度$L &#x3D; M$</li></ol><p>早期的文献中，卷积一般默认为窄卷积；目前的文献中，卷积一般都认为是等宽卷积</p><h3 id="互相关"><a href="#互相关" class="headerlink" title="互相关"></a>互相关</h3><p>计算卷积的过程中，由于下标逆序，因此需要进行卷积核翻转，但卷积操作目标的本质是提取特征，这个过程与卷积核是否翻转没有关系，因此可以直接使用互相关代替卷积核<br>$$<br>y_{ij} &#x3D; \sum_{u&#x3D;1} ^m \sum_ {v&#x3D;1} ^n w_ {uv} ·x_ {i+u-1,j+v-1}<br>$$</p><h3 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h3><p><img src="/20200316110210841.gif" alt="img"></p><h3 id="多个卷积核"><a href="#多个卷积核" class="headerlink" title="多个卷积核"></a>多个卷积核</h3><p><img src="/post/9c21e22e/image-20221028205612740.png" alt="image-20221028205612740"></p><p>多个卷积核可以作为平行的单位进行卷积，每个卷积核可以卷积多次，因此输入和输出的第三维度可以有所不同。<br>$$<br>\begin{align}<br>\pmb Z^p &amp; &#x3D; \pmb W^p \otimes \pmb  X + b^p &#x3D; \sum_{d&#x3D;1} ^D \pmb W^{p,d} \otimes \pmb X^d + b^p, \\\<br>Y^p &amp; &#x3D; f(\pmb Z^p)<br>\end{align}<br>$$<br><img src="/20200316110211268.gif" alt="img"></p><h2 id="池化-x2F-汇聚"><a href="#池化-x2F-汇聚" class="headerlink" title="池化&#x2F;汇聚"></a>池化&#x2F;汇聚</h2><p>卷积层虽然可以显著减少连接的个数，但每个特征映射的神经元个数并没有显著减少，为了进一步减少卷积后的特征维度，可以采用池化的方法，通过一定策略进行缩小。<img src="/post/9c21e22e/image-20221028213224959.png" alt="image-20221028213224959"></p><h2 id="卷积网络的结构"><a href="#卷积网络的结构" class="headerlink" title="卷积网络的结构"></a>卷积网络的结构</h2><h3 id="典型结构"><a href="#典型结构" class="headerlink" title="典型结构"></a>典型结构</h3><p><img src="/post/9c21e22e/image-20221028215631823.png" alt="image-20221028215631823"></p><p>池化层&#x2F;汇聚层其实可以看作是一种stride为K，卷积策略特殊的卷积层；一个卷积块为连续的$M$个卷积层和$b$个汇聚层（M通常设置为2-5，b为0或1）。一个卷积网络中可以堆叠$N$个连续的卷积块，然后再接着$K$个全连接层（$N$的取值区间比较大，比如1-100或者更大；$K$一般为0~2）。</p><h2 id="其他卷积种类"><a href="#其他卷积种类" class="headerlink" title="其他卷积种类"></a>其他卷积种类</h2><h3 id="转置卷积-x2F-微步卷积"><a href="#转置卷积-x2F-微步卷积" class="headerlink" title="转置卷积&#x2F;微步卷积"></a>转置卷积&#x2F;微步卷积</h3><p>中间补充空洞（0），之前只需要走一步现在需要走两步，因此可以看作$stride &#x3D; {1\over2}$</p><p><img src="/post/9c21e22e/cnn-no_padding_strides_transposed.gif" alt="查看源图像"></p><h3 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h3><p>空洞卷积可以通过给卷积核插入”空洞“来变相增加其大小，从而提高输出单元的感受野</p><p><img src="/post/9c21e22e/v2-4959201e816888c6648f2e78cccfd253_b.gif" alt="动图"></p><h2 id="典型卷积网络"><a href="#典型卷积网络" class="headerlink" title="典型卷积网络"></a>典型卷积网络</h2><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p>最早的成功在业界进行应用的神经网络模型，用于手写数字识别，共有七层。</p><p><img src="/post/9c21e22e/image-20221028222742829.png" alt="image-20221028222742829"></p><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>2012 ILSVRC winner</p><p>第一个现代深度卷积网络模型，采用了五个卷积层、三个池化层和两个全连接层，分成两路来降低显卡的显存要求。</p><p><img src="/post/9c21e22e/image-20221028224724159.png" alt="image-20221028224724159"></p><h3 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h3><p>2014 ILSVRC winner</p><p>在Inception网络中，一个卷积层包含多个不同大小的卷积操作，称为Inception模块，并将得到的特征映射在深度上拼接起来作为输出特征映射。</p><p><img src="/post/9c21e22e/image-20221028225144744.png" alt="image-20221028225144744"></p><p><img src="/post/9c21e22e/image-20221028225157574.png" alt="image-20221028225157574"></p><p>在Inception v3中，用了多层小卷积核代替了大卷积核，进一步降低了计算量和参数量</p><h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>残差网络：是通过给非线性的卷积层增加直连边的方式来提高信息的传播效率。假设在一个深度网络中，我们期望一个非线性单元$f(x;\theta)$去逼近一个目标函数为$h(x)$，可以将目标函数拆成恒等函数（不可逼近）和残差函数（可逼近）两个部分。<br>$$<br>h(\pmb x) &#x3D; \pmb x+ (h(\pmb x)-\pmb x) \approx \pmb x+f(\pmb x;\theta)<br>$$<br>其中，直连边可以直接传递恒等函数，即$\pmb x$的部分，由中间的卷积部分去逼近残差函数的部分。</p><p><img src="/post/9c21e22e/image-20221028230918825.png" alt="image-20221028230918825"></p><p>对于含残差的函数，可以得到<br>$$<br>h ^\prime(x) &#x3D; 1+{\partial f(\pmb x;\theta) \over \partial \pmb x}<br>$$<br>导数中有一个常数1，因此导数值无论如何不会过低，也降低了出现梯度消失的可能性。</p><h3 id="文本序列卷积"><a href="#文本序列卷积" class="headerlink" title="文本序列卷积"></a>文本序列卷积</h3><p>以部分的单词编码为基础，卷积核不一定保持size一致，需要在池化层通过采用时间维度pooling等手段将句子卷积结果合并到一起，形成合理的维度结构。</p><p><img src="/post/9c21e22e/image-20221028231335599.png" alt="image-20221028231335599"></p><p><img src="/post/9c21e22e/image-20221028231657503.png" alt="image-20221028231657503"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python课程作业三——文件读取时间</title>
      <link href="/post/e87a5ae2.html"/>
      <url>/post/e87a5ae2.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h1><p>下载 参考资料–》课上代码–〉make_sentence.py，该文件的功能是可以生成一个句子，把该文件作为一个模块引入到你的程序中。</p><p>程序功能如下：</p><p>1、调用make_sentence.py模块，把生成的句子写到一个文本文件中，写100000000次，得到大约2G的一个文件</p><p>2、分别通过以下三种方式访问生成的文件，要求从文件中统计cat出现的次数。</p><p>  1）磁盘文件检索方式</p><p>  2）按行读取文件，一行一行读入内存处理</p><p>  3）一次性读入内存处理</p><p>3、对上面的三种形式通过装饰器计算每种方式的运行时间。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> makesentence <span class="hljs-keyword">as</span> ms<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>():<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot; &quot;</span>.join(ms.sentence())<br><span class="hljs-comment">#装饰器</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">timer</span>(<span class="hljs-params">func</span>):<br>    <span class="hljs-comment"># 传入了参数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">count_time</span>(<span class="hljs-params">*args</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件开始读取，计时开始&quot;</span>)<br>        start = time.time()<br>        <span class="hljs-comment"># 调用原函数</span><br>        func(*args)<br>        end = time.time()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件结束读取，计时结束&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;函数 &#123;name&#125; 的用时为 &#123;time:.6f&#125; 秒(s)&quot;</span>.<span class="hljs-built_in">format</span>(name = func.__name__,time = end-start))<br>    <span class="hljs-keyword">return</span> count_time<br><br><span class="hljs-comment">#以1e8次生成句子</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_sentences</span>(<span class="hljs-params">name</span>):<br>    file = <span class="hljs-built_in">open</span>(name,<span class="hljs-string">&#x27;w&#x27;</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-built_in">int</span>(<span class="hljs-number">1e8</span>)):<br>        <span class="hljs-built_in">print</span>(i)<br>        file.write(generate()+<span class="hljs-string">&#x27;\n&#x27;</span>)<br>    file.close()<br><span class="hljs-comment">#生成样例</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_samples</span>(<span class="hljs-params">num,name,tmp_name</span>):<br>    new_file = <span class="hljs-built_in">open</span>(tmp_name,<span class="hljs-string">&#x27;w&#x27;</span>)<br>    file = <span class="hljs-built_in">open</span>(name,<span class="hljs-string">&#x27;r&#x27;</span>)<br>    time = <span class="hljs-number">0</span><br>    <span class="hljs-comment"># 从源文件中读取样例</span><br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file:<br>        <span class="hljs-keyword">if</span> time&lt;num :<br>            new_file.write(line)<br>            time+=<span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span> :<span class="hljs-keyword">break</span><br>    new_file.close()<br>    file.close()<br><br><span class="hljs-meta">@timer</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_cat_os</span>(<span class="hljs-params">name</span>):<br>    cat = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">try</span>:<br>        file = <span class="hljs-built_in">open</span>(name,<span class="hljs-string">&#x27;r&#x27;</span>)<br>        <span class="hljs-comment"># 直接从file中检索，磁盘文件检索方法</span><br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file:<br>            words = line.split(<span class="hljs-string">&quot; &quot;</span>)<br>            <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;cat&quot;</span> <span class="hljs-keyword">in</span> word:cat+=<span class="hljs-number">1</span><br>        file.close()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件中共有：\t&quot;</span>, cat, <span class="hljs-string">&quot;个cat&quot;</span>)<br>    <span class="hljs-comment"># 对FileNotFoundError单独处理</span><br>    <span class="hljs-keyword">except</span> FileNotFoundError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件名错误，请重新输入&quot;</span>)<br>    <span class="hljs-keyword">except</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;出现异常，请重试&quot;</span>)<br><br><span class="hljs-meta">@timer</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_cat_line</span>(<span class="hljs-params">name</span>):<br>    cat = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">try</span>:<br>        file = <span class="hljs-built_in">open</span>(name,<span class="hljs-string">&#x27;r&#x27;</span>)<br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            <span class="hljs-comment"># 读取每行</span><br>            line = file.readline()<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> line: <span class="hljs-keyword">break</span><br>            words = line.split(<span class="hljs-string">&quot; &quot;</span>)<br>            <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;cat&quot;</span> <span class="hljs-keyword">in</span> word: cat += <span class="hljs-number">1</span><br>        file.close()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件中共有：\t&quot;</span>, cat, <span class="hljs-string">&quot;个cat&quot;</span>)<br>    <span class="hljs-keyword">except</span> FileNotFoundError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件名错误，请重新输入&quot;</span>)<br>    <span class="hljs-keyword">except</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;出现异常，请重试&quot;</span>)<br><br><span class="hljs-comment"># @timer</span><br><span class="hljs-comment"># def count_cat_3(name):</span><br><span class="hljs-comment">#     cat = 0</span><br><span class="hljs-comment">#     try:</span><br><span class="hljs-comment">#         file = open(name,&#x27;r&#x27;)</span><br><span class="hljs-comment">#         chars = file.read()</span><br><span class="hljs-comment">#         words = chars.split()</span><br><span class="hljs-comment">#         for each in words:</span><br><span class="hljs-comment">#             if &quot;cat&quot; in each: cat += 1</span><br><span class="hljs-comment">#         file.close()</span><br><span class="hljs-comment">#         print(&quot;文件中共有：\t&quot;, cat, &quot;个cat&quot;)</span><br><span class="hljs-comment">#     except FileNotFoundError:</span><br><span class="hljs-comment">#         print(&quot;文件名错误，请重新输入&quot;)</span><br><br><span class="hljs-comment"># 按字符处理</span><br><span class="hljs-meta">@timer</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_cat_mem</span>(<span class="hljs-params">name</span>):<br>    cat = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">try</span>:<br>        file = <span class="hljs-built_in">open</span>(name,<span class="hljs-string">&#x27;r&#x27;</span>)<br>        <span class="hljs-comment"># 一次性读到内存中</span><br>        chars = file.read()<br>        word = <span class="hljs-string">&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> chars:<br>            <span class="hljs-keyword">if</span> each == <span class="hljs-string">&#x27; &#x27;</span> <span class="hljs-keyword">or</span> each == <span class="hljs-string">&#x27;\n&#x27;</span>:<br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;cat&quot;</span> <span class="hljs-keyword">in</span> word: cat += <span class="hljs-number">1</span><br>                word = <span class="hljs-string">&quot;&quot;</span><br>            <span class="hljs-keyword">else</span> :word += each<br>        file.close()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件中共有：\t&quot;</span>, cat, <span class="hljs-string">&quot;个cat&quot;</span>)<br>    <span class="hljs-keyword">except</span> FileNotFoundError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;文件名错误，请重新输入&quot;</span>)<br>    <span class="hljs-comment"># 由于需要一次性读到内存，对内存有要求，因此设置MemoryError处理</span><br>    <span class="hljs-keyword">except</span> MemoryError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;内存溢出！请选择其他方法或扩大文件存储&quot;</span>)<br>    <span class="hljs-keyword">except</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;出现异常，请重试&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment"># 生成句子</span><br>    <span class="hljs-comment"># get_sentences(&quot;sentences.txt&quot;)</span><br>    <span class="hljs-comment"># 生成sample用来测试</span><br>    <span class="hljs-comment"># generate_samples(10000,&quot;sentences.txt&quot;,&quot;s.txt&quot;)</span><br>    count_cat_os(<span class="hljs-string">&quot;sentences.txt&quot;</span>)<br>    count_cat_line(<span class="hljs-string">&quot;sentences.txt&quot;</span>)<br>    count_cat_mem(<span class="hljs-string">&quot;sentences.txt&quot;</span>)<br><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    文件开始读取，计时开始</span><br><span class="hljs-string">    文件中共有： 48485821 个cat</span><br><span class="hljs-string">    文件结束读取，计时结束</span><br><span class="hljs-string">    函数 count_cat_1 的用时为 68.235201 秒(s)</span><br><span class="hljs-string">    文件开始读取，计时开始</span><br><span class="hljs-string">    文件中共有： 48485821 个cat</span><br><span class="hljs-string">    文件结束读取，计时结束</span><br><span class="hljs-string">    函数 count_cat_2 的用时为 80.687801 秒(s)</span><br><span class="hljs-string">    文件开始读取，计时开始</span><br><span class="hljs-string">    文件中共有： 48485821 个cat</span><br><span class="hljs-string">    文件结束读取，计时结束</span><br><span class="hljs-string">    函数 count_cat_3 的用时为 229.200805 秒(s)</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="附：makesentence-py"><a href="#附：makesentence-py" class="headerlink" title="附：makesentence.py"></a>附：makesentence.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#### 生成复合英文语法的单词序列</span><br><br><span class="hljs-keyword">from</span> random <span class="hljs-keyword">import</span> choice, seed, randrange<br><br>articles = (<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;the&quot;</span>,<span class="hljs-string">&quot;1&quot;</span>,<span class="hljs-string">&quot;2&quot;</span>,<span class="hljs-string">&quot;3&quot;</span>,<span class="hljs-string">&quot;4&quot;</span>,<span class="hljs-string">&quot;5&quot;</span>,<span class="hljs-string">&quot;6&quot;</span>,<span class="hljs-string">&quot;7&quot;</span>,<span class="hljs-string">&quot;8&quot;</span>,<span class="hljs-string">&quot;9&quot;</span>,<span class="hljs-string">&quot;10&quot;</span>)<br>nouns = (<span class="hljs-string">&quot;cat&quot;</span>, <span class="hljs-string">&quot;dog&quot;</span>, <span class="hljs-string">&quot;sheep&quot;</span>, <span class="hljs-string">&quot;rabbit&quot;</span>, <span class="hljs-string">&quot;tiger&quot;</span>, <span class="hljs-string">&quot;chicken&quot;</span>,<br>         <span class="hljs-string">&quot;fish&quot;</span>, <span class="hljs-string">&quot;grass&quot;</span>, <span class="hljs-string">&quot;seed&quot;</span>, <span class="hljs-string">&quot;carrot&quot;</span>, <span class="hljs-string">&quot;apple&quot;</span>)<br>verbs = (<span class="hljs-string">&quot;eats&quot;</span>, <span class="hljs-string">&quot;catches&quot;</span>, <span class="hljs-string">&quot;finds&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sentence</span>():<br>    <span class="hljs-keyword">return</span> noun_phrase() + verb_phrase()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">noun_phrase</span>():<br>    <span class="hljs-keyword">return</span> [choice(articles), choice(nouns)]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">verb_phrase</span>():<br>    vp = [choice(verbs)]<br>    <span class="hljs-keyword">if</span> randrange(<span class="hljs-number">3</span>) &gt; <span class="hljs-number">0</span>:<br>        vp.extend(noun_phrase())<br>    <span class="hljs-keyword">return</span> vp<br>    <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    seed()<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot; &quot;</span>.join(sentence()))<br><br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>前馈神经网络笔记</title>
      <link href="/post/f014be16.html"/>
      <url>/post/f014be16.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="《神经网络与机器学习》第四章笔记"><a href="#《神经网络与机器学习》第四章笔记" class="headerlink" title="《神经网络与机器学习》第四章笔记"></a>《神经网络与机器学习》第四章笔记</h1><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ul><li>连续可导（允许少数点不可导）的非线性函数</li><li>尽可能简单</li><li>导函数的值域在合理的范围内（防止梯度消失与梯度爆炸）</li></ul><h3 id="常见激活函数"><a href="#常见激活函数" class="headerlink" title="常见激活函数"></a>常见激活函数</h3><p>1、$ \sigma(x) &#x3D; {1 \over {1+exp(-x)}} \rightarrow tanh(x) &#x3D; { {exp(x)-exp(-x)} \over {exp(x)+exp(-p)}} $</p><p> 饱和函数，$tanh(x)$零中心化，$logistic$函数恒大于零。非零中心化的输出会使之后的输入发生偏置偏移，并进一步使得梯度下降的收敛速度变慢。</p><p>2、$ReLU(x)&#x3D;\begin{cases} x &amp; x \ge 0  \\\\ 0 &amp; x \lt 0 \end{cases}$</p><p>$\rightarrow LeakyReLU(x)&#x3D;\begin{cases} x &amp; x \gt 0  \\\\ \gamma x  &amp; x \le 0 \end{cases}$</p><p>$\rightarrow ELU(x) &#x3D; \begin{cases} x &amp; x \gt 0  \\\\ \gamma (exp(x)-1)  &amp; x \le 0 \end{cases} &#x3D; max(0,x)+max(0,\gamma (exp(x)-1))$</p><p>$softplus(x) &#x3D; log(1+exp(x))$</p><p><img src="/post/f014be16/image-20221026212203494.png" alt="image-20221026212203494"></p><p>以上激活函数左侧不激活，右侧随信号提升而增大，在生物学上有着更高的合理性，因此目前使用$ReLU$系列激活函数的有很多</p><p>3、$Swish(x) &#x3D; x\sigma(\beta x)$</p><p>一种软门控的激活函数，结合了第一种斜坡函数与第二种$sigmoid$函数，左半部分$x$作为斜坡函数，是主信号；右半部分$\sigma(\beta x)$是控制门，从[0,1]进行变化，等于0时信号完全关闭，等于1时信号放开。$Swish$函数的变化范围会随着$\beta$的变化而有差异。</p><p><img src="/post/f014be16/image-20221026212115181.png" alt="image-20221026212115181"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/post/f014be16/image-20221026212326566.png" alt="image-20221026212326566"></p><h2 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h2><h3 id="三要素"><a href="#三要素" class="headerlink" title="三要素"></a>三要素</h3><p>神经元的激活规则、网络的拓扑结构、学习算法</p><h2 id="前馈神经网络（全连接神经网络、多层感知机）"><a href="#前馈神经网络（全连接神经网络、多层感知机）" class="headerlink" title="前馈神经网络（全连接神经网络、多层感知机）"></a>前馈神经网络（全连接神经网络、多层感知机）</h2><p>各神经元分别属于不同的层，层内无连接，相邻两层之间的神经元全部两两连接，信号从输入层传播到输出层单向传播，无循环。</p><p><img src="/post/f014be16/image-20221026213354130.png" alt="image-20221026213354130"></p><h3 id="矩阵微积分"><a href="#矩阵微积分" class="headerlink" title="矩阵微积分"></a>矩阵微积分</h3><p><b>分母布局</b></p><ul><li><p>标量关于向量的偏导数<br>$$<br>{\partial y\over \partial \pmb x} &#x3D; [{\partial y \over \partial x_1 },{\partial y \over \partial x_2 },…,{\partial y \over \partial x_p} ]<br>$$</p></li><li><p>向量关于向量的偏导数<br>$$<br>{\partial f(\pmb x) \over \partial \pmb x} &#x3D;<br>\left[<br>\begin{matrix}<br>{\partial y_1 \over \partial x_1 } &amp; … &amp; {\partial y_q \over \partial x_1 } \\<br>\vdots &amp; \vdots &amp; \vdots \\<br>{\partial y_1 \over \partial x_p } &amp; … &amp; {\partial y_q \over \partial x_p } \\<br>\end{matrix}<br>\right]<br>\in \mathbb R^{p\times q}<br>$$</p></li></ul><h3 id="传播公式"><a href="#传播公式" class="headerlink" title="传播公式"></a>传播公式</h3><p>$$<br>\begin{align}<br>\pmb z^{(l)} &amp; &#x3D; \pmb W^{(l)}\pmb a^{(l-1)}+\pmb b^{(l)} \\\<br>\pmb a^{(l)} &amp; &#x3D; f_l(\pmb z^{(l)})<br>\end{align}<br>$$</p><h3 id="前馈计算"><a href="#前馈计算" class="headerlink" title="前馈计算"></a>前馈计算</h3><p>$$<br>\pmb x &#x3D; \pmb a^{(0)} \rightarrow \pmb z^{(1)} \rightarrow \pmb a^{(1)} \rightarrow \pmb z^{(2)} \rightarrow … \rightarrow \pmb a^{(L-1)} \rightarrow \pmb z^{(L)} \rightarrow \pmb a^{(L)} &#x3D; \phi (\pmb x;\pmb W,\pmb b)<br>$$</p><h3 id="通用近似定理"><a href="#通用近似定理" class="headerlink" title="通用近似定理"></a>通用近似定理</h3><p>对于具有线性输出层和至少一个使用”挤压“性质的激活函数的隐藏层组成的前馈神经网络，<b>其可以以任意精度近似一个定义在实数空间的有限闭集函数</b>，换言之，前馈神经网络在近似函数时是万能的。</p><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>链式法则是在微积分中求复合函数的一种常见方法。</p><ol><li><p>若$x \in \mathbb R,\pmb u&#x3D;u(x) \in \mathbb R^s,\pmb g&#x3D;g(\pmb u)\in \mathbb R^t$，则<br>$$<br>{\partial \pmb g \over \partial x}&#x3D;{\partial \pmb u \over \partial x}{\partial \pmb g \over \partial \pmb u} \in \mathbb R^{1\times t}<br>$$</p></li><li><p>若$x \in \mathbb R^p,\pmb y&#x3D;g(\pmb x) \in \mathbb R^s,\pmb z&#x3D;f(\pmb y)\in \mathbb R^t$，则<br>$$<br>{\partial \pmb z \over \partial \pmb x}&#x3D;{\partial \pmb y \over \partial \pmb x}{\partial \pmb z \over \partial \pmb y} \in \mathbb R^{p \times t}<br>$$</p></li><li><p>若$X\in \mathbb R^{p\times q}$为矩阵，$\pmb y&#x3D;g(X) \in \mathbb R^s,z&#x3D;f(\pmb y)\in \mathbb R$，则<br>$$<br>{\partial z\over \partial X_{ij}} &#x3D; {\partial \pmb y \over {\partial X_{ij}}}{\partial z \over \partial \pmb y} \in \mathbb R<br>$$</p></li></ol><p>以前向传播公式为例<br>$$<br>\begin{align}<br>\pmb z^{(l)} &amp; &#x3D; \pmb W^{(l)}\pmb a^{(l-1)}+\pmb b^{(l)} \\\<br>\pmb a^{(l)} &amp; &#x3D; f_l(\pmb z^{(l)})<br>\end{align}<br>$$<br>对其中的参数$W$和$b$求导可得<br>$$<br>{\partial \mathcal L(y,\hat y) \over \partial w^{(l)}_ {ij} }&#x3D; {\partial \pmb z^{(l)} \over \partial w^{(l)}_ {ij}} {\partial \mathcal L (y, \hat y) \over \partial \pmb z^ {(l)} } \\\<br>{\partial \mathcal L(y,\hat y) \over \partial \pmb b^{(l)}_ {ij} }&#x3D; {\partial \pmb z^{(l)} \over \partial \pmb b^{(l)}_ {ij}} {\partial \mathcal L (y, \hat y) \over \partial \pmb z^ {(l)} }<br>$$<br>若需要实现对前向传播过程，需要对参数的求导结果进行计算，根据链式法则得，可以从后到前执行计算。进一步，我们对参数求导公式的部分结果进行推导。</p><p>对于${\partial \pmb z^{(l)} \over \partial w^{(l)}_ {ij}}$，向量中只有一项与$w_{ij} ^{(l)}$有关<br>$$<br>\begin{align}<br>{\partial \pmb z^{(l)} \over \partial w^{(l)}_ {ij}} &amp; &#x3D; [{\partial z_1 ^{(l)} \over \partial w^{(l)}_ {ij}}, … , {\partial z_i ^{(l)} \over \partial w^{(l)}_ {ij}}, …,  {\partial z_{m^{(l)}} ^{(l)} \over \partial w^{(l)}_ {ij}}] \\\<br>&amp; &#x3D; [\ 0, … ,{\partial (\pmb w_{i:} ^{(l)} \pmb a^{(l-1)})+b_i ^{(l)} \over \partial w_ {ij} ^{(l)}}, … , 0\ ]     \\\<br>&amp; &#x3D; [\ 0, … , a_j^{(l-1)}, … , 0\ ]  \\\<br>&amp; &#x3D; \mathbb I_i (a_j ^{(l-1)}) \in \mathbb R^ {m^ {(l)}}<br>\end{align}<br>$$<br>对于${\partial \pmb z^{(l)} \over \partial b^{(l)}}$，因为函数是一个线性函数，对于每一维的位置应该为1，因此结果应该为单位阵<br>$$<br>{\partial \pmb z^{(l)} \over \partial b^{(l)}} &#x3D; \pmb I_ {m^{(l)}} \in \mathbb R^ {m^{(l)} \times m^{(l)}}<br>$$<br>对于$\delta^{(l)} &#x3D; {\partial \mathcal L(\pmb {y,\hat y}) \over \partial \pmb z^{(l)}} \in \mathbb R^{m^{(l)}}$，根据<br>$$<br>\begin{align}<br>\pmb z^{(l)} &amp; &#x3D; \pmb W^{(l)}\pmb a^{(l-1)}+\pmb b^{(l)} \\\<br>\pmb a^{(l)} &amp; &#x3D; f_l(\pmb z^{(l)})<br>\end{align}<br>$$<br>有<br>$$<br>\begin{align}<br>\delta^{(l)} &amp;  &#x3D; {\partial \mathcal L(\pmb {y,\hat y}) \over \partial \pmb z^{(l)}} \\\<br>&amp; &#x3D; {\partial \pmb a^ {(l)} \over \partial \pmb z^{(l)}}·{\partial \pmb z^ {(l+1)} \over \partial \pmb a^{(l)}}·{\partial \mathcal L(\pmb {y,\hat y}) \over \partial \pmb z^{(l+1)}} \\\<br>&amp; &#x3D; diag(f_l ^\prime (\pmb z^{(l)})) · (\pmb W^{(l+1)})^\top  · \delta^{(l+1)} \\\<br>&amp; &#x3D; f_l ^\prime (\pmb z^{(l)}) \  \odot \ ((\pmb W^{(l+1)})^\top  \delta^{(l+1)})<br>\end{align}<br>$$<br>在上式的结果中我们可以看到，$\delta^{(l)}$的结果和$\delta^{(l+1)}$有关，以此类推；其他部分可以通过计算得到。反向传播算法的原理就在这里，是通过后面的导数值向前推，从$\delta^{(L)}$向前推到$\delta^{(1)}$，从而计算出整个网络的导数值。</p><p>因此，对于$W$和$b$的求解可以写成以下形式：<br>$$<br>{\partial \mathcal L(\pmb {y,\hat y}) \over \partial w_{ij} ^{(l)}} &#x3D; \mathbb I_i (a_j ^{(l-1)}) \delta^ {(l)} &#x3D; \delta_i ^{l} a_j ^{(l-1)} \\\\<br>{\partial \mathcal L(\pmb {y,\hat y}) \over \partial \pmb b^ {(l)}} &#x3D; \delta^{(l)}<br>$$<br>进一步的，$L(\pmb{y,\hat y})$关于第$l$层权重$W^{(l)}$的梯度为：<br>$$<br>{\partial \mathcal L(\pmb {y,\hat y}) \over \partial W ^{(l)}} &#x3D; \mathbb I_i (a_j ^{(l-1)}) \delta^ {(l)} &#x3D; \delta_i ^{l} (\pmb a ^{(l-1)})^\top<br>$$<br>反向传播算法的总体流程如下：</p><p><img src="/post/f014be16/image-20221028142809028.png" alt="image-20221028142809028"></p><h2 id="计算图与自动微分"><a href="#计算图与自动微分" class="headerlink" title="计算图与自动微分"></a>计算图与自动微分</h2><p>自动微分是利用链式法则来自动计算一个复合函数的梯度的方法。</p><p>如以下的计算图：</p><p><img src="/post/f014be16/image-20221028143025227.png" alt="image-20221028143025227"></p><p>对于以上的计算图，我们可以从左到右分步完成每一步计算的导数值，然后链式求解。</p><p><img src="/post/f014be16/image-20221028143144048.png" alt="image-20221028143144048"></p><p>当x&#x3D;1，w&#x3D;0，b&#x3D;0时，可得<br>$$<br>\begin{align}<br>{\partial f(x;w,b)\over \partial w }|_{x&#x3D;1,w&#x3D;0,b&#x3D;0} &amp; &#x3D; {\partial f(x;w,b)\over \partial h_6} {\partial h_6 \over \partial h_5}{\partial h_5 \over \partial h_4}{\partial h_4 \over \partial h_3}{\partial h_3 \over \partial h_2}{\partial h_2 \over \partial h_1}{\partial h_1 \over \partial w} \\\<br>&amp; &#x3D; 1 \times -0.25 \times 1 \times 1 \times -1 \times 1 \times 1 \\\<br>&amp; &#x3D; 0.25<br>\end{align}<br>$$<br>如果函数和参数之间有多条路径存在，可以将多条路径上的导数再进行相加，得到最终的梯度。</p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><ol><li><font color="red">前向计算</font>每层的状态和激活值，直到最后一层</li><li><font color="red">反向计算</font>每一层的参数的偏导数</li><li><font color="red">更新参数</font></li></ol><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>分为静态计算图和动态计算图，静态图在编译时构建，动态图在运行时构建。</p><h2 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h2><ol><li>参数过多，影响训练</li><li>非凸优化问题：即存在局部最优而非全局最优解，影响迭代</li><li>梯度消失问题：下层参数难调</li><li>参数解释问题</li></ol>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英语pre演讲稿</title>
      <link href="/post/cf0451ef.html"/>
      <url>/post/cf0451ef.html</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">  <script id="hbeData" type="hbeData" data-hmacdigest="0ca8e2fb93917ca770e8fd2fe6866068e0a28422851d2412c86f4251457b4d2e">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>10.20组会笔记——对抗深度学习</title>
      <link href="/post/bda5918e.html"/>
      <url>/post/bda5918e.html</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">  <script id="hbeData" type="hbeData" data-hmacdigest="1531f8d916577b18037c2e3345ad9af25e20fa5f74cf0d3e6434043849f21e1f">93ccda1b85d0057f7bfd7e490ef564c6e794c9db37789ee573592357757a6a86ccbfb0e274701bfa36245ee1642f8ee17892edc5ae04cd82825a7bf56d5039a621bbb3386b6053f4346e00c598033f68c84e94c22ea6cd1289da3c94e2ebfdf3a45a6f7d369146b58197027cee717e8f85201d6e238ee2c75fb4ba954b84e3e9eb9f863a4e3d97638de01c01c3ce7664eb481d69e55eee9d6b6271e8e3bebc9804221598222b57a3a047c72aca0258f02ec69bce2305db6236db3b87c80d6d6877ee6412501f0bb85bba40d916c745eeda89ca6d3916d8f58622543b41cbad141c6100763769ead4e31910d0c1ad823c167f67319e95a294d9173419b8c06ee81faf69dc4ba47cbb008fd73f57a3e5f92f23aae5ad0e669c933b0126a5b11635bb4e61e5aa57fb1aa2ac1445a323431e83ccb7a894a2e98a7a0ab2001d28e8024098351b9562c521a97ceaa7954edf47d3d842c3b7edb36cc2c0ffa213c10419aafd172758ca454f91521390d924279a321b928eaeb81a3e8c9d2cbb319225beb237e6ae281d0dd010d4003e9a75a6221340b7cc4a6f08ea7edc8c9beeedecf8f687b41cdbb6dbff5cd1edf523759704d2ac6ae4318f636a9a9445bd9137d4ce58e7c042dfc6829f25d9f5b07195236c9b96d81b6d0e8f022e797b93afefdb9764dfbb8ad745a2c9d159aa3c5b47012318d0e4212954ce6629539da9ed1a5fb7fd8dfc0f456e0b6f0b0544510e7674db0896e194c9dffbc5757aef0be27e849b8366035e252c8f7e3f1d9d1c25a2e3a2bd4e2e25672e5c3fc894818cabbcaf104a937baa11fa6137b3b751e067f52cf4e0ef34ab675a51b19dcfd230fa893b7683b61bfed427a9c6fa78ef894de021e6fedf35a6ff32928df07e4388486229d5014d91cd305089ddf722ac15d541df9a2f0fbc3cb80a996f6cecda653b6b0911a874b8337a70f8dbf58d92bc9af72db6a60589e007d6b42446d772ba641dbad558dd7a6649cdb7615e0ce189c6ab2f912226bd83037c0b2c5c36542ffb06adcae874d3f190cac60a77a2b6eb1dd04a5cab67cd5be0705e6c482a9d757932ffae79cb0477fd4b237a2138fd6ea0c1ae2149f455534adccbfc5116e5e4c25ee99aeb0a32025c7f438869a723eca43a6c5774acaf39b140e2f3d161e8c008f87c50cadc201f420fe0d923e5a2ec9a5d090cc2a752e705e2c540ac95af8f52c050b02214d7117ba505026773d2026517601b678e5952745e3b66121b84cff9f41a3dd441d86bd6870d5678f74e020dfb09ae5d69b992ee5e51beb751425ea608344f783b8cfaf9b7de3b3d97eb9000982682c17f7fe115a86ffbe7a5fe39d84fd5920457476a19986c61da130cdf82439abe</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 实验室工作 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 对抗网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性模型笔记</title>
      <link href="/post/2409b139.html"/>
      <url>/post/2409b139.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="《神经网络与机器学习》第三章笔记"><a href="#《神经网络与机器学习》第三章笔记" class="headerlink" title="《神经网络与机器学习》第三章笔记"></a>《神经网络与机器学习》第三章笔记</h1><h2 id="线性分类问题"><a href="#线性分类问题" class="headerlink" title="线性分类问题"></a>线性分类问题</h2><h3 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h3><p>$$<br>x^\in\mathbb R^D,y^\in{0,1}<br>$$</p><h3 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h3><p><img src="/post/2409b139/image-20221019232930891.png" alt="image-20221019232930891"></p><h2 id="信息论基础"><a href="#信息论基础" class="headerlink" title="信息论基础"></a>信息论基础</h2><h3 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h3><p>在信息论中，熵用来衡量一个随机事件的不确定性，自信息用于衡量一个随机事件所携带的信息量，一般来说概率越小，其携带的可能信息越多，即：<br>$$<br>I(x) &#x3D; -log(p(x))<br>$$<br>自信息本身具有可加性，而熵基于自信息，<b>是随机变量X的自信息的期望</b>，熵越高，其随机变量的信息越多。<br>$$<br>H(x)&#x3D;\mathbb E_X[I(x)]&#x3D;\mathbb E_X[-logp(x)] &#x3D; -\displaystyle\sum_{x \in \mathcal X}p(x)log(p(x))<br>$$<br>在对分布$q(y)$的符号进行编码时，其熵$H(q)$也是理论上的最优平均编码长度，这种编码方式称为<b>熵编码。</b></p><h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><p>交叉熵是按照概率分布$q$的最优编码对真实分布为$p$的信息进行编码的长度。<br>$$<br>H(p,q)&#x3D;\mathbb E_p[-logq(x)]&#x3D;-\displaystyle \sum_xp(x)logq(x)<br>$$<br>当$p&#x3D;q$时，交叉熵&#x3D;&#x3D;熵，而熵编码是最优的编码方式，即$H(p,q)$最小的方式，因此在给定了$q$的情况下，如果$p$和$q$越接近，交叉熵就会越小。</p><h3 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h3><p>KL散度使用概率分布$q$来近似$p$时所造成的信息损失量，是按照概率分布$q$的最优编码对真实分布为$p$的信息进行编码，其平均编码长度（即交叉熵）$H(p,q)$和$p$的最优平均编码长度（即熵）$H(p)$之间的差异。<br>$$<br>KL(p,q) &#x3D; H(p,q)-H(p)&#x3D;\displaystyle \sum_xp(x) log{p(x)\over q(x)}<br>$$</p><h3 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h3><p>$$<br>\begin{align}<br>D_ {KL}(p_r(y|x) || p_\theta(y|x)) &amp; &#x3D; \displaystyle \sum_{y&#x3D;0}^1 p_r(y|x)log{p_r(y|x) \over p\theta(y|x)}<br>\propto -\displaystyle \sum_{y&#x3D;0}^1p_ r(y|x)logp_\theta(y|x) \\\\<br>&amp; &#x3D; -I(y^*&#x3D;1)logp_\theta(y&#x3D;1|x)-I(y^*&#x3D;0)logp_\theta(y&#x3D;0|x) \\\\<br>&amp; &#x3D; -y^* logp_ \theta(y&#x3D;1|x) - (1- y^*)logp_ \theta (y&#x3D;0|x) \\\\<br>&amp; &#x3D; -logp_\theta(y^*|x)<br>\end{align}<br>$$</p><h2 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h2><p>首先确定一个观点：可以将分类问题看作是一种条件概率估计问题，因此可以引入一个非线性函数$g$来将线性函数的值域压缩到条件概率的(0,1)之间，来表示分类问题的概率，即：<br>$$<br>p(y&#x3D;1|\pmb x)&#x3D; g(f(\pmb x;\pmb w))<br>$$<br>为了能让$g$函数进一步可微分，我们采用logistic函数（以$\sigma(x)$表示）来代替原有的二分类的分段函数<br>$$<br>\sigma(x) &#x3D; {1 \over {1+exp(-x)}}<br>$$<br>在线性分类中$y&#x3D;\pmb w\top \pmb x$需要进入分类，将其看作条件概率，即：<br>$$<br>p_\theta(y&#x3D;1|\pmb x) &#x3D; \sigma(\pmb w\top\pmb x) &#x3D; {1 \over 1+exp(-\pmb w\top\pmb x)} \\<br>p_\theta(y&#x3D;0|\pmb x) &#x3D; 1-\sigma(\pmb w\top\pmb x) &#x3D; {exp(-\pmb w\top\pmb x) \over {1+exp(-\pmb w\top\pmb x)}}<br>$$<br>而对于一个样本$(x,y^*)$，其真实条件概率为<br>$$<br>p(y&#x3D;1|\pmb x) &#x3D; y^* \\<br>p(y&#x3D;0|\pmb x) &#x3D; 1-y^*<br>$$<br>对这两者的差异进行衡量可以采用上文所提到的交叉熵损失，即<br>$$<br>\begin{align}<br>KL (p_ r,p_ \theta) &#x3D; &amp; &#x3D; -I (y^*&#x3D;1)\ log\ p_ \theta (y&#x3D;1|x) - I(y^*&#x3D;0)\ log\ p_ \theta (y&#x3D;0|x)<br>\end{align}<br>$$<br>所以Logistic回归的损失函数可以写作：<br>$$<br>\mathcal R(\pmb w)&#x3D;-{1\over N}\displaystyle \displaystyle\sum_{i&#x3D;1}^N (y^{(i)}log(\sigma(\pmb w\top \pmb x^{(i)})) +(1-y^{(i)})log(1-\sigma(\pmb w\top \pmb x^{(i)})))<br>$$<br>令$\sigma(\pmb w\top\pmb x)&#x3D;\hat y$其在参数$\pmb w$上求梯度可得<br>$$<br>\begin{align}<br>{\partial\mathcal R(\pmb w) \over \partial\pmb w} &amp; &#x3D; -C_1 \ {1\over N}\displaystyle\sum_{i&#x3D;1}^N(y^{(i)}{1\over \hat y^{(i)}}\pmb x^{(i)} - (1-y^{(i)}){1\over {1-\hat y^{(i)}}}\pmb x^{(i)}) \\\\<br>&amp; &#x3D; -C_2 \ {1\over N}\displaystyle\sum_{i&#x3D;1}^N(y^{(i)}{ \hat y^{(i)} (1-\hat y^{(i)})\over \hat y^{(i)}}\pmb x^{(i)} - (1-y^{(i)}){\hat y^{(i)} (1-\hat y^{(i)})\over {1-\hat y^{(i)}}}\pmb x^{(i)}) \\\\<br>&amp; &#x3D; -C_2 \ {1\over N}\displaystyle\sum_{i&#x3D;1}^N(y^{(i)}(1-\hat y^{(i)})\pmb x^{(i)} - (1-y^{(i)})\hat y^{(i)}\pmb x^{(i)})\\\\<br>&amp; &#x3D; -C_2 \ {1\over N}\displaystyle\sum_{i&#x3D;1}^N(y^{(i)} - \hat y^{(i)})x^{(i)}<br>\end{align}<br>$$<br>因此，梯度下降的参数更新将从梯度的反方向进行。即$\pmb w_{t+1} \leftarrow \pmb w_{t+1} + \alpha {1\over N}\displaystyle\sum_{i&#x3D;1}^N(y^{(i)} - \hat y^{(i)})x^{(i)}$</p><h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h2><p>softmax从logistic回归出发，基于多分类问题进行，基于多分类问题的argmax法可得<br>$$<br>y&#x3D;\arg \max_{c&#x3D;1}^C f_c(\pmb x;\pmb w_c)<br>$$<br>利用softmax函数，可以得到$y&#x3D;c$的条件概率为<br>$$<br>P(y&#x3D;c|\pmb x)&#x3D;softmax(\pmb w_c^\top \pmb x) &#x3D; {exp(\pmb w_c^\top \pmb x) \over \displaystyle \sum_{i&#x3D;1}^Cexp(\pmb w_i^\top \pmb x)}<br>$$<br>其交叉熵损失函数依然是真实标签和条件概率的交叉熵，即<br>$$<br>Loss(\pmb y,f(\pmb x,\theta)) &#x3D; -\displaystyle \sum_{c&#x3D;1}^C y_c\ log f_c(\pmb x,\theta)<br>$$<br>梯度下降的方式和logistic很相似，是两个标签向量相减<br>$$<br>{\partial\mathcal R(\pmb w) \over \partial\pmb w} &#x3D; {1\over N}\displaystyle\sum_{i&#x3D;1}^N({\pmb y}^{(i)} - \hat {\pmb y}^{(i)})x^{(i)}<br>$$</p><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>感知机是一种<b>错误驱动的在线学习算法</b>，初始化$\pmb w$为全零向量，每次分错一个样本$(\pmb x,y)$时，即当$y\pmb w^\top\pmb x&lt;0$时，用这个样本来更新整体的权重$\pmb w \leftarrow \pmb w+y\pmb x$，直到所有样本分对。</p><p>根据感知机的学习策略，可以反推出，当样本分错时才会产生损失，即：<br>$$<br>\mathcal L(\pmb w;\pmb x,y) &#x3D; max(0,-y\pmb w^\top\pmb x)<br>$$<br><img src="/post/2409b139/image-20221021230419896.png" alt="image-20221021230419896"></p><p>当数据集是两类线性可分时，两类感知机更新权重的次数不超过一个常数$R^2\over r^2$，这个性质称之为感知机的收敛性。</p><p><img src="/post/2409b139/image-20221021230722922.png" alt="image-20221021230722922"></p><p>证明过程可以通过放缩来确定第K次更新感知器的权重向量时的上界和下界，这里不再推导。</p><h2 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h2><p>支持向量机是这样的一种模型，它的目的在于寻找一个超平面$(\pmb w^*,b^*)$，使得间隔最大，即选择间隔最大的决策边界。</p><p>首先是求最短边界$\gamma$的一般方法：<br><img src="/post/2409b139/image-20221021232304601.png" alt="image-20221021232304601"></p><p>那么，支持向量机的最大间隔的决策边界可以通过以下方式表示：<br>$$<br>\max_{w,b} \ \ \ \ \mathcal \gamma \\\\<br>s.t. \ \ \ \ {y^{n}(\pmb w^\top\pmb x^{n}+b) \over ||\pmb w||} \ge \gamma<br>$$<br>限制$||\pmb w||·\gamma &#x3D; 1$，上式可以转写为<br>$$<br>\min_{w,b} \ \ \ \ \mathcal  ||\pmb w|| \\\\<br>s.t. \ \ \ \ {y^{n}(\pmb w^\top\pmb x^{n}+b)} \ge 1, <br>$$<br>根据支持向量机的条件时，我们可以得知，获取决策边界时会以距离为$\pm$1为边界，判断其是否成功分类，因此就有了支持向量的概念。</p><p><img src="/post/2409b139/image-20221022193900627.png" alt="image-20221022193900627"></p><p>若训练集线性不可分，则SVM算法本身无法找到最优解，为了能够容忍部分不满足约束的样本，使SVM能够有更高的使用空间，引入松弛变量$\xi$</p><p><img src="/post/2409b139/image-20221022194400357.png" alt="image-20221022194400357"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><img src="/post/2409b139/image-20221022194718927.png" alt="image-20221022194718927"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mathjax使用tips</title>
      <link href="/post/2b8d08b.html"/>
      <url>/post/2b8d08b.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="mathjax使用中的一些tips"><a href="#mathjax使用中的一些tips" class="headerlink" title="mathjax使用中的一些tips"></a>mathjax使用中的一些tips</h1><h2 id="1、大括号"><a href="#1、大括号" class="headerlink" title="1、大括号"></a>1、大括号</h2><p>如：$\mathcal{F}&#x3D;\lbrace f(\boldsymbol{x};\theta)\mid\theta\in\mathbb{R}^{D} \rbrace$</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mathjax">\lbrace ... \rbrace<br></code></pre></td></tr></table></figure><h2 id="2、多行公式"><a href="#2、多行公式" class="headerlink" title="2、多行公式"></a>2、多行公式</h2><p>如：$cdf(x) &#x3D; \begin{cases}<br>P(X \le x) &amp; \text{if X是离散随机变量} \\[]<br>\int^x_ {-\infty} &amp; \text{if X是连续变量} \\<br>\end{cases}$</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs mathjax">\begin&#123;cases&#125;<br>...  &amp; \text&#123;&#125; \\\\[2ex]<br>...  &amp; \text&#123;&#125; \\\\<br>\end&#123;cases&#125;<br></code></pre></td></tr></table></figure><blockquote><p>注意每行公式后面的斜线，使用四个横线来防止hexo的渲染引擎将斜线转义错误</p></blockquote><h2 id="3、左对齐"><a href="#3、左对齐" class="headerlink" title="3、左对齐"></a>3、左对齐</h2><p>$$<br>\begin{align}<br>D_ {KL}(p_r(y|x) || p_\theta(y|x)) &amp; &#x3D; \displaystyle \sum_{y&#x3D;0}^1 p_r(y|x)log{p_r(y|x) \over p\theta(y|x)}<br>\propto \displaystyle \sum_{y&#x3D;0}^1p_ r(y|x)logp_\theta(y|x) \\\\<br>&amp; &#x3D; -I(y^*&#x3D;1)logp_\theta(y&#x3D;1|x)-I(y^*&#x3D;0)logp_\theta(y&#x3D;0|x) \\\\<br>&amp; &#x3D; -y^* logp_ \theta(y&#x3D;1|x) - (1- y^*)logp_ \theta (y&#x3D;0|x) \\\\<br>&amp; &#x3D; -logp_\theta(y^*|x)<br>\end{align}<br>$$</p><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs taggerscript">\begin&#123;align&#125;<br>&amp; = 公式 <span class="hljs-symbol">\\</span><span class="hljs-symbol">\\</span><span class="hljs-symbol">\\</span><span class="hljs-symbol">\\</span><br>&amp; = 公式 <span class="hljs-symbol">\\</span><span class="hljs-symbol">\\</span><span class="hljs-symbol">\\</span><span class="hljs-symbol">\\</span><br>&amp; = 公式 <span class="hljs-symbol">\\</span><span class="hljs-symbol">\\</span><span class="hljs-symbol">\\</span><span class="hljs-symbol">\\</span><br>&amp; = 公式 <br>\end&#123;align&#125;<br></code></pre></td></tr></table></figure><blockquote><p>注意begin-end段的设定</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathjax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mathjax常用符号列表</title>
      <link href="/post/ca7258e9.html"/>
      <url>/post/ca7258e9.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="mathjax常用符号列表"><a href="#mathjax常用符号列表" class="headerlink" title="mathjax常用符号列表"></a>mathjax常用符号列表</h1><p><a href="https://www.oscaner.com/skill/others/mathjax-symbol.html">MathJax 常用数学符号表 - Oscaner 的博客 | Oscaner Blog</a></p>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathjax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习概述笔记</title>
      <link href="/post/6d1ba54c.html"/>
      <url>/post/6d1ba54c.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="《神经网络与机器学习》第二章笔记"><a href="#《神经网络与机器学习》第二章笔记" class="headerlink" title="《神经网络与机器学习》第二章笔记"></a>《神经网络与机器学习》第二章笔记</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="机器学习的三要素"><a href="#机器学习的三要素" class="headerlink" title="机器学习的三要素"></a>机器学习的三要素</h3><p>模型、学习准则、优化</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>模型：$f(\boldsymbol{x} ; \boldsymbol{w},b)&#x3D;\boldsymbol{w}^{\top} \boldsymbol{x}+b$，使用增广矩阵表示</p><h2 id="概率基础"><a href="#概率基础" class="headerlink" title="概率基础"></a>概率基础</h2><h4 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h4><p>在一次试验中，事件A出现的概率为$µ$，不出现的概率为$1 − µ$。若用变量X表示事件A出现的次数，则X的取值为0和1，其相应的分布为<br>$$<br>P(X)&#x3D;μ^{X}(1-μ)^{1-X}<br>$$</p><h4 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h4><p>在n次伯努利分布中，若以变量X表示事件A出现的次数，则X 的取值为{0,… ,n}<br>$$<br>P(x&#x3D;k)&#x3D;C^k_ nμ^{k}(1-μ)^{1-k}<br>$$</p><h4 id="累计随机变量-CDF"><a href="#累计随机变量-CDF" class="headerlink" title="累计随机变量(CDF)"></a>累计随机变量(CDF)</h4><p>$$<br>cdf(x) &#x3D; \begin{cases}<br>P(X \le x) &amp; \text{if X是离散随机变量} \\[2ex]<br>\int^x_ {-\infty} &amp; \text{if X是连续变量} \\<br>\end{cases}<br>$$</p><h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>对于离散随机向量$(X,Y)$，已知$X &#x3D; x$的条件下，随机变量$Y &#x3D; y$的条件概率为：<br>$$<br>p(y|x) &#x3D; P(Y&#x3D;y|X&#x3D;x) &#x3D; {p(x,y) \over p(x)}<br>$$</p><h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><p>两个条件概率 $p(y|x)$ 和 $p(x|y)$ 之间的关系<br>$$<br>p(y|x)&#x3D;{p(x|y)p(y) \over p(x)}<br>$$</p><h3 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h3><p>随机变量的均值<br>$$<br>\mathbb E[X] &#x3D;<br>\begin{cases}<br>\sum^N_ {n&#x3D;1}{x_ np(x_ n)} \\[2em]<br>\int_\mathbb R xp(x) dx<br>\end{cases}<br>$$</p><h3 id="大数定律"><a href="#大数定律" class="headerlink" title="大数定律"></a>大数定律</h3><p>当样本足够大时，样本的均值将无限接近于分布期望</p><h3 id="最大似然与最大后验"><a href="#最大似然与最大后验" class="headerlink" title="最大似然与最大后验"></a>最大似然与最大后验</h3><p>根据贝叶斯公式$p(w|x) &#x3D; {p(x|w)p(w) \over p(x)}$得<br>$$<br>p(w|x) \propto p(x|w)p(w)<br>$$<br>其中，我们称$p(x|w)$是基于先验的似然函数，$p(w)$是先验，$p(w|x)$是后验函数，即：<br>$$<br>后验(posterior) \propto 似然(likelihood) \times 先验(prior)<br>$$<br>似然函数是关于统计模型$p(x;w)$的参数w的函数，和概率$p(x;w)$的表示方法一致，但看待角度不同，概率$p(x;w)$是描述固定参数$w$后，随机变量$x$的分布情况，是对$x$的看待角度；似然$p(x;w)$则是描述已知随机变量$x$时，不同参数$w$对分布产生的影响，是对$w$的看法。</p><p>一般我们看到的是这样的表示形式<br>$$<br>p(w|X,y;\nu,\sigma) \propto p(y|X,w;\sigma) \times p(w,\nu), (2.1)<br>$$<br>最大似然估计是找到一组参数$w$使得似然函数$p(y|X,w;\sigma)$最大，即<br>$$<br>MLE&#x3D;\arg\max_w\ p(y|X,w;\sigma)&#x3D;\arg\max_w\ log(p(y|X,w;\sigma))<br>$$<br>令${\partial logp(y|X,w;\sigma) \over \partial w} &#x3D; 0$，可得$w^{ML}&#x3D;(XX^\top)^{-1}Xy$（计算略）。</p><p>后验是对似然与先验的综合考量，更多的，最大后验估计是找到一组$w$使得后验函数$p(w|X,y;\nu,\sigma)$最大，即有<br>$$<br>MAP&#x3D;\arg\max_w\ p(w|X,y;\nu,\sigma)<br>\propto \arg\max_w\ p(y|X,w;\sigma) \times p(w,\nu) \\<br>&#x3D; \arg\max_w\ log(p(y|X,w;\sigma) \times p(w,\nu))<br>&#x3D; \arg\max_w\ log(p(y|X,w;\sigma))+ \arg\max_w\ p(w,\nu)\\<br>&#x3D; MLE+||w||<br>$$<br>可知，MAP就是在MLE的基础上加入了正则化惩罚项。更多的，我们可以得出以下表格（计算略）</p><p><img src="/post/6d1ba54c/image-20221019140106669.png" alt="image-20221019140106669"></p><h2 id="几个关键点"><a href="#几个关键点" class="headerlink" title="几个关键点"></a>几个关键点</h2><p>常见的机器学习类型包括监督学习、无监督学习与强化学习三种</p><p><img src="/post/6d1ba54c/image-20221019142704903.png" alt="image-20221019142704903"></p><h3 id="模型选择问题——验证集"><a href="#模型选择问题——验证集" class="headerlink" title="模型选择问题——验证集"></a>模型选择问题——验证集</h3><p>引入验证集，将训练集按照7:3或者8:2的比例分成训练集和验证集，其中验证集用于选择在验证集上表现最好的模型。</p><h3 id="数据稀疏问题——交叉验证"><a href="#数据稀疏问题——交叉验证" class="headerlink" title="数据稀疏问题——交叉验证"></a>数据稀疏问题——交叉验证</h3><p>将训练集分成S组，每次使用S-1组作为训练集，剩下的一组作为验证集，选择在验证集上平均性能最好的一组。</p><p><img src="/post/6d1ba54c/image-20221019143017193.png" alt="image-20221019143017193"></p><h3 id="期望风险中的偏差与方差求解"><a href="#期望风险中的偏差与方差求解" class="headerlink" title="期望风险中的偏差与方差求解"></a>期望风险中的偏差与方差求解</h3><p><img src="/post/6d1ba54c/image-20221019143858287.png" alt="image-20221019143858287"></p><p><img src="/post/6d1ba54c/image-20221019143915122.png" alt="image-20221019143915122"></p><p>偏差与方差都是在模型中需要去考虑和中和的点。</p><p><img src="/post/6d1ba54c/image-20221019144632273.png" alt="image-20221019144632273"></p><h3 id="PAC学习"><a href="#PAC学习" class="headerlink" title="PAC学习"></a>PAC学习</h3><p>根据大数定律，当训练集大小$|D|$趋向无穷大时，泛化错误趋向于0，即经验风险趋近于期望风险。</p><p>PAC学习的表达式如下：<br>$$<br>P((\mathcal R(\mathcal f) - \mathcal R^{emp}_\mathcal D(f)) \le \epsilon) \ge 1-\delta<br>$$<br>其中，$(\mathcal R(\mathcal f) - \mathcal R^{emp}_\mathcal D(f)) \le \epsilon$表示经验误差与结构误差需近似正确，$\epsilon \in (0,0.5)$；$P(…)\ge 1 - \delta $ 表示可能性，$\delta \in (0,0.5)$。如果固定$\epsilon和\delta$，可以反过来计算出样本的复杂度为（计算略）<br>$$<br>n(\epsilon,\delta) \ge {1 \over 2\epsilon^2}(ln|\mathcal F|+ln{2\over\delta})<br>$$<br>公式表明，如果希望模型的假设空间$|\mathcal F|$越大，泛化误差越小，那么其需要的样本数量$n$也就越多。</p><h2 id="常见定理"><a href="#常见定理" class="headerlink" title="常见定理"></a>常见定理</h2><h3 id="天下没有午餐定理"><a href="#天下没有午餐定理" class="headerlink" title="天下没有午餐定理"></a>天下没有午餐定理</h3><p>对于基于迭代的最优化算法，<b>不存在某种算法对所有问题（有限的搜索空间内）都有效。</b>如果一个算法对某些问题有效，那么它一定在另外一些问题上比纯随机搜索算法更差。</p><h3 id="丑小鸭定理"><a href="#丑小鸭定理" class="headerlink" title="丑小鸭定理"></a>丑小鸭定理</h3><p>丑小鸭与白天鹅之间的区别和两只白天鹅之间的区别一样大，即特征差别一样大。<b>比较时需要确定比较标准。</b></p><h3 id="奥卡姆剃刀原理"><a href="#奥卡姆剃刀原理" class="headerlink" title="奥卡姆剃刀原理"></a>奥卡姆剃刀原理</h3><p>在效力相同情况下，简单的模型要优于更为复杂的模型</p><h3 id="归纳偏置"><a href="#归纳偏置" class="headerlink" title="归纳偏置"></a>归纳偏置</h3><p>很多学习算法经常会对学习的问题做一些假设，这些假设就称为<strong>归纳偏置</strong>，归纳偏置在贝叶斯学习中也经常称为先验（Prior）。</p><p>索引-&gt;B树</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pip国内镜像源</title>
      <link href="/post/c6d2374f.html"/>
      <url>/post/c6d2374f.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="pip国内镜像源"><a href="#pip国内镜像源" class="headerlink" title="pip国内镜像源"></a>pip国内镜像源</h1><p>清华：<a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p><p>阿里云：<a href="http://mirrors.aliyun.com/pypi/simple/">http://mirrors.aliyun.com/pypi/simple/</a></p><p>中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/">https://pypi.mirrors.ustc.edu.cn/simple/</a></p><p>豆瓣：<a href="http://pypi.douban.com/simple/">http://pypi.douban.com/simple/</a></p><h2 id="临时使用"><a href="#临时使用" class="headerlink" title="临时使用"></a>临时使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip install matplotlib -i https://pypi.tuna.tsinghua.edu.cn/simple<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络与深度学习实验二——机器学习基础</title>
      <link href="/post/eb959071.html"/>
      <url>/post/eb959071.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="此实验对应NNDL书第二章"><a href="#此实验对应NNDL书第二章" class="headerlink" title="此实验对应NNDL书第二章"></a>此实验对应NNDL书第二章</h1><h2 id="机器学习的要素"><a href="#机器学习的要素" class="headerlink" title="机器学习的要素"></a>机器学习的要素</h2><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>在实践中，数据的质量会很大程度上影响模型最终的性能，通常数据预处理是完成机器学习实践的第一步，噪音越少、规模越大、覆盖范围越广的数据集往往能够训练出性能更好的模型。数据预处理可分为两个环节：先对收集到的数据进行基本的预处理，如基本的统计、特征归一化和异常值处理等；再将数据划分为训练集、验证集（开发集）和测试集。</p><ul><li><b>训练集</b>：用于模型训练时调整模型的参数，在这份数据集上的误差被称为训练误差；</li><li><b>验证集（开发集）</b>：对于复杂的模型，常常有一些超参数需要调节，因此需要<b>尝试多种超参数的组合来分别训练多个模型</b>，然后对比它们在验证集上的表现，选择一组相对最好的超参数，最后才使用这组参数下训练的模型在测试集上评估测试误差。</li><li><b>测试集</b>：模型在这份数据集上的误差被称为测试误差。训练模型的目的是为了通过从训练数据中找到规律来预测未知数据，因此测试误差是更能反映出模型表现的指标。</li></ul><p>数据划分时要考虑到两个因素：更多的训练数据会降低参数估计的方差，从而得到更可信的模型；而更多的测试数据会降低测试误差的方差，从而得到更可信的测试误差。如果给定的数据集没有做任何划分，我们一般可以大致按照7:3或者8:2的比例划分训练集和测试集，再根据7:3或者8:2的比例从训练集中再次划分出训练集和验证集。</p><p>需要强调的是，测试集只能用来评测模型最终的性能，<b>在整个模型训练过程中不能有测试集的参与。</b></p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>我们希望能让计算机从一个函数集合 $\mathcal{F} &#x3D; {f_1(\boldsymbol{x}), f_2(\boldsymbol{x}), \cdots }$中自动寻找一个“最优”的函数$f^∗(\boldsymbol{x})$ 来近似每个样本的特征向量 $\boldsymbol{x}$ 和标签 $y$ 之间<br>的真实映射关系，实际上这个函数集合也被称为<b>假设空间</b>，在实际问题中，假设空间$\mathcal{F}$通常为一个参数化的函数族<br>$$<br>\mathcal{F}&#x3D;\lbrace f(\boldsymbol{x};\theta)\mid\theta\in\mathbb{R}^{D} \rbrace, (2.1)<br>$$<br>其中$f(\boldsymbol{x} ; \theta)$是参数为$\theta$的函数，也称为模型，𝐷为参数的数量。</p><p>常见的假设空间可以分为线性和非线性两种，对应的模型 $f$ 也分别称为线性模型和非线性模型。<strong>线性模型</strong>的假设空间为一个参数化的线性函数族，即：<br>$$<br>f(\boldsymbol{x} ; \theta)&#x3D;\boldsymbol{w}^{\top} \boldsymbol{x}+b, (2.2)<br>$$<br>其中参数$\theta$ 包含了权重向量$\boldsymbol{w}$和偏置$b$。</p><p>线性模型可以由<strong>非线性基函数</strong>$\phi(\boldsymbol{x})$变为<strong>非线性模型</strong>，从而增强模型能力:</p><p>$$<br>f(\boldsymbol{x} ; \theta)&#x3D;\boldsymbol{w}^{\top} \phi(\boldsymbol{x})+b, (2.3)<br>$$<br>其中$\phi(\boldsymbol{x})&#x3D;\left[\phi_{1}(\boldsymbol{x}), \phi_{2}(\boldsymbol{x}), \cdots, \phi_{K}(\boldsymbol{x})\right]^{\top}$为𝐾 个非线性基函数组成的向量，参数 $\theta$ 包含了权重向量$\boldsymbol{w}$和偏置$b$。</p><h3 id="学习准则"><a href="#学习准则" class="headerlink" title="学习准则"></a>学习准则</h3><p>为了衡量一个模型的好坏，我们需要定义一个损失函数$\mathcal{L}(\boldsymbol{y},f(\boldsymbol{x};\theta))$。损失函数是一个非负实数函数，用来量化模型预测标签和真实标签之间的差异。常见的损失函数有 0-1 损失、平方损失函数、交叉熵损失函数等。</p><p>机器学习的目标就是找到最优的模型$𝑓(𝒙;\theta^∗)$在真实数据分布上损失函数的期望最小。然而在实际中，我们无法获得真实数据分布，通常会用在训练集上的平均损失替代。</p><p>一个模型在训练集$\mathcal{D}&#x3D;{(\boldsymbol{x}^{(n)},y^{(n)})}_{n&#x3D;1}^N$上的平均损失称为<b>经验风险</b>{Empirical Risk}，即:</p><p>$$<br>\mathcal{R}^{emp}_ \mathcal{D}(\theta)&#x3D;\frac{1}{N}\sum_{n&#x3D;1}^{N}\mathcal{L}(y^{(n)},f({x}^{(n)};\theta))。 (2.4)<br>$$<br>$\mathcal{L}(\boldsymbol{y},f(\boldsymbol{x};\theta))$为损失函数。损失函数是一个非负实数函数，用来量化模型预测和真实标签之间的差异。常见的损失函数有0-1损失、平方损失函数、交叉熵损失函数等。</p><p>在通常情况下，我们可以通过使得<strong>经验风险最小化</strong>来获得具有预测能力的模型。然而，当模型比较复杂或训练数据量比较少时，经验风险最小化获得的模型在测试集上的效果比较差。而模型在测试集上的性能才是我们真正关心的指标．当一个模型在训练集错误率很低，而在测试集上错误率较高时，通常意味着发生了<strong>过拟合</strong>（Overfitting）现象。为了缓解模型的过拟合问题，我们通常会在经验损失上加上一定的正则化项来限制模型能力。</p><p>过拟合通常是由于模型复杂度比较高引起的。在实践中，最常用的正则化方式有对模型的参数进行约束，比如$\ell_1$或者$\ell_2$范数约束。这样，我们就得到了<b>结构风险（Structure Risk）</b>。<br>$$<br>\mathcal{R}^{struct}_ {\mathcal{D}}(\theta)&#x3D;\mathcal{R}^{emp}_{\mathcal{D}}(\theta)+\lambda \ell_p(\theta), (2.5)<br>$$</p><p>其中$\lambda$为正则化系数，$p&#x3D;1$或$2$表示$\ell_1$或者$\ell_2$范数。</p><h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><p>在有了优化目标之后，机器学习问题就转化为优化问题，我们可以利用已知的优化算法来学习最优的参数。当优化函数为凸函数时，我们可以令参数的偏导数等于0来计算最优参数的解析解。当优化函数为非凸函数时，我们可以用一阶的优化算法来进行优化。</p><p>目前机器学习中最常用的优化算法是<strong>梯度下降法</strong>(Gradient Descent Method)。当使用梯度下降法进行参数优化时，还可以利用验证集来<strong>早停法</strong>(Early-Stop)来中止模型的优化过程，避免模型在训练集上过拟合。早停法也是一种常用并且十分有效的正则化方法。</p><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p><strong>评估指标</strong>(Metric)用于评价模型效果，即给定一个测试集，用模型对测试集中的每个样本进行预测，并根据预测结果计算评价分数。回归任务的评估指标一般有预测值与真实值的<b>均方差</b>，分类任务的评估指标一般有<b>准确率、召回率、F1值等</b>。</p><p>对于一个机器学习任务，一般会先确定任务类型，再确定任务的评价指标，再根据评价指标来建立模型，选择学习准则。由于评价指标不可微等问题有时候学习准则并不能完全和评价指标一致，我们往往会选择一定的损失函数使得两者尽可能一致。</p><h2 id="实现线性回归"><a href="#实现线性回归" class="headerlink" title="实现线性回归"></a>实现线性回归</h2><h3 id="回归函数"><a href="#回归函数" class="headerlink" title="回归函数"></a>回归函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 真实函数的参数缺省值为 w=1.2，b=0.5</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">linear_func</span>(<span class="hljs-params">x,w=<span class="hljs-number">1.2</span>,b=<span class="hljs-number">0.5</span></span>):<br>    y = w*x + b<br>    <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure><h3 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_toy_data</span>(<span class="hljs-params">func, interval, sample_num, noise = <span class="hljs-number">0.0</span>, add_outlier = <span class="hljs-literal">False</span>, outlier_ratio = <span class="hljs-number">0.001</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据给定的函数，生成样本</span><br><span class="hljs-string">    输入：</span><br><span class="hljs-string">       - func：函数</span><br><span class="hljs-string">       - interval： x的取值范围</span><br><span class="hljs-string">       - sample_num： 样本数目</span><br><span class="hljs-string">       - noise： 噪声均方差</span><br><span class="hljs-string">       - add_outlier：是否生成异常值</span><br><span class="hljs-string">       - outlier_ratio：异常值占比</span><br><span class="hljs-string">    输出：</span><br><span class="hljs-string">       - X: 特征数据，shape=[n_samples,1]</span><br><span class="hljs-string">       - y: 标签数据，shape=[n_samples,1]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># 均匀采样</span><br>    <span class="hljs-comment"># 使用paddle.rand在生成sample_num个随机数,随机数用什么都可以</span><br>    <span class="hljs-comment"># 乘法放大到interval范围，interval[0]移动到以范围中点为中心</span><br>    X = paddle.rand(shape = [sample_num]) * (interval[<span class="hljs-number">1</span>]-interval[<span class="hljs-number">0</span>]) + interval[<span class="hljs-number">0</span>]<br>    <br>    y = func(X)<br><br>    <span class="hljs-comment"># 生成高斯分布的标签噪声</span><br>    <span class="hljs-comment"># 使用paddle.normal生成0均值，noise标准差的数据,大小为y的输出张量</span><br>    <span class="hljs-comment"># 同理于torch.normal方法</span><br>    epsilon = paddle.normal(<span class="hljs-number">0</span>,noise,paddle.to_tensor(y.shape[<span class="hljs-number">0</span>]))<br>    y = y + epsilon<br>    <span class="hljs-comment"># 生成额外的异常点</span><br>    <span class="hljs-keyword">if</span> add_outlier:     <br>        outlier_num = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(y)*outlier_ratio)<br>        <span class="hljs-keyword">if</span> outlier_num != <span class="hljs-number">0</span>:<br>            <span class="hljs-comment"># 使用paddle.randint生成服从均匀分布的、范围在[0, len(y))的随机Tensor</span><br>            outlier_idx = paddle.randint(<span class="hljs-built_in">len</span>(y),shape = [outlier_num])<br>            y[outlier_idx] = y[outlier_idx] * <span class="hljs-number">5</span><br>    <span class="hljs-keyword">return</span> X, y<br></code></pre></td></tr></table></figure><h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt <span class="hljs-comment"># matplotlib 是 Python 的绘图库</span><br><br>func = linear_func<br>interval = (-<span class="hljs-number">10</span>,<span class="hljs-number">10</span>)<br>train_num = <span class="hljs-number">100</span> <span class="hljs-comment"># 训练样本数目</span><br>test_num = <span class="hljs-number">50</span> <span class="hljs-comment"># 测试样本数目</span><br>noise = <span class="hljs-number">2</span><br>X_train, y_train = create_toy_data(func=func, interval=interval, sample_num=train_num, noise = noise, add_outlier = <span class="hljs-literal">False</span>)<br>X_test, y_test = create_toy_data(func=func, interval=interval, sample_num=test_num, noise = noise, add_outlier = <span class="hljs-literal">False</span>)<br><br>X_train_large, y_train_large = create_toy_data(func=func, interval=interval, sample_num=<span class="hljs-number">5000</span>, noise = noise, add_outlier = <span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># paddle.linspace返回一个Tensor，Tensor的值为在区间start和stop上均匀间隔的num个值，输出Tensor的长度为num</span><br>X_underlying = paddle.linspace(interval[<span class="hljs-number">0</span>],interval[<span class="hljs-number">1</span>],train_num) <br>y_underlying = linear_func(X_underlying)<br><br><span class="hljs-comment"># 绘制数据</span><br>plt.scatter(X_train, y_train, marker=<span class="hljs-string">&#x27;*&#x27;</span>, facecolor=<span class="hljs-string">&quot;none&quot;</span>, edgecolor=<span class="hljs-string">&#x27;#e4007f&#x27;</span>, s=<span class="hljs-number">50</span>, label=<span class="hljs-string">&quot;train data&quot;</span>)<br>plt.scatter(X_test, y_test, facecolor=<span class="hljs-string">&quot;none&quot;</span>, edgecolor=<span class="hljs-string">&#x27;#f19ec2&#x27;</span>, s=<span class="hljs-number">50</span>, label=<span class="hljs-string">&quot;test data&quot;</span>)<br>plt.plot(X_underlying, y_underlying, c=<span class="hljs-string">&#x27;#000000&#x27;</span>, label=<span class="hljs-string">r&quot;underlying distribution&quot;</span>)<br>plt.legend(fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>) <span class="hljs-comment"># 给图像加图例</span><br>plt.savefig(<span class="hljs-string">&#x27;ml-vis.pdf&#x27;</span>) <span class="hljs-comment"># 保存图像到PDF文件中</span><br>plt.show()<br><br></code></pre></td></tr></table></figure><p><img src="/post/eb959071/image-20221018153352091.png" alt="image-20221018153352091"></p><h3 id="线性模型的构建"><a href="#线性模型的构建" class="headerlink" title="线性模型的构建"></a>线性模型的构建</h3><p>在线性回归中，自变量为样本的特征向量$\boldsymbol{x}\in \mathbb{R}^D$(每一维对应一个自变量)，因变量是连续值的标签$y\in R$。</p><p>线性模型定义为：<br>$$<br>f(\boldsymbol{x};\boldsymbol{w},b)&#x3D;\boldsymbol{w}^T\boldsymbol{x}+b。 (2.6)<br>$$</p><p>其中权重向量$\boldsymbol{w}\in \mathbb{R}^D$和偏置$b\in \mathbb{R}$都是可学习的参数。</p><p>注意：《神经网络与深度学习》中为了表示的简洁性，使用<strong>增广权重向量</strong>来定义模型。而在本书中，为了和代码实现保持一致，我们使用非增广向量的形式来定义模型。</p><p>在实践中，为了提高预测样本的效率，我们通常会将$N$样本归为一组进行<b>成批地预测</b>，这样可以更好地利用GPU设备的并行计算能力。</p><p>$$<br>\boldsymbol{y} &#x3D;\boldsymbol{X} \boldsymbol{w} + b, (2.7)<br>$$</p><p>其中$\boldsymbol{X}\in \mathbb{R}^{N\times D}$为$N$个样本的特征矩阵，$\boldsymbol{y}\in \mathbb{R}^N$为$N$个预测值组成的列向量。</p><p>注意：在实践中，样本的矩阵$\boldsymbol{X}$是由$N$个$\boldsymbol{x}$的<strong>行向量</strong>组成。而原教材中$\boldsymbol{x}$为列向量，其特征矩阵与本书中的特征矩阵刚好为转置关系。</p><h4 id="线性算子"><a href="#线性算子" class="headerlink" title="线性算子"></a>线性算子</h4><p>实现公式(2.7)中的线性函数非常简单，我们直接利用如下张量运算来实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># X: tensor, shape=[N,D]</span><br><span class="hljs-comment"># y_pred: tensor, shape=[N]</span><br><span class="hljs-comment"># w: shape=[D,1]</span><br><span class="hljs-comment"># b: shape=[1]</span><br>y_pred = paddle.matmul(X,w)+b<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> torch<br><br>torch.seed() <span class="hljs-comment">#设置随机种子</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Op</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">pass</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-keyword">return</span> self.forward(inputs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br><br><span class="hljs-comment"># 线性算子</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">           - input_size:模型要处理的数据特征向量长度</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        self.input_size = input_size<br><br>        <span class="hljs-comment"># 模型参数</span><br>        self.params = &#123;&#125;<br>        <span class="hljs-comment"># 行向量</span><br>        self.params[<span class="hljs-string">&#x27;w&#x27;</span>] = paddle.randn(shape=[self.input_size,<span class="hljs-number">1</span>],dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>        <span class="hljs-comment"># self.params[&#x27;w&#x27;] = torch.randn(self.input_size,1,dtype=torch.float32)</span><br>        <span class="hljs-number">1</span>*<span class="hljs-number">1</span><br>        self.params[<span class="hljs-string">&#x27;b&#x27;</span>] = paddle.zeros(shape=[<span class="hljs-number">1</span>],dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>        <span class="hljs-comment"># self.params[&#x27;b&#x27;] = torch.zeros(1,dtype=torch.float32)</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.forward(X)<br><br>    <span class="hljs-comment"># 前向函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">           - X: tensor, shape=[N,D]</span><br><span class="hljs-string">           注意这里的X矩阵是由N个x向量的转置拼接成的，与原教材行向量表示方式不一致</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">           - y_pred： tensor, shape=[N]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        N,D = X.shape<br><br>        <span class="hljs-keyword">if</span> self.input_size==<span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> paddle.full(shape=[N,<span class="hljs-number">1</span>], fill_value=self.params[<span class="hljs-string">&#x27;b&#x27;</span>])<br>            <span class="hljs-comment"># return torch.full(size=[N,1],fill_value=self.params[&#x27;b&#x27;])</span><br>        <br>        <span class="hljs-keyword">assert</span> D==self.input_size <span class="hljs-comment"># 输入数据维度合法性验证</span><br><br>        <span class="hljs-comment"># 使用paddle.matmul计算两个tensor的乘积</span><br>        y_pred = paddle.matmul(X,self.params[<span class="hljs-string">&#x27;w&#x27;</span>])+self.params[<span class="hljs-string">&#x27;b&#x27;</span>]<br>        <span class="hljs-comment"># y_pred = torch.matmul(X,self.params[&#x27;w&#x27;]+self.params[&#x27;b&#x27;])</span><br>        <br>        <span class="hljs-keyword">return</span> y_pred<br><br><span class="hljs-comment"># 注意这里我们为了和后面章节统一，这里的X矩阵是由N个x向量的转置拼接成的，与原教材行向量表示方式不一致</span><br>input_size = <span class="hljs-number">3</span><br>N = <span class="hljs-number">2</span><br><span class="hljs-number">2</span>*<span class="hljs-number">3</span><br>X = paddle.randn(shape=[N, input_size],dtype=<span class="hljs-string">&#x27;float32&#x27;</span>) <span class="hljs-comment"># 生成2个维度为3的数据</span><br><br><span class="hljs-comment"># X = torch.randn(N,input_size,dtype=torch.float32)</span><br>model = Linear(input_size)<br>y_pred = model(X)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y_pred:&quot;</span>,y_pred) <span class="hljs-comment">#输出结果的个数也是2个</span><br><br></code></pre></td></tr></table></figure><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>回归任务是对<strong>连续值</strong>的预测，希望模型能根据数据的特征输出一个连续值作为预测值。因此回归任务中常用的评估指标是<strong>均方误差</strong>。</p><p>令$\boldsymbol{y}\in \mathbb{R}^N$，$\hat{\boldsymbol{y}}\in \mathbb{R}^N$分别为$N$个样本的真实标签和预测标签，均方误差的定义为：</p><p>$$<br>\mathcal{L}(\boldsymbol{y},\hat{\boldsymbol{y}})&#x3D;\frac{1}{2N}|\boldsymbol{y}-\hat{\boldsymbol{y}}|^2&#x3D;\frac{1}{2N}|\boldsymbol{X}\boldsymbol{w}+\boldsymbol{b}-\boldsymbol{y}|^2, (2.8)<br>$$<br>其中$\boldsymbol{b}$为$N$维向量，所有元素取值都为$b$。</p><p>均方误差的代码实现如下:</p><blockquote><p>注意：代码实现中没有除2。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_squared_error</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    输入：</span><br><span class="hljs-string">       - y_true: tensor，样本真实标签</span><br><span class="hljs-string">       - y_pred: tensor, 样本预测标签</span><br><span class="hljs-string">    输出：</span><br><span class="hljs-string">       - error: float，误差值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 确定两者的维度一致</span><br>    <span class="hljs-keyword">assert</span> y_true.shape[<span class="hljs-number">0</span>] == y_pred.shape[<span class="hljs-number">0</span>]<br>    <br>    <span class="hljs-comment"># paddle.square计算输入的平方值</span><br>    <span class="hljs-comment"># paddle.mean沿 axis 计算 x 的平均值，默认axis是None，则对输入的全部元素计算平均值。</span><br>    <span class="hljs-comment"># error = paddle.mean(paddle.square(y_true - y_pred))</span><br>    error = torch.mean(torch.square(y_true-y_pred))<br>    <span class="hljs-keyword">return</span> error<br><br><br><span class="hljs-comment"># 构造一个简单的样例进行测试:[N,1], N=2</span><br><span class="hljs-comment"># y_true = paddle.to_tensor([[-0.2],[4.9]],dtype=&#x27;float32&#x27;)</span><br>y_true = torch.tensor([[-<span class="hljs-number">0.2</span>],[<span class="hljs-number">4.9</span>]],dtype=torch.float32)<br><span class="hljs-comment"># y_pred = paddle.to_tensor([[1.3],[2.5]],dtype=&#x27;float32&#x27;)</span><br>y_pred = torch.tensor([[<span class="hljs-number">1.3</span>],[<span class="hljs-number">2.5</span>]],dtype=torch.float32)<br><br>error = mean_squared_error(y_true=y_true, y_pred=y_pred).item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;error:&quot;</span>,error)<br></code></pre></td></tr></table></figure><p><code>error: 4.005000114440918</code></p><h3 id="2-2-4-模型优化"><a href="#2-2-4-模型优化" class="headerlink" title="2.2.4 模型优化"></a>2.2.4 模型优化</h3><p>采用<strong>经验风险最小化</strong>，线性回归可以通过最小二乘法求出参数$\boldsymbol{w}$和$b$的解析解。计算公式(2.8)中均方误差对参数$b$的偏导数，得到<br>$$<br>\frac{\partial \mathcal{L}(\boldsymbol{y},\hat{\boldsymbol{y}})}{\partial b} &#x3D; \mathbf{1}^T (\boldsymbol{X}\boldsymbol{w}+\boldsymbol{b}-\boldsymbol{y}), (2.9)<br>$$</p><p>其中$\mathbf{1}$为$N$维的全1向量。<strong>这里为了简单起见省略了均方误差的系数$\frac{1}{N}$，并不影响最后的结果</strong>。</p><p>令上式等于0，得到<br>$$<br>b^* &#x3D;\bar{y}-\bar{\boldsymbol{x}}^T \boldsymbol{w},(2.10)<br>$$</p><p>其中$\bar{y} &#x3D; \frac{1}{N}\mathbf{1}^T\boldsymbol{y}$为所有标签的平均值，$\bar{\boldsymbol{x}} &#x3D; \frac{1}{N}(\mathbf{1}^T \boldsymbol{X})^T$ 为所有特征向量的平均值。将$b^*$代入公式(2.8)中均方误差对参数$\boldsymbol{w}$的偏导数，得到<br>$$<br>\frac{\partial \mathcal{L}(\boldsymbol{y},\hat{\boldsymbol{y}})}{\partial \boldsymbol{w}} &#x3D; (\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T \Big((\boldsymbol{X}-\bar{\boldsymbol{x}}^T)\boldsymbol{w}-(\boldsymbol{y}-\bar{y})\Big).(2.11)<br>$$<br>令上式等于0，得到最优的参数为<br>$$<br>\boldsymbol{w}^*&#x3D;\Big((\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T(\boldsymbol{X}-\bar{\boldsymbol{x}}^T)\Big)^{\mathrm{-}1}(\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T (\boldsymbol{y}-\bar{y}),(2.12)<br>$$</p><p>$$<br>b^* &#x3D;  \bar{y}-\bar{\boldsymbol{x}}^T \boldsymbol{w}^*.(2.13)<br>$$</p><p>若对参数$\boldsymbol{w}$加上$\ell_2$正则化，则最优的$\boldsymbol{w}^*$变为<br>$$<br>\boldsymbol{w}^*&#x3D;\Big((\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T(\boldsymbol{X}-\bar{\boldsymbol{x}}^T)+\lambda \boldsymbol{I}\Big)^{\mathrm{-}1}(\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T (\boldsymbol{y}-\bar{y}),(2.14)<br>$$</p><p>其中$\lambda&gt;0$为预先设置的正则化系数，$\boldsymbol{I}\in \mathbb{R}^{D\times D}$为单位矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">optimizer_lsm</span>(<span class="hljs-params">model, X, y, reg_lambda=<span class="hljs-number">0</span></span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    输入：</span><br><span class="hljs-string">       - model: 模型</span><br><span class="hljs-string">       - X: tensor, 特征数据，shape=[N,D]</span><br><span class="hljs-string">       - y: tensor,标签数据，shape=[N]</span><br><span class="hljs-string">       - reg_lambda: float, 正则化系数，默认为0</span><br><span class="hljs-string">    输出：</span><br><span class="hljs-string">       - model: 优化好的模型</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>  N, D = X.shape<br><br>  <span class="hljs-comment"># 对输入特征数据所有特征向量求平均</span><br>  x_bar_tran = paddle.mean(X,axis=<span class="hljs-number">0</span>).T <br>  <br>  <span class="hljs-comment"># 求标签的均值,shape=[1]</span><br>  y_bar = paddle.mean(y)<br>  <br>  <span class="hljs-comment"># paddle.subtract通过广播的方式实现矩阵减向量</span><br>  x_sub = paddle.subtract(X,x_bar_tran)<br><br>  <span class="hljs-comment"># 使用paddle.all判断输入tensor是否全0</span><br>  <span class="hljs-keyword">if</span> paddle.<span class="hljs-built_in">all</span>(x_sub==<span class="hljs-number">0</span>):<br>    model.params[<span class="hljs-string">&#x27;b&#x27;</span>] = y_bar<br>    model.params[<span class="hljs-string">&#x27;w&#x27;</span>] = paddle.zeros(shape=[D])<br>    <span class="hljs-keyword">return</span> model<br>  <br>  <span class="hljs-comment"># paddle.inverse求方阵的逆</span><br>  tmp = paddle.inverse(paddle.matmul(x_sub.T,x_sub)+<br>          reg_lambda*paddle.eye(num_rows = (D)))<br><br>  w = paddle.matmul(paddle.matmul(tmp,x_sub.T),(y-y_bar))<br>  <br>  b = y_bar-paddle.matmul(x_bar_tran,w)<br>  <br>  model.params[<span class="hljs-string">&#x27;b&#x27;</span>] = b<br>  model.params[<span class="hljs-string">&#x27;w&#x27;</span>] = paddle.squeeze(w,axis=-<span class="hljs-number">1</span>)<br><br>  <span class="hljs-keyword">return</span> model<br><br>input_size = <span class="hljs-number">1</span><br>model = Linear(input_size)<br>model = optimizer_lsm(model,X_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]),y_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;w_pred:&quot;</span>,model.params[<span class="hljs-string">&#x27;w&#x27;</span>].item(), <span class="hljs-string">&quot;b_pred: &quot;</span>, model.params[<span class="hljs-string">&#x27;b&#x27;</span>].item())<br><br>y_train_pred = model(X_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])).squeeze()<br>train_error = mean_squared_error(y_true=y_train, y_pred=y_train_pred).item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;train error: &quot;</span>,train_error)<br></code></pre></td></tr></table></figure><p><code>w_pred: 1.1961743831634521 b_pred:  0.5631881952285767 train error:  3.670858144760132</code></p><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>下面用训练好的模型预测一下测试集的标签，并计算在测试集上的损失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">y_test_pred = model(X_test.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])).squeeze()<br>test_error = mean_squared_error(y_true=y_test, y_pred=y_test_pred).item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;test error: &quot;</span>,test_error)<br></code></pre></td></tr></table></figure><p><code>test error: 4.327543258666992</code></p><h2 id="实现多项式回归"><a href="#实现多项式回归" class="headerlink" title="实现多项式回归"></a>实现多项式回归</h2><p>多项式回归是回归任务的一种形式，其中自变量和因变量之间的关系是$M$次多项式的一种线性回归形式，即：<br>$$<br>f(\boldsymbol{x};\boldsymbol{w})&#x3D;w_1x+w_2x^2+…+w_Mx^M+b&#x3D;\boldsymbol{w}^T\phi(x)+b, （2.10）<br>$$<br>其中$M$为多项式的阶数，$\boldsymbol{w}&#x3D;[w_1,…,w_M]^T$为多项式的系数，$\phi(x)&#x3D;[x,x^2,\cdots,x^M]^T$为多项式基函数，将原始特征$x$映射为$M$维的向量。当$M&#x3D;0$时，$f(\boldsymbol{x};\boldsymbol{w})&#x3D;b$。</p><p>公式（2.10）展示的是特征维度为1的多项式表达，<b>当特征维度大于1时，存在不同特征之间交互的情况，这是线性回归无法实现。</b>公式（2.11）展示的是当特征维度为2，多项式阶数为2时的多项式回归：</p><p>$$<br>f(\boldsymbol{x};\boldsymbol{w})&#x3D;w_1x_1+w_2x_2+w_3x_1^2+w_4x_1x_2+w_5x_2^2+b, （2.11）<br>$$</p><p>当自变量和因变量之间并不是线性关系时，我们可以定义非线性基函数对特征进行变换，从而可以使得线性回归算法实现非线性的曲线拟合。</p><p>接下来我们基于特征维度为1的自变量介绍多项式回归实验。</p><h3 id="数据生成-1"><a href="#数据生成-1" class="headerlink" title="数据生成"></a>数据生成</h3><p>假设我们要拟合的非线性函数为一个缩放后的$sin$函数。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo-cactus魔改笔记+bug修复记录</title>
      <link href="/post/94815387.html"/>
      <url>/post/94815387.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="hexo-cactus魔改笔记-bug修复记录（自22-10-15始）"><a href="#hexo-cactus魔改笔记-bug修复记录（自22-10-15始）" class="headerlink" title="hexo+cactus魔改笔记+bug修复记录（自22.10.15始）"></a>hexo+cactus魔改笔记+bug修复记录（自22.10.15始）</h1><h2 id="1、目录跳转功能失效"><a href="#1、目录跳转功能失效" class="headerlink" title="1、目录跳转功能失效"></a>1、目录跳转功能失效</h2><p>mongodb+srv:&#x2F;&#x2F;yaowenjin:<a href="mailto:&#x77;&#x73;&#121;&#119;&#x6a;&#57;&#x39;&#x30;&#50;&#x32;&#54;&#x40;&#98;&#108;&#111;&#103;&#46;&#x69;&#112;&#109;&#103;&#99;&#49;&#57;&#x2e;&#109;&#111;&#x6e;&#103;&#x6f;&#100;&#98;&#x2e;&#110;&#101;&#x74;">&#x77;&#x73;&#121;&#119;&#x6a;&#57;&#x39;&#x30;&#50;&#x32;&#54;&#x40;&#98;&#108;&#111;&#103;&#46;&#x69;&#112;&#109;&#103;&#99;&#49;&#57;&#x2e;&#109;&#111;&#x6e;&#103;&#x6f;&#100;&#98;&#x2e;&#110;&#101;&#x74;</a>&#x2F;?retryWrites&#x3D;true&amp;w&#x3D;majority</p><p>找到【…\blog\node_modules\hexo-toc\lib】中的filter.js，将28-31行修改成以下代码</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs js">$title.<span class="hljs-title function_">attr</span>(<span class="hljs-string">&#x27;id&#x27;</span>, id);<br><span class="hljs-comment">//$title.children(&#x27;a&#x27;).remove();</span><br><span class="hljs-comment">//$title.html( &#x27;&lt;span id=&quot;&#x27; + id + &#x27;&quot;&gt;&#x27; + $title.html() + &#x27;&lt;/span&gt;&#x27; );</span><br><span class="hljs-comment">//$title.removeAttr(&#x27;id&#x27;);</span><br></code></pre></td></tr></table></figure><h2 id="2、添加版权提醒"><a href="#2、添加版权提醒" class="headerlink" title="2、添加版权提醒"></a>2、添加版权提醒</h2><p>在post主页的前端渲染代码【…\blog\themes\cactus\layout\post.ejs】中的页面末尾加入一个div，写入以下代码：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs js">&lt;div&gt;    <br>    <span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">ul</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;post-copyright&quot;</span>&gt;</span></span><br><span class="language-xml">       <span class="hljs-tag">&lt;<span class="hljs-name">li</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;post-copyright-link&quot;</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">strong</span>&gt;</span>本文作者：<span class="hljs-tag">&lt;/<span class="hljs-name">strong</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;/&quot;</span>&gt;</span>&lt;%= config.author %&gt;<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;/<span class="hljs-name">li</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">li</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;post-copyright-link&quot;</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">strong</span>&gt;</span>本文标题：<span class="hljs-tag">&lt;/<span class="hljs-name">strong</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;&#123;&#123; url_for(config.permalink) &#125;&#125;&quot;</span>&gt;</span>&lt;%- partial(&#x27;_partial/post/title&#x27;, &#123; post: page, index: true, class_name: &#x27;&#x27; &#125;) %&gt;<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;/<span class="hljs-name">li</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">li</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;post-copyright-link&quot;</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">strong</span>&gt;</span>本文链接：<span class="hljs-tag">&lt;/<span class="hljs-name">strong</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">title</span>=<span class="hljs-string">&quot;&lt;%- page.title %&gt;&quot;</span>&gt;</span>&lt;%- config.url %&gt;/&lt;%- page.path %&gt;<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;/<span class="hljs-name">li</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">li</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;post-copyright-license&quot;</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">strong</span>&gt;</span>版权声明： <span class="hljs-tag">&lt;/<span class="hljs-name">strong</span>&gt;</span></span><br><span class="language-xml">        本文由 &lt;%- config.author %&gt; 原创，采用 <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh&quot;</span> <span class="hljs-attr">title</span>=<span class="hljs-string">&quot;Attribution-NonCommercial-NoDerivatives 4.0 International&quot;</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;license&quot;</span> <span class="hljs-attr">target</span>=<span class="hljs-string">&quot;_blank&quot;</span>&gt;</span>CC BY-NC-ND 4.0<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span>, 转载请保留以上声明信息！</span><br><span class="language-xml">      <span class="hljs-tag">&lt;/<span class="hljs-name">li</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">ul</span>&gt;</span></span><br>&lt;/div&gt;<br></code></pre></td></tr></table></figure><p>然后在网页的样式文件中加入样式设置，这里我偷了个懒直接加到了总文件</p><p>【…\blog\themes\cactus\source\css\style.styl】中</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-class">.post-copyright</span> &#123;<br>    <span class="hljs-attribute">margin</span>: <span class="hljs-number">2em</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span>;<br>    <span class="hljs-attribute">padding</span>: <span class="hljs-number">0.5em</span> <span class="hljs-number">1em</span>;<br>    <span class="hljs-attribute">border-left</span>: <span class="hljs-number">3px</span> solid <span class="hljs-number">#FF1700</span>;<br>    <span class="hljs-attribute">background-color</span>: <span class="hljs-number">#F9F9F9</span>;<br>    <span class="hljs-attribute">list-style</span>: none;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="3、添加图片放大功能"><a href="#3、添加图片放大功能" class="headerlink" title="3、添加图片放大功能"></a>3、添加图片放大功能</h2><blockquote><p>这里参考了<a href="https://vccv.cc/article/fancybox-cactus.html">cactus使用上fancybox图片放大</a>的实现步骤。</p></blockquote><p>首先在【…\blog\themes\cactus\layout_partial\head.ejs】中加入css的引入文件</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs js">&lt;% <span class="hljs-keyword">if</span> (theme.<span class="hljs-property">fancybox</span>.<span class="hljs-property">enabled</span>) &#123;%&gt;<br>  <span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">link</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;stylesheet&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://cdn.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.css&quot;</span> /&gt;</span></span><br>&lt;% &#125; %&gt;<br></code></pre></td></tr></table></figure><p>并在同级的script.ejs中加入调取代码</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs js">&lt;!-- <span class="hljs-title class_">FancyBox</span> --&gt;<br>&lt;% <span class="hljs-keyword">if</span> (theme.<span class="hljs-property">fancybox</span>.<span class="hljs-property">enabled</span>) &#123;%&gt;<br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://cdn.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span></span><br>&lt;% &#125; %&gt;<br></code></pre></td></tr></table></figure><p>写到这里，网页可以通过在每个img加上标签属性调用，在这里我们直接在渲染引擎hexo-renderer-marked中对img加入fancybox的渲染引擎。</p><p>找到【..\node_modules\hexo-renderer-marked\lib&#x2F;renderer.js】，搜索”img”并将img相关的代码修改如下：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-comment">// 原代码</span><br><span class="hljs-keyword">let</span> out = <span class="hljs-string">`&lt;img src=&quot;<span class="hljs-subst">$&#123;encodeURL(href)&#125;</span>&quot;`</span>;<br><span class="hljs-keyword">if</span> (text) out += <span class="hljs-string">` alt=&quot;<span class="hljs-subst">$&#123;text&#125;</span>&quot;`</span>;<br><span class="hljs-keyword">if</span> (title) out += <span class="hljs-string">` title=&quot;<span class="hljs-subst">$&#123;title&#125;</span>&quot;`</span>;<br><span class="hljs-keyword">if</span> (lazyload) out += <span class="hljs-string">&#x27; loading=&quot;lazy&quot;&#x27;</span>;<br><br>out += <span class="hljs-string">&#x27;&gt;&#x27;</span>;<br><br><span class="hljs-comment">// 修改后</span><br><span class="hljs-keyword">let</span> out = <span class="hljs-string">`&lt;a data-fancybox=&quot;gallery&quot; data-src=&quot;<span class="hljs-subst">$&#123;href&#125;</span>&quot; data-caption=&quot;<span class="hljs-subst">$&#123;text&#125;</span>&quot;&gt;&lt;img src=&quot;<span class="hljs-subst">$&#123;encodeURL(href)&#125;</span>&quot;`</span>;<br><span class="hljs-keyword">if</span> (text) out += <span class="hljs-string">` alt=&quot;<span class="hljs-subst">$&#123;text&#125;</span>&quot;`</span>;<br><span class="hljs-keyword">if</span> (title) out += <span class="hljs-string">` title=&quot;<span class="hljs-subst">$&#123;title&#125;</span>&quot;`</span>;<br><span class="hljs-keyword">if</span> (lazyload) out += <span class="hljs-string">&#x27; loading=&quot;lazy&quot;&#x27;</span>;<br><br>out += <span class="hljs-string">&#x27;&gt;&lt;/a&gt;&#x27;</span>;<br></code></pre></td></tr></table></figure><p>最后，在blog的配置文件_config.yaml中打开开关即可</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Enable FancyBox</span><br><span class="hljs-attr">fancybox:</span><br>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p><img src="/post/94815387/image-20221015162949197.png" alt="image-20221015162949197"></p><h2 id="4、添加gittalk评论功能"><a href="#4、添加gittalk评论功能" class="headerlink" title="4、添加gittalk评论功能"></a>4、添加gittalk评论功能</h2><p>gittalk是github可提供的一种application，其设置相对比较简单。</p><p>首先确定一个github的仓库用于保存评论，这里可以直接使用博客的库，比如我的库（paopao0226.github.io），然后打开github，按照Setting–&gt;Developing settings–&gt;OAuth application的过程进行</p><p><img src="/post/94815387/image-20221016143600395.png" alt="image-20221016143600395"><img src="/post/94815387/image-20221016143629825.png" alt="image-20221016143629825"><img src="/post/94815387/image-20221016143834201.png" alt="image-20221016143834201"></p><p>如果没有用过OAuth的朋友应该是需要创建一个新的。<br><img src="/post/94815387/image-20221016143934779.png" alt="image-20221016143934779"></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs txt">Application name: 应用的名字，可以随便取（其他人想要评论的话，需要登录Github授权，授权的时候显示的就是这里的Application name）<br>Homepage URL: 应用主页的完整URL，如: https://pyihe.github.io/<br>Application description: 应用描述，选填项<br>Authorization callback URL: 登录授权后回调的页面，直接填成与Homepage URL一样<br></code></pre></td></tr></table></figure><p>完成OAuth Application的申请，并申请创建密钥之后，就会得到如下界面，其中Client ID和Client Sercets记住，之后要用到。<br><img src="/post/94815387/image-20221016144134192.png" alt="image-20221016144134192"></p><p>下面我们继续设置gittalk，在【…\blog\themes\cactus\_config.yml】</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">disqus:</span><br>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 关闭官方的disqus</span><br> <span class="hljs-attr">shortname:</span> <span class="hljs-string">paopao0226</span> <br><br><span class="hljs-attr">gitalk:</span><br>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span> <span class="hljs-comment"># 开关开启</span><br>    <span class="hljs-attr">owner:</span> <span class="hljs-string">paopao0226</span> <span class="hljs-comment"># 你的 github 用户名</span><br>    <span class="hljs-attr">repo:</span> <span class="hljs-string">paopao0226.github.io</span> <span class="hljs-comment"># 保存评论的 repo 库，注意不要加https，网站导航会直接到paopao0226/paopao0226.github.io访问仓库</span><br>    <span class="hljs-attr">admin:</span> <span class="hljs-string">paopao0226</span> <span class="hljs-comment"># 管理员，你的 github 用户名</span><br>    <span class="hljs-attr">clientID:</span> <span class="hljs-string">XXX</span> <span class="hljs-comment"># 你的Client ID</span><br>    <span class="hljs-attr">clientSecret:</span> <span class="hljs-string">XXX</span> <span class="hljs-comment"># 你的Client Sercets</span><br></code></pre></td></tr></table></figure><p>然后在【…\blog\themes\cactus\layout_partial\comment.ejs】中加入以下代码</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-comment">// disqus 评论系统部分</span><br>&lt;% <span class="hljs-keyword">if</span>(page.<span class="hljs-property">comments</span> &amp;&amp; theme.<span class="hljs-property">disqus</span>.<span class="hljs-property">enabled</span>)&#123; %&gt;<br>    <span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;blog-post-comments&quot;</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;disqus_thread&quot;</span>&gt;</span></span><br><span class="language-xml">            <span class="hljs-tag">&lt;<span class="hljs-name">noscript</span>&gt;</span>&lt;%= __(&#x27;comments.no_js&#x27;) %&gt;<span class="hljs-tag">&lt;/<span class="hljs-name">noscript</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span></span><br>&lt;% &#125; %&gt;<br><span class="hljs-comment">// 下面添加 Gitalk 部分</span><br>&lt;% <span class="hljs-keyword">if</span>(page.<span class="hljs-property">comments</span> &amp;&amp; theme.<span class="hljs-property">gitalk</span>.<span class="hljs-property">enabled</span>)&#123; %&gt;<br>    <span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">link</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;stylesheet&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css&quot;</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;gitalk-container&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text/javascript&quot;</span>&gt;</span><span class="language-javascript"></span></span><br><span class="language-javascript"><span class="language-xml">        <span class="hljs-keyword">var</span> gitalk = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Gitalk</span>(&#123;</span></span><br><span class="language-javascript"><span class="language-xml">            <span class="hljs-attr">clientID</span>: <span class="hljs-string">&#x27;&lt;%= theme.gitalk.clientID %&gt;&#x27;</span>,</span></span><br><span class="language-javascript"><span class="language-xml">            <span class="hljs-attr">clientSecret</span>: <span class="hljs-string">&#x27;&lt;%= theme.gitalk.clientSecret %&gt;&#x27;</span>,</span></span><br><span class="language-javascript"><span class="language-xml">            <span class="hljs-attr">id</span>: <span class="hljs-title function_">md5</span>(<span class="hljs-variable language_">window</span>.<span class="hljs-property">location</span>.<span class="hljs-property">pathname</span>),</span></span><br><span class="language-javascript"><span class="language-xml">            <span class="hljs-attr">repo</span>: <span class="hljs-string">&#x27;&lt;%= theme.gitalk.repo %&gt;&#x27;</span>,</span></span><br><span class="language-javascript"><span class="language-xml">            <span class="hljs-attr">owner</span>: <span class="hljs-string">&#x27;&lt;%= theme.gitalk.owner %&gt;&#x27;</span>,</span></span><br><span class="language-javascript"><span class="language-xml">            <span class="hljs-attr">admin</span>: <span class="hljs-string">&#x27;&lt;%= theme.gitalk.admin %&gt;&#x27;</span>,</span></span><br><span class="language-javascript"><span class="language-xml">            <span class="hljs-attr">distractionFreeMode</span>: <span class="hljs-string">&#x27;&lt;%= theme.gitalk.on %&gt;&#x27;</span></span></span><br><span class="language-javascript"><span class="language-xml">        &#125;)</span></span><br><span class="language-javascript"><span class="language-xml">        gitalk.<span class="hljs-title function_">render</span>(<span class="hljs-string">&#x27;gitalk-container&#x27;</span>)</span></span><br><span class="language-javascript"><span class="language-xml">    </span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span></span><br><span class="language-xml">&lt;% &#125; %&gt;</span><br></code></pre></td></tr></table></figure><p>配置gittalk可能出现的两种错误可以参考<a href="https://blog.csdn.net/qq_38463737/article/details/120288329">Gridea 配置Gitalk 的异常：Error Bad credentials 和 Error Not Found</a></p><h2 id="5、加入博客加密"><a href="#5、加入博客加密" class="headerlink" title="5、加入博客加密"></a>5、加入博客加密</h2><p>下载encrypt插件</p><blockquote><p>npm install –save hexo-blog-encrypt</p></blockquote><p>在每个需要加密的文件头加入password字段</p><figure class="highlight node-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs node-repl">title: Hello World<br><span class="hljs-meta prompt_">...</span><br>password: 123456<br><span class="hljs-meta prompt_">...</span><br></code></pre></td></tr></table></figure><p>可以在博客根目录的配置文件_config.yml中加入整体的文本配置</p><figure class="highlight handlebars"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs handlebars"><span class="language-xml"># 安全</span><br><span class="language-xml">encrypt: # hexo-blog-encrypt</span><br><span class="language-xml">  abstract: 这里有东西被加密了，需要输入密码查看哦。</span><br><span class="language-xml">  message: 您好, 这里需要密码.</span><br><span class="language-xml">  tags:</span><br><span class="language-xml">  - &#123;name: tagName, password: 密码A&#125;</span><br><span class="language-xml">  - &#123;name: tagName, password: 密码B&#125;</span><br><span class="language-xml">  template: <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;hexo-blog-encrypt&quot;</span> <span class="hljs-attr">data-wpm</span>=<span class="hljs-string">&quot;</span></span></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">hbeWrongPassMessage</span>&#125;&#125;</span><span class="language-xml"><span class="hljs-tag"><span class="hljs-string">&quot;</span> <span class="hljs-attr">data-whm</span>=<span class="hljs-string">&quot;</span></span></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">hbeWrongHashMessage</span>&#125;&#125;</span><span class="language-xml"><span class="hljs-tag"><span class="hljs-string">&quot;</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;hbe-input-container&quot;</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;password&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;hbePass&quot;</span> <span class="hljs-attr">placeholder</span>=<span class="hljs-string">&quot;</span></span></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">hbeMessage</span>&#125;&#125;</span><span class="language-xml"><span class="hljs-tag"><span class="hljs-string">&quot;</span> /&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">label</span>&gt;</span></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">hbeMessage</span>&#125;&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;bottom-line&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;hbeData&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;hbeData&quot;</span> <span class="hljs-attr">data-hmacdigest</span>=<span class="hljs-string">&quot;</span></span></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">hbeHmacDigest</span>&#125;&#125;</span><span class="language-xml"><span class="hljs-tag"><span class="hljs-string">&quot;</span>&gt;</span></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">hbeEncryptedData</span>&#125;&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span></span><br><span class="language-xml">  wrong_pass_message: 抱歉, 这个密码看着不太对, 请再试试.</span><br><span class="language-xml">  wrong_hash_message: 抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.</span><br></code></pre></td></tr></table></figure><h2 id="6、加入类github活动日历"><a href="#6、加入类github活动日历" class="headerlink" title="6、加入类github活动日历"></a>6、加入类github活动日历</h2><blockquote><p>参考自<a href="https://cloud.tencent.com/developer/article/1597223">Hexo主题美化 | 给你的博客加上GITHUB日历云和分类雷达图 - 腾讯云开发者社区-腾讯云 (tencent.com)</a></p></blockquote><p>首先，将<a href="/download/echarts.min.js">echarts.min.js</a>下载到本地，并放到博客【…\blog\themes\cactus\source\js】位置，</p><p><img src="/post/94815387/image-20221029223233645.png" alt="image-20221029223233645"></p><p>然后在【…\blog\themes\cactus\layout_partial】位置中新建一个文件【post-calendar.ejs】，将以下代码放进去。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><code class="hljs ejs">&lt;div class=&quot;container archive-calendar&quot;&gt;<br>    &lt;div class=&quot;card&quot;&gt;<br>        &lt;div id=&quot;post-calendar&quot; class=&quot;card-content&quot;&gt;&lt;/div&gt;<br>    &lt;/div&gt;<br>&lt;/div&gt;<br><br>&lt;script type=&quot;text/javascript&quot; src=&quot;/js/echarts.min.js&quot;&gt;&lt;/script&gt;<br>&lt;script type=&quot;text/javascript&quot;&gt;<br>    let myChart = echarts.init(document.getElementById(&#x27;post-calendar&#x27;));<br><br>    &lt;%<br>    var nameMap = (config.language &amp;&amp; config.language.indexOf(&#x27;zh&#x27;) &gt;= 0) ? &#x27;cn&#x27; : &#x27;en&#x27;;<br>    var titleText = (config.language &amp;&amp; config.language.indexOf(&#x27;zh&#x27;) &gt;= 0) ? &#x27;文章日历&#x27; : &#x27;Post Calendar&#x27;;<br><br>    // calculate range.<br>    var startDate = moment().subtract(1, &#x27;years&#x27;);<br>    var endDate = moment();<br>    var rangeArr = &#x27;[&quot;&#x27; + startDate.format(&#x27;YYYY-MM-DD&#x27;) + &#x27;&quot;, &quot;&#x27; + endDate.format(&#x27;YYYY-MM-DD&#x27;) + &#x27;&quot;]&#x27;;<br><br>    // post and count map.<br>    var dateMap = new Map();<br>    site.posts.forEach(function (post) &#123;<br>        var date = post.date.format(&#x27;YYYY-MM-DD&#x27;);<br>        var count = dateMap.get(date);<br>        dateMap.set(date, count == null || count == undefined ? 1 : count + 1);<br>    &#125;);<br><br>    // loop the data for the current year, generating the number of post per day<br>    var i = 0;<br>    var datePosts = &#x27;[&#x27;;<br>    var dayTime = 3600 * 24 * 1000;<br>    for (var time = startDate; time &lt;= endDate; time += dayTime) &#123;<br>        var date = moment(time).format(&#x27;YYYY-MM-DD&#x27;);<br>        datePosts = (i === 0 ? datePosts + &#x27;[&quot;&#x27; : datePosts + &#x27;, [&quot;&#x27;) + date + &#x27;&quot;, &#x27;<br>                + (dateMap.has(date) ? dateMap.get(date) : 0) + &#x27;]&#x27;;<br>        i++;<br>    &#125;<br>    datePosts += &#x27;]&#x27;;<br>    %&gt;<br>    /*<br>title: &#123;<br>            top: 0,<br>            text: &#x27;&lt;%- titleText %&gt;&#x27;,<br>            left: &#x27;center&#x27;,<br>            textStyle: &#123;<br>                color: &#x27;#3C4858&#x27;<br>            &#125;<br>        &#125;,<br>   */<br>    let option = &#123;<br>        tooltip: &#123;<br>            padding: 10,<br>            backgroundColor: &#x27;#555&#x27;,<br>            borderColor: &#x27;#777&#x27;,<br>            borderWidth: 1,<br>            formatter: function (obj) &#123;<br>                var value = obj.value;<br>                return &#x27;&lt;div style=&quot;font-size: 14px;&quot;&gt;&#x27; + value[0] + &#x27;：&#x27; + value[1] + &#x27;&lt;/div&gt;&#x27;;<br>            &#125;<br>        &#125;,<br>        visualMap: &#123;<br>            show: true,<br>            showLabel: true,<br>            categories: [0, 1, 2, 3, 4],<br>            calculable: true,<br>            inRange: &#123;<br>                symbol: &#x27;rect&#x27;,<br>                color: [&#x27;#ebedf0&#x27;, &#x27;#c6e48b&#x27;, &#x27;#7bc96f&#x27;, &#x27;#239a3b&#x27;, &#x27;#196127&#x27;]<br>            &#125;,<br>            itemWidth: 12,<br>            itemHeight: 12,<br>            orient: &#x27;horizontal&#x27;,<br>            left: &#x27;center&#x27;,<br>            bottom: 0<br>        &#125;,<br>        calendar: [&#123;<br>            left: &#x27;center&#x27;,<br>            range: &lt;%- rangeArr %&gt;,<br>            cellSize: [13, 13],<br>            splitLine: &#123;<br>                show: false<br>            &#125;,<br>            itemStyle: &#123;<br>                color: &#x27;#196127&#x27;,<br>                borderColor: &#x27;#fff&#x27;,<br>                borderWidth: 2<br>            &#125;,<br>            yearLabel: &#123;<br>                show: true<br>            &#125;,<br>            monthLabel: &#123;<br>                nameMap: &#x27;&lt;%- nameMap %&gt;&#x27;,<br>                fontSize: 11<br>            &#125;,<br>            dayLabel: &#123;<br>                formatter: &#x27;&#123;start&#125;  1st&#x27;,<br>                nameMap: &#x27;&lt;%- nameMap %&gt;&#x27;,<br>                fontSize: 11<br>            &#125;<br>        &#125;],<br>        series: [&#123;<br>            type: &#x27;heatmap&#x27;,<br>            coordinateSystem: &#x27;calendar&#x27;,<br>            calendarIndex: 0,<br>            data: &lt;%- datePosts %&gt;<br>        &#125;]<br><br>    &#125;;<br>         <br>    myChart.setOption(option);<br>&lt;/script&gt;<br></code></pre></td></tr></table></figure><p>之后，找到【…\blog\themes\cactus\source\css】中的【style.styl】文件，这个文件是整个博客的总CSS样式。在文件末尾加入：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-comment">/*日历云*/</span><br><span class="hljs-selector-id">#post-calendar</span> &#123;<br>  <span class="hljs-attribute">width</span>: <span class="hljs-number">90%</span>;<br>  <span class="hljs-attribute">height</span>: <span class="hljs-number">180px</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>其中，日历的宽度和高度均可以自行调整。这样就调整好了整体的配置。最后，将配置语句放到博客的任何位置。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ejs">&lt;span class=&quot;h2&quot;&gt;<br>  &lt;%= __(&#x27;Post Calender&#x27;) %&gt;<br> &lt;%- partial(&#x27;_partial/post-calendar&#x27;) %&gt;<br>&lt;/span&gt;<br></code></pre></td></tr></table></figure><p>比如，我是将日历放到了主页，因此在主页的ejs文件【…\blog\themes\cactus\layout\index.ejs】中加入此语句。</p><p><img src="/post/94815387/image-20221029224006328.png" alt="image-20221029224006328"></p><p>这样就完成啦。</p><h2 id="7、添加附件下载功能"><a href="#7、添加附件下载功能" class="headerlink" title="7、添加附件下载功能"></a>7、添加附件下载功能</h2><p>hexo默认打开了文件的链接功能，只需找到blog的配置文件，搜索【post_asset_folder】，将开关都打开。</p><p><img src="/post/94815387/image-20221029225413450.png" alt="image-20221029225413450"></p><p>然后在【…\blog\source】下新建一个文件夹download，将文件都放进去。</p><p><img src="/post/94815387/image-20221029225504580.png" alt="image-20221029225504580"></p><p>在文件中使用</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">[<span class="hljs-string">点击下载</span>](<span class="hljs-link">/download/文件名称.格式</span>)<br></code></pre></td></tr></table></figure><p>hexo会直接引导到download的位置访问文件。</p><h2 id="8、添加文本复制批注"><a href="#8、添加文本复制批注" class="headerlink" title="8、添加文本复制批注"></a>8、添加文本复制批注</h2><blockquote><p>参考自CSDN<a href="https://blog.csdn.net/qq_33430083/article/details/105626840">新手如何给Hexo博客在复制时添加版权声明_只是学习学习的博客-CSDN博客</a></p></blockquote><p>直接在post对应的ejs文件中写入js监听，找到【…\blog\themes\cactus\layout\post.ejs】，在其末尾加入以下js语句</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs js">&lt;script&gt;<br>    <span class="hljs-keyword">function</span> <span class="hljs-title function_">setClipboardText</span>(<span class="hljs-params">event</span>)&#123;<br>        <span class="hljs-comment">// clipboardData 对象是为通过编辑菜单、快捷菜单和快捷键执行的编辑操作所保留的，也就是你复制或者剪切内容</span><br>        <span class="hljs-keyword">let</span> clipboardData = event.<span class="hljs-property">clipboardData</span> || <span class="hljs-variable language_">window</span>.<span class="hljs-property">clipboardData</span>;<br>        <span class="hljs-comment">// 如果未复制或者未剪切，则return出去</span><br>        <span class="hljs-keyword">if</span> (!clipboardData) &#123; <span class="hljs-keyword">return</span>; &#125;<br>        event.<span class="hljs-title function_">preventDefault</span>();<br>        <span class="hljs-comment">// Selection 对象，表示用户选择的文本范围或光标的当前位置。</span><br>        <span class="hljs-comment">//     声明一个变量接收 -- 用户输入的剪切或者复制的文本转化为字符串</span><br>        <span class="hljs-keyword">let</span> text = <span class="hljs-variable language_">window</span>.<span class="hljs-title function_">getSelection</span>().<span class="hljs-title function_">toString</span>();<br>    <br>        <span class="hljs-keyword">if</span> (text) &#123;<br>            <span class="hljs-comment">// 如果文本存在则先取消文本默认事件</span><br>            event.<span class="hljs-title function_">preventDefault</span>();<br>            <span class="hljs-comment">// 通过调用常clipboardData对象的 setData(format, data) 方法；来设置相关文本</span><br>            <span class="hljs-comment">// format: 一个DOMString 表示要添加到 drag object的拖动数据的类型。</span><br>            <span class="hljs-comment">// data: 一个 DOMString表示要添加到 drag object的数据。</span><br>            <span class="hljs-keyword">var</span> copyright = <span class="hljs-string">&#x27;\n\n&#x27;</span><br>            + <span class="hljs-string">&#x27;——————————————————————&#x27;</span><br>            + <span class="hljs-string">&#x27;\n版权声明：本文为「Ywj226」的原创文章，遵循CC BY-NC-ND 4.0版权协议，转载请附上原文出处链接及本声明。&#x27;</span><br>            + <span class="hljs-string">&#x27;\n原文链接：&lt;%= page.permalink %&gt;&#x27;</span><br>    <br>            clipboardData.<span class="hljs-title function_">setData</span>(<span class="hljs-string">&#x27;text/plain&#x27;</span>, text + copyright);<br>    <br>        &#125;<br>    &#125;;<br>    <span class="hljs-keyword">var</span> contents = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementsByClassName</span>(<span class="hljs-string">&quot;content&quot;</span>);<br>    <span class="hljs-comment">// 监听文章内容的copy事件</span><br>    contents[<span class="hljs-number">0</span>].<span class="hljs-title function_">addEventListener</span>(<span class="hljs-string">&#x27;copy&#x27;</span>,<span class="hljs-keyword">function</span>(<span class="hljs-params">e</span>)&#123;<br>        <span class="hljs-title function_">setClipboardText</span>(e);<br>    &#125;);<br>&lt;/script&gt;<br></code></pre></td></tr></table></figure><p>重新启动即可。</p><h2 id="9、添加追番页面（√）"><a href="#9、添加追番页面（√）" class="headerlink" title="9、添加追番页面（√）"></a>9、添加追番页面（√）</h2><blockquote><p>参考自<a href="https://opensourcelibs.com/lib/hexo-bilibili-bangumi">Hexo Bilibili Bangumi - hexo 番剧页面插件，可选数据源：Bilibili, Bangumi - (hexo-bilibili-bangumi) (opensourcelibs.com)</a></p></blockquote><p>首先设置一个界面用来展示追番界面，这里不再赘述</p><p>然后，使用npm指令安装插件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs git">npm install hexo-bilibili-bangumi --save<br></code></pre></td></tr></table></figure><p>之后在博客配置文件中加入以下配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">bangumi:</span> <span class="hljs-comment"># 追番设置</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">path:</span> <span class="hljs-string">/bilibili/</span><br>  <span class="hljs-attr">vmid:</span> <span class="hljs-comment"># bilibili的uid</span><br>  <span class="hljs-attr">title:</span> <span class="hljs-string">&#x27;追番&#x27;</span><br>  <span class="hljs-attr">quote:</span> <span class="hljs-string">&#x27;生命不息，追番不止！&#x27;</span><br>  <span class="hljs-attr">show:</span> <span class="hljs-number">1</span><br>  <span class="hljs-attr">lazyload:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">loading:</span><br>  <span class="hljs-attr">metaColor:</span><br>  <span class="hljs-attr">color:</span><br>  <span class="hljs-attr">webp:</span><br>  <span class="hljs-attr">progress:</span><br>  <span class="hljs-attr">extra_options:</span><br>    <span class="hljs-attr">key:</span> <span class="hljs-string">value</span><br>    <br><span class="hljs-attr">cinema:</span> <span class="hljs-comment"># 追剧设置</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">path:</span><br>  <span class="hljs-attr">vmid:</span><br>  <span class="hljs-attr">title:</span> <span class="hljs-string">&#x27;追剧列表&#x27;</span><br>  <span class="hljs-attr">quote:</span> <span class="hljs-string">&#x27;生命不息，追剧不止！&#x27;</span><br>  <span class="hljs-attr">show:</span> <span class="hljs-number">1</span><br>  <span class="hljs-attr">lazyload:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">loading:</span><br>  <span class="hljs-attr">metaColor:</span><br>  <span class="hljs-attr">color:</span><br>  <span class="hljs-attr">webp:</span><br>  <span class="hljs-attr">progress:</span><br>  <span class="hljs-attr">extra_options:</span><br>    <span class="hljs-attr">key:</span> <span class="hljs-string">value</span><br></code></pre></td></tr></table></figure><p>在每次进行<code>hexo clean</code>与<code>hexo g</code>之后，再运行一步<code>hexo bangumi -u</code>来更新番剧数据即可。</p><p>若需要加入在b站上未上线的番剧，可以自行在【…\blog\source_data】文件夹中新建一个【extra_bangumis.json】文件，然后以json格式将数据自行写入。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;watchedExtra&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>    <span class="hljs-punctuation">&#123;</span><br>      <span class="hljs-attr">&quot;title&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;缘之空&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;番剧&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;area&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;日本&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;cover&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;https://cdn.jsdelivr.net/gh/mmdjiji/bangumis@main/Yosuga-no-Sora/cover.jpg&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;totalCount&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;全12话&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;id&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;follow&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;不可用&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;view&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;不可用&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;danmaku&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;不可用&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;coin&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;不可用&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;score&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;不可用&quot;</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">&quot;des&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;远离都市的田园小镇，奥木染。春日野悠带着妹妹穹，来到了这座城镇。坐落在这里的是，儿时暑假经常造访的充满回忆的已故祖父的家。双亲因意外事故而丧生，变得无依无靠...&quot;</span><br>    <span class="hljs-punctuation">&#125;</span><br>  <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>这样就完成了。</p>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> cactus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python课程学习笔记</title>
      <link href="/post/614876e7.html"/>
      <url>/post/614876e7.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><h3 id="基础元素"><a href="#基础元素" class="headerlink" title="基础元素"></a>基础元素</h3><p>python是一种<strong>弱类型</strong>的语言，变量类型由值的类型决定。</p><p>对于赋值计算，会先计算出赋值号右边所有的值，然后再进行赋值，因此x,y&#x3D;y,x不会出现数据污染的问题。</p><h3 id="Number类型"><a href="#Number类型" class="headerlink" title="Number类型"></a>Number类型</h3><p>python有int、float、complex共三种不同的数字类型。</p><p>转义字符的意义在于将控制信息与数值信息区分开。</p><h3 id="String类型"><a href="#String类型" class="headerlink" title="String类型"></a>String类型</h3><p>切片操作：左闭右开+左右idx（逆序用负数）+不可修改性</p><h3 id="List（列表）"><a href="#List（列表）" class="headerlink" title="List（列表）"></a>List（列表）</h3><p>List中可以存储各种类型的数据，甚至可以嵌套一个另外的list。支持切片操作。可以修改list中的单个或多个数据</p><h3 id="Tuple（元素）"><a href="#Tuple（元素）" class="headerlink" title="Tuple（元素）"></a>Tuple（元素）</h3><p>与列表的表达形式基本一致，但内容不可修改。</p><h3 id="Set（集合）"><a href="#Set（集合）" class="headerlink" title="Set（集合）"></a>Set（集合）</h3><p>元素无序，不可重复，不可hash访问</p><p>创建集合必须有set关键词，如set{}；</p><h3 id="Dictionray（字典）"><a href="#Dictionray（字典）" class="headerlink" title="Dictionray（字典）"></a>Dictionray（字典）</h3><p>key+value，key不可重复且可hash</p><h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><h4 id="不定长参数："><a href="#不定长参数：" class="headerlink" title="不定长参数："></a>不定长参数：</h4><p>*args代表位置不定长参数；**agrs代表关键词不定长参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Stu</span>(<span class="hljs-params">name,*args</span>):<br>    <span class="hljs-built_in">print</span>(name,args)<br>Stu(<span class="hljs-string">&quot;a&quot;</span>,<span class="hljs-string">&quot;b&quot;</span>,<span class="hljs-string">&quot;c&quot;</span>)<br><span class="hljs-comment">#输出a (&#x27;b&#x27;, &#x27;c&#x27;)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Stu</span>(<span class="hljs-params">name,**args</span>):<br>    <span class="hljs-built_in">print</span>(name,args)<br>Stu(<span class="hljs-string">&quot;a&quot;</span>,b=<span class="hljs-string">&quot;b&quot;</span>,c=<span class="hljs-string">&quot;c&quot;</span>)<br><span class="hljs-comment">#输出a &#123;&#x27;b&#x27;: &#x27;b&#x27;, &#x27;c&#x27;: &#x27;c&#x27;&#125;</span><br></code></pre></td></tr></table></figure><h3 id="拆分参数"><a href="#拆分参数" class="headerlink" title="拆分参数"></a>拆分参数</h3><p>使用星号进行拆分，列表和元组类型加一个，字典类型加两个，其意义可以大致当成C语言中的指针来理解。列表、元组拆成位置参数；字典拆成关键词参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sum</span>(<span class="hljs-params">*args</span>):<br>    res=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> args:<br>        res+=i<br>    <span class="hljs-built_in">print</span>(res)<br><br>ls=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]<br><span class="hljs-built_in">sum</span>(ls)<br><span class="hljs-comment">#结果报错</span><br><span class="hljs-built_in">sum</span>(*ls)<br><span class="hljs-comment">#结果为1+2+3+4+5=15，将列表进行了拆分。</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">prints</span>(<span class="hljs-params">**args</span>):<br>    <span class="hljs-built_in">print</span>(args)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> args.keys():<br>        <span class="hljs-built_in">print</span>(i+<span class="hljs-string">&quot; &quot;</span>+args.get(i))<br><br><span class="hljs-built_in">dict</span>=&#123;<span class="hljs-string">&quot;A&quot;</span>:<span class="hljs-number">1</span>,<span class="hljs-string">&quot;B&quot;</span>:<span class="hljs-number">2</span>&#125;<br>prints(**<span class="hljs-built_in">dict</span>)<br><span class="hljs-comment">#结果为&#123;&#x27;A&#x27;: 1, &#x27;B&#x27;: 2&#125;</span><br><span class="hljs-comment">#1</span><br><span class="hljs-comment">#2</span><br></code></pre></td></tr></table></figure><h2 id="全局变量-name"><a href="#全局变量-name" class="headerlink" title="全局变量 name"></a>全局变量 <strong>name</strong></h2><p>_ _ name_ _ 是当前模块名，当模块被直接运行时模块名为 _ _ main_ _ 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#确定程序入口</span><br><span class="hljs-keyword">if</span> __name__ == __main__:<br>    <span class="hljs-comment">#操作</span><br></code></pre></td></tr></table></figure><h2 id="猴子补丁"><a href="#猴子补丁" class="headerlink" title="猴子补丁"></a>猴子补丁</h2><p>指在运行时动态替换已有的代码，而不需要修改源码</p><p><img src="/post/614876e7/image-20221008182137458.png" alt="image-20221008182137458"></p><h3 id="作用域"><a href="#作用域" class="headerlink" title="作用域"></a>作用域</h3><p>函数局部域和函数外的全局域</p><h3 id="global关键字和nonlocal关键字"><a href="#global关键字和nonlocal关键字" class="headerlink" title="global关键字和nonlocal关键字"></a>global关键字和nonlocal关键字</h3><p>global：在函数内部使用函数外的全局变量。</p><p>nonlocal：在嵌套函数中，内部函数使用外部函数定义的局部变量。</p><h3 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h3><p>将函数作为参数的一种函数。</p><img src="/post/image-20221008184104048.png" alt="image-20221008184104048" style="zoom:50%;"><h3 id="lambda"><a href="#lambda" class="headerlink" title="lambda"></a>lambda</h3><p>格式：lambda [参数1[参数2,参数3,…,参数n]]: 表达式</p><p>例子：</p><p><img src="/post/614876e7/image-20221008184552667.png" alt="image-20221008184552667"></p><h3 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h3><p>在修饰的函数外部进行一些自定义修饰，丰富功能。</p><p>修饰示例：</p><p>注意：func函数加上不定长参数表示修饰函数在修饰中的具体位置，在本例中是f函数的位置</p><p><img src="/post/614876e7/image-20221008190835605.png" alt="image-20221008190835605"></p><p><img src="/post/614876e7/image-20221008190841723.png" alt="image-20221008190841723"></p><p>装饰器修饰的是函数定义，调用时不需要进行声明。声明方法是“@装饰器函数名“</p><p><img src="/post/614876e7/image-20221008190851626.png" alt="image-20221008190851626"></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络与深度学习实验一——实验基础</title>
      <link href="/post/37103f0b.html"/>
      <url>/post/37103f0b.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="【注】"><a href="#【注】" class="headerlink" title="【注】"></a>【注】</h1><p>本实验及之后的实验都将从PaddlePaddle和Pytorch两个版本进行，其中paddlepaddle是邱锡鹏老师官方的实验工具，pytorch是我为了能尽快上手而尝试的工具（×）</p><p>实验文档的大部分文字都来源于邱锡鹏老师的网课练习notebook中，部分会进行补充或精简</p><h2 id="本节实验介绍"><a href="#本节实验介绍" class="headerlink" title="本节实验介绍"></a>本节实验介绍</h2><p>本次实验是有关于实验基础知识的讲解，包括张量、算子和数据集等信息</p><ul><li><b>张量</b>：深度学习中表示和存储数据的主要形式。</li><li><b>算子</b>：构建神经网络模型的基础组件。每个算子有前向和反向计算过程，前向计算对应一个数学函数，而反向计算对应这个数学函数的梯度计算。有了算子，我们就可以很方便地通过算子来搭建复杂的神经网络模型，而不需要手工计算梯度。</li></ul><h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2><p>在深度学习的实践中，我们通常使用向量或矩阵运算来提高计算效率。比如$w_1x_1 + w_2 x_2 +\cdots +w_N x_N$的计算可以用$w^\top x$来代替，这样可以充分利用计算机的并行计算能力，特别是利用GPU来实现高效矩阵运算。在深度学习中，以张量来存储与表示数据，张量是矩阵的扩展与延伸，可以认为是<b>高阶的矩阵</b>。1阶张量为向量，2阶张量为矩阵。如果你对Numpy熟悉，那么张量是类似于Numpy的多维数组(ndarray)的概念，可以具有任意多的维度。</p><p><img src="/post/37103f0b/image-20221012160709716.png" alt="image-20221012160709716"></p><p>单个张量的数据类型必须是一致的，这个数据类型可以是布尔型、整型、复数等，因此需要给张量定义一个数据类型<b>(dtype)</b>来表示其数据类型。</p><h3 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建多维Tensor</span><br><span class="hljs-comment"># paddlepaddle</span><br>ndim_n_Tensor = paddle.to_tensor([[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>],<br>                                   [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>]],<br>                                  [[<span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>],<br>                                   [<span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>]]])<br><span class="hljs-comment"># pytorch</span><br>ndim_n_Tensor = torch.tensor([[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>],<br>                                   [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>]],<br>                                  [[<span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>],<br>                                   [<span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>]]])<br></code></pre></td></tr></table></figure><p><img src="/post/37103f0b/image-20221012212400850.png" alt="image-20221012212400850"></p><p>需要注意的是，<b>张量在任何一个维度上的元素数量必须相等。</b></p><h3 id="指定形状（torch和paddle的API用法一致）"><a href="#指定形状（torch和paddle的API用法一致）" class="headerlink" title="指定形状（torch和paddle的API用法一致）"></a>指定形状（torch和paddle的API用法一致）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">m, n = <span class="hljs-number">2</span>, <span class="hljs-number">3</span><br><br><span class="hljs-comment"># 使用zeros创建数据全为0，形状为[m, n]的Tensor</span><br>zeros_paddle = paddle.zeros([m, n])<br>zeros_torch = torch.zeros([m,n])<br><span class="hljs-comment"># 使用ones创建数据全为1，形状为[m, n]的Tensor</span><br>ones_paddle = paddle.ones([m, n])<br>ones_torch = torch.ones([m,n])<br><span class="hljs-comment"># 使用full创建数据全为指定值，形状为[m, n]的Tensor，这里我们指定数据为10</span><br>full_paddle = paddle.full([m, n], <span class="hljs-number">10</span>)<br>full_torch = torch.full([m,n],<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 使用arange创建以步长step均匀分隔数值区间[start, end)的一维Tensor</span><br>arange_paddle = paddle.arange(start=<span class="hljs-number">1</span>, end=<span class="hljs-number">5</span>, step=<span class="hljs-number">1</span>)<br>arrage_torch = torch.arange(start=<span class="hljs-number">0</span>, end, step=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 使用linspace创建以元素个数num均匀分隔数值区间[start, stop]的Tensor</span><br>linspace_paddle = paddle.linspace(start=<span class="hljs-number">1</span>, stop=<span class="hljs-number">5</span>, num=<span class="hljs-number">5</span>)<br>linspace_torch = torch.linspace(start=<span class="hljs-number">0</span>, end=<span class="hljs-number">10</span>, step=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p>张量有如下属性：</p><ul><li><code>ndim</code>：张量的维度，例如向量的维度为1，矩阵的维度为2。</li><li><code>shape</code>： 张量每个维度上元素的数量。</li><li><code>shape[n]</code>：张量第nn维的大小。第nn维也称为轴（axis）。</li><li><code>size</code>：张量中全部元素的个数。</li></ul><p>以torch的张量属性为例，paddle与torch不同的用法另外标注：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">ndim_4_Tensor = torch.ones([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<br>ndim_4_p = paddle.ones([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Number of dimensions:&quot;</span>, ndim_4_Tensor.ndim)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Shape of Tensor:&quot;</span>, ndim_4_Tensor.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Elements number along axis 0 of Tensor:&quot;</span>, ndim_4_Tensor.shape[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Elements number along the last axis of Tensor:&quot;</span>, ndim_4_Tensor.shape[-<span class="hljs-number">1</span>])<br><span class="hljs-comment">#torch:numel()函数计算元素数量</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Number of elements in Torch tensor: &#x27;</span>, ndim_4_Tensor.numel())<br><span class="hljs-comment">#paddle:size属性计算元素数量</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Number of elements in Paddle tensor: &quot;</span>, ndim_4_p.size)<br></code></pre></td></tr></table></figure><p><img src="/post/37103f0b/image-20221012235143510.png" alt="image-20221012235143510"></p><p>张量的形状也可以通过reshape函数来进行变换，变换后数据本身和数据的相对顺序都不会有变化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#paddle用法一致</span><br>ndim_3_Torch = torch.tensor([[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>],<br>                                   [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>]],<br>                                  [[<span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>],<br>                                   [<span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>]],<br>                                  [[<span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>, <span class="hljs-number">24</span>, <span class="hljs-number">25</span>],<br>                                   [<span class="hljs-number">26</span>, <span class="hljs-number">27</span>, <span class="hljs-number">28</span>, <span class="hljs-number">29</span>, <span class="hljs-number">30</span>]]])<br>reshape_torch = torch.reshape(ndim_3_Torch,[<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;After reshape:&quot;</span>, reshape_Tensor)<br></code></pre></td></tr></table></figure><p><img src="/post/37103f0b/image-20221013001623336.png" alt="image-20221013001623336"></p><p>注意这里的修改前和修改后的shape_size应该是一样的，否则会报错</p><p><img src="/post/37103f0b/image-20221013001934299.png" alt="image-20221013001934299"></p><p>使用reshape时有一些简单技巧：</p><ol><li>reshape中有且仅有一维可以为-1，表示按照原有size直接补齐</li><li>reshape中可以有多维为0，表示继承原有张量在当前维度的元素个数</li></ol><p>除了reshape变换外，还可以使用unsqueeze函数在原有张量基础上插入尺寸为1的维度，其中paddlepaddle支持多个维度的插入，而pytorch的dim参数为整型，限制只能有一个维度的插入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">ones_Torch = torch.ones([<span class="hljs-number">5</span>, <span class="hljs-number">10</span>])<br>new_Torch1 = torch.unsqueeze(ones_Tensor, <span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;new Tensor 1 shape: &#x27;</span>, new_Tensor1.shape)<br>new_Torch2 = torch.unsqueeze(ones_Tensor, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;new Tensor 2 shape: &#x27;</span>, new_Tensor2.shape)<br>ones_Paddle = paddle.ones([<span class="hljs-number">5</span>,<span class="hljs-number">10</span>])<br>new_Paddle1 = paddle.unsqueeze(ones_Paddle,[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;new Tensor 3 shape: &#x27;</span>,new_Paddle1.shape)<br></code></pre></td></tr></table></figure><p><img src="/post/37103f0b/image-20221013003536929.png" alt="image-20221013003536929"></p><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>dtype参数用来查看张量的数据类型，支持类型支持bool、float16、float32、float64、uint8、int8、int16、int32、int64和复数类型等数据。其中通过Python元素创建的数据<b>可以使用dtype指定类型</b>，默认整型为int64，浮点类型为float32；通过Numpy数组创建的张量，则与其原来的<b>数据类型保持相同</b>。</p><p>paddle的to_tensor函数和torch的as_tensor函数可以实现从其他类型（包括Numpy）到tensor的转换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用paddle.to_tensor通过已知数据来创建一个Tensor</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Tensor dtype from Python integers:&quot;</span>, torch.as_tensor(<span class="hljs-number">1</span>).dtype)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Tensor dtype from Python floating point:&quot;</span>, paddle.to_tensor(<span class="hljs-number">1.0</span>).dtype)<br></code></pre></td></tr></table></figure><p>如果想要修改数据类型，paddle使用cast函数，torch使用to方法可以实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义dtype为float32的Tensor</span><br>float32_Tensor_Torch = torch.as_tensor(<span class="hljs-number">1.0</span>)<br>float32_Tensor_Paddle = paddle.to_tensor(<span class="hljs-number">1.0</span>)<br><span class="hljs-comment"># paddle.cast和torch的to可以将输入数据的数据类型转换为指定的dtype并输出。支持输出和输入数据类型相同。</span><br><span class="hljs-comment"># 注意两者的使用区别</span><br>int64_Tensor_Paddle = paddle.cast(float32_Tensor_Paddle,dtype=<span class="hljs-string">&#x27;int64&#x27;</span>)<br>int64_Tensor_Torch = float32_Tensor_Torch.to(torch.int64)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Tensor after cast to int64:&quot;</span>, int64_Tensor_Paddle.dtype,int64_Tensor_Torch.dtype)<br><br></code></pre></td></tr></table></figure><p><img src="/post/37103f0b/image-20221013152808890.png" alt="image-20221013152808890"></p><h3 id="设备转换"><a href="#设备转换" class="headerlink" title="设备转换"></a>设备转换</h3><p>在paddle中共有三种可使用的设备类型，使用place参数进行设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建CPU上的Tensor</span><br>cpu_Tensor = paddle.to_tensor(<span class="hljs-number">1</span>, place=paddle.CPUPlace())<br><span class="hljs-comment"># 通过Tensor.place查看张量所在设备位置</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;cpu Tensor: &#x27;</span>, cpu_Tensor.place)<br><span class="hljs-comment"># 创建GPU上的Tensor</span><br>gpu_Tensor = paddle.to_tensor(<span class="hljs-number">1</span>, place=paddle.CUDAPlace(<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;gpu Tensor: &#x27;</span>, gpu_Tensor.place)<br><span class="hljs-comment"># 创建固定内存上的Tensor，固定内存的读写效率会更高，但存储空间的消耗也会更多</span><br>pin_memory_Tensor = paddle.to_tensor(<span class="hljs-number">1</span>, place=paddle.CUDAPinnedPlace())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;pin memory Tensor: &#x27;</span>, pin_memory_Tensor.place)<br></code></pre></td></tr></table></figure><p>在torch中需要使用device方法设置具体的设备对象，<b>torch.device()</b>方法是在torch中使用频率十分高的一种方法。torch.device代表将torch.tensor分配到的设备的对象。torch.device可以采用字符串和编号的参数定义设备</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#字符串方法</span><br>torch.device(<span class="hljs-string">&#x27;cuda:0&#x27;</span>)<br>torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>)<br>torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>)  <span class="hljs-comment"># current cuda device</span><br><span class="hljs-comment">#字符串+编号方法</span><br><span class="hljs-comment">#0号显卡</span><br>torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>, <span class="hljs-number">0</span>)<br>torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h3 id="访问张量"><a href="#访问张量" class="headerlink" title="访问张量"></a>访问张量</h3><p>paddle和torch都支持python和Numpy的索引和切片操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># torch.as_tensor定义1个一维Tensor</span><br>ndim_1_Tensor = torch.as_tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>])<br><span class="hljs-comment"># paddle.to_tensor定义1个一维Tensor</span><br>ndim_1_Tensor = paddle.to_tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Origin Tensor:&quot;</span>, ndim_1_Tensor)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;First element:&quot;</span>, ndim_1_Tensor[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Last element:&quot;</span>, ndim_1_Tensor[-<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;All element:&quot;</span>, ndim_1_Tensor[:])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Before 3:&quot;</span>, ndim_1_Tensor[:<span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Interval of 3:&quot;</span>, ndim_1_Tensor[::<span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Reverse:&quot;</span>, ndim_1_Tensor[::-<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>二维的张量同样可以采用索引或切片访问与修改，其中每个参数对应其中的一维。以torch为例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#%% torch为例，paddle和torch的操作一致</span><br><span class="hljs-comment"># 定义1个二维Tensor</span><br>ndim_2_Tensor = torch.as_tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                                  [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>],<br>                                  [<span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Origin Tensor:&quot;</span>, ndim_2_Tensor)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;First row:&quot;</span>, ndim_2_Tensor[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;First row:&quot;</span>, ndim_2_Tensor[<span class="hljs-number">0</span>, :])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;First column:&quot;</span>, ndim_2_Tensor[:, <span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Last column:&quot;</span>, ndim_2_Tensor[:, -<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;All element:&quot;</span>, ndim_2_Tensor[:])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;First row and second column:&quot;</span>, ndim_2_Tensor[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br><br>ndim_2_Tensor[<span class="hljs-number">0</span>:<span class="hljs-number">2</span>,<span class="hljs-number">2</span>:] = <span class="hljs-number">100</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;After Change: &quot;</span>,ndim_2_Tensor)<br></code></pre></td></tr></table></figure><p><img src="/post/37103f0b/image-20221014143213976.png" alt="image-20221014143213976"></p><p>【提醒】<b>慎重</b>通过索引或切片操作来修改张量，此操作仅会<b>原地修改该张量的数值，且原值不会被保存。</b>如果被修改的张量参与梯度计算，将仅会使用修改后的数值，这可能会给梯度计算引入风险。</p><h3 id="张量运算"><a href="#张量运算" class="headerlink" title="张量运算"></a>张量运算</h3><p>在这里我们更加推荐使用数学函数进行计算，这里给出比较常用的运算函数，更多请见<a href="https://pytorch.org/docs/stable/tensors.html">pytorch-Tensor操作文档</a>和<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html">Paddle官方文档</a></p><p>数值计算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#加减乘除运算</span><br>x.add(y)               <span class="hljs-comment"># 逐元素加</span><br>x.subtract(y)         <span class="hljs-comment"># 逐元素减</span><br>x.multiply(y)         <span class="hljs-comment"># 逐元素乘（积）</span><br>x.divide(y)           <span class="hljs-comment"># 逐元素除</span><br>x.mod(y)              <span class="hljs-comment"># 逐元素除并取余</span><br>x.<span class="hljs-built_in">pow</span>(y)              <span class="hljs-comment"># 逐元素幂</span><br><span class="hljs-comment">#取整与约分</span><br>x.<span class="hljs-built_in">abs</span>()                       <span class="hljs-comment"># 逐元素取绝对值</span><br>x.ceil()                      <span class="hljs-comment"># 逐元素向上取整</span><br>x.floor()                     <span class="hljs-comment"># 逐元素向下取整</span><br>x.<span class="hljs-built_in">round</span>()                     <span class="hljs-comment"># 逐元素四舍五入</span><br><span class="hljs-comment">#计算函数</span><br>x.exp()                       <span class="hljs-comment"># 逐元素计算自然常数为底的指数</span><br>x.log()                       <span class="hljs-comment"># 逐元素计算x的自然对数</span><br>x.reciprocal()                <span class="hljs-comment"># 逐元素求倒数</span><br>x.square()                    <span class="hljs-comment"># 逐元素计算平方</span><br>x.sqrt()                      <span class="hljs-comment"># 逐元素计算平方根</span><br>x.sin()                       <span class="hljs-comment"># 逐元素计算正弦</span><br>x.cos()                       <span class="hljs-comment"># 逐元素计算余弦</span><br><span class="hljs-comment">#求和与比较</span><br>x.<span class="hljs-built_in">max</span>()                       <span class="hljs-comment"># 指定维度上元素最大值，默认为全部维度</span><br>x.<span class="hljs-built_in">min</span>()                       <span class="hljs-comment"># 指定维度上元素最小值，默认为全部维度</span><br>x.prod()                      <span class="hljs-comment"># 指定维度上元素累乘，默认为全部维度</span><br>x.<span class="hljs-built_in">sum</span>()      <span class="hljs-comment"># 指定维度上元素的和，默认为全部维度</span><br></code></pre></td></tr></table></figure><p>逻辑计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">x.isfinite()                  <span class="hljs-comment"># 判断Tensor中元素是否是有限的数字，即不包括inf与nan</span><br>x.equal(y)                    <span class="hljs-comment"># 判断两个Tensor的每个元素是否相等，并返回形状相同的布尔类Tensor</span><br>x.not_equal(y)                <span class="hljs-comment"># 判断两个Tensor的每个元素是否不相等</span><br>x.less(y)                <span class="hljs-comment"># 判断Tensor x的元素是否小于Tensor y的对应元素</span><br>x.less_equal(y)               <span class="hljs-comment"># 判断Tensor x的元素是否小于或等于Tensor y的对应元素</span><br>x.greater(y)             <span class="hljs-comment"># 判断Tensor x的元素是否大于Tensor y的对应元素</span><br>x.greater_equal(y)            <span class="hljs-comment"># 判断Tensor x的元素是否大于或等于Tensor y的对应元素</span><br>x.allclose(y)                 <span class="hljs-comment"># 判断两个Tensor的全部元素是否接近</span><br></code></pre></td></tr></table></figure><p><b>矩阵运算</b></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x.t()                         <span class="hljs-comment"># 矩阵转置</span><br>x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>)           <span class="hljs-comment"># 交换第 0 维与第 1 维的顺序</span><br>x.norm(<span class="hljs-string">&#x27;fro&#x27;</span>)                 <span class="hljs-comment"># 矩阵的弗罗贝尼乌斯范数</span><br>x.dist(y, p=<span class="hljs-number">2</span>)                <span class="hljs-comment"># 矩阵（x-y）的2范数</span><br>x.matmul(y)                   <span class="hljs-comment"># 矩阵乘法</span><br></code></pre></td></tr></table></figure><h3 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h3><p>pytorch和paddle中都支持使用广播机制对两个维度不同的矩阵进行计算，属于数组的广播机制。通常来讲，如果有一个形状较小和一个形状较大的张量，会希望多次使用较小的张量来对较大的张量执行某些操作，看起来像是形状较小的张量首先被扩展到和较大的张量形状一致，然后再做运算。</p><p>广播的规则：</p><ol><li>每个张量至少为一维</li><li>从后往前比较张量的形状，当前维度的大小要么相等，要么其中一个等于1，要么其中一个不存在。</li></ol><p>广播的计算规则：</p><ol><li>如果两个张量shape的长度不一致，那么需要在较小长度的shape前添加1，直到两个张量的形状长度相等</li><li>保证两个张量形状相等之后，每个维度上的结果维度就是当前维度上较大的那个。</li></ol><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 当两个Tensor的形状一致时，可以广播</span><br>x = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br>y = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br>z = x + y<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;broadcasting with two same shape tensor: &#x27;</span>, z.shape)<br><br>x = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>))<br>y = torch.ones((<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>))<br><span class="hljs-string">&#x27;&#x27;&#x27;首先补足y = shape(1,3,4,1)</span><br><span class="hljs-string"> x = shape(2,3,1,5)</span><br><span class="hljs-string">   按位进行合并，对应位上或者相等，或者有一位为1可以直接合并</span><br><span class="hljs-string">   1 cpr 2 --&gt; 2</span><br><span class="hljs-string">   3 cpr 3 --&gt; 3</span><br><span class="hljs-string">   4 cpr 1 --&gt; 4</span><br><span class="hljs-string">   1 cpr 5 --&gt; 5</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>z = x + y<br><span class="hljs-comment">#z = shape(2,3,4,5)</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;broadcasting with two different shape tensor:&#x27;</span>, z.shape)<br></code></pre></td></tr></table></figure><p>矩阵乘法matmul中也用到了广播的机制，其将矩阵看作一个对象单位，和数据广播的数值处在同一等级，然后进行广播计算，具体的</p><ul><li>如果两个张量均为一维，则获得点积结果。</li><li>如果两个张量都是二维的，则获得矩阵与矩阵的乘积。</li><li>如果张量x是一维，y是二维，则将x的shape前面补一维变成[1, D]，与y进行矩阵相乘后再删除前置尺寸。</li><li>如果张量x是二维，y是一维，则获得矩阵与向量的乘积。</li><li>如果两个张量都是N维张量（N &gt; 2），则根据广播规则广播非矩阵维度（除最后两个维度外其余维度）。比如：如果输入x是形状为[j,1,n,m]的张量，另一个y是[k,m,p]的张量，则输出张量的形状为[j,k,n,p]。</li></ul><h3 id="原位操作"><a href="#原位操作" class="headerlink" title="原位操作"></a>原位操作</h3><p>在torch和paddle中，计算函数的返回结果都是创一个新的张量来存储，而不会形象原有的张量，这种处理方式称为”非原位“，部分函数支持通过在函数后加一个下划线来实现原位操作，如x.add(y) –&gt; x.add_(y)</p><h2 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h2><p>基于深度学习的前向传播与反向传播两个流程，从$x$到$y$的计算看作一个前向计算过程。前向的传播流程$y&#x3D;f_L(\cdots f_2(f_1(x)))$，则$f_l(⋅)$称为前向函数；而神经网络的参数学习需要找到函数对所有参数的偏导数。</p><p>依据链式法则：$\begin{aligned}\frac{\partial y}{\partial \theta_l} &amp;&#x3D; {\frac{\partial f_l}{\partial \theta_l}} \frac{\partial y}{\partial f_l} \&amp;&#x3D; \frac{\partial f_l}{\partial \theta_l} \frac{\partial f_{l+1}}{\partial f_l} \cdots \frac{\partial f_L}{\partial f_{L-1}} .\end{aligned}$，一种比较高效的方法就是递归计算每个偏导，反向计算。令$\delta_l\triangleq \frac{\partial y}{\partial f_l}$，则有$\delta_{l-1} &#x3D; \frac{\partial f_l}{\partial f_{l-1}} \delta_{l}.$，则从后往前可以计算出所有的偏导。</p><p>如果我们实现每个基础函数的前向函数和反向函数，就可以非常方便地通过这些基础函数组合出复杂函数，并通过链式法则反向计算复杂函数的偏导数。 在深度学习框架中，这些基本函数的实现称为算子(Operator，Op)。有了算子，就可以像搭积木一样构建复杂的模型。</p><h3 id="算子定义"><a href="#算子定义" class="headerlink" title="算子定义"></a>算子定义</h3><p>算子是构建复杂机器学习模型的基础组件，包含一个函数$f(x)$的前向函数和反向函数。为了可以更便捷地进行算子组合，</p><p>本书中定义算子Op的接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Op</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">pass</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-keyword">return</span> self.forward(inputs)<br><br>    <span class="hljs-comment"># 前向函数</span><br>    <span class="hljs-comment"># 输入：张量inputs</span><br>    <span class="hljs-comment"># 输出：张量outputs</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-comment"># return outputs</span><br>        <span class="hljs-keyword">raise</span> NotImplementedError<br><br>    <span class="hljs-comment"># 反向函数</span><br>    <span class="hljs-comment"># 输入：最终输出对outputs的梯度outputs_grads</span><br>    <span class="hljs-comment"># 输出：最终输出对inputs的梯度inputs_grads</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, outputs_grads</span>):<br>        <span class="hljs-comment"># return inputs_grads</span><br>        <span class="hljs-keyword">raise</span> NotImplementedError<br></code></pre></td></tr></table></figure><h3 id="算子应用"><a href="#算子应用" class="headerlink" title="算子应用"></a>算子应用</h3><p>以$g &#x3D; \exp(a \times b+c \times d)$为例，分别实现加法、乘法和指数运算三个算子，通过算子组合计算$y$值。</p><p><b>加法算子</b></p><p>加法算子的计算过程如下：</p><p><img src="/post/37103f0b/image-20221015180158188.png" alt="image-20221015180158188"></p><p>前向计算的过程输出的是x+y&#x3D;z，即输出z做结果；反向计算梯度，假设输出的结果为L，最终输出为$L$，令$\delta_z&#x3D;\frac{\partial L}{\partial z}$，$\delta_x&#x3D;\frac{\partial L}{\partial x}$，$\delta_y&#x3D;\frac{\partial L}{\partial y}$。加法算子的反向计算的输入是梯度$\delta_z$，输出是梯度$\delta_x$和$\delta_y$。根据链式法则，$\delta_y&#x3D;\frac{\partial z}{\partial y}\delta_z$，可以直接根据z &#x3D; x+y求得$\delta_x &#x3D;  \delta_z \times 1$，$\delta_y &#x3D;  \delta_z \times 1$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">add</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(add, self).__init__()<br><span class="hljs-comment">#调用add函数时默认调用前向过程</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, x, y</span>):<br>        <span class="hljs-keyword">return</span> self.forward(x, y)<br><span class="hljs-comment">#前向过程</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, y</span>):<br>        self.x = x<br>        self.y = y<br>        outputs = x + y<br>        <span class="hljs-keyword">return</span> outputs<br><span class="hljs-comment">#反向过程</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, grads</span>):<br>        grads_x = grads * <span class="hljs-number">1</span><br>        grads_y = grads * <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> grads_x, grads_y<br></code></pre></td></tr></table></figure><p><b>乘法算子</b></p><p>同加法算子的原理，前向传播计算乘法的结果，反向传播计算梯度，根据z &#x3D; x*y求得$\delta_x &#x3D;  \delta_z \times y$，$\delta_y &#x3D;  \delta_z \times x$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">multiply</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(multiply, self).__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, x, y</span>):<br>        <span class="hljs-keyword">return</span> self.forward(x, y)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, y</span>):<br>        self.x = x<br>        self.y = y<br>        outputs = x * y<br>        <span class="hljs-keyword">return</span> outputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, grads</span>):<br>        grads_x = grads * self.y<br>        grads_y = grads * self.x<br>        <span class="hljs-keyword">return</span> grads_x, grads_y<br></code></pre></td></tr></table></figure><p><b>指数算子</b></p><p>输入x，前向传播计算指数函数$z&#x3D;e^x$，反向传播计算梯度，根据公式可得$\delta_x &#x3D;  \delta_z \times e^x$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">exponential</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(exponential, self).__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        self.x = x<br>        outputs = math.exp(x)<br>        <span class="hljs-keyword">return</span> outputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, grads</span>):<br>        grads = grads * math.exp(self.x)<br>        <span class="hljs-keyword">return</span> grads<br></code></pre></td></tr></table></figure><p>对算子进行结合应用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">a, b, c, d = <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span><br><span class="hljs-comment"># 实例化算子</span><br>multiply_op = multiply()<br>add_op = add()<br>exp_op = exponential()<br>y = exp_op(add_op(multiply_op(a, b), multiply_op(c, d)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;y: &#x27;</span>, y)<br><br>z = exp_op.backward(z)<br>x,y = add_op.backward(z)<br>x1,y1 = multiply_op.backward(x)<br>x2,y2 = multiply_op.backward(y)<br><span class="hljs-built_in">print</span>(x1,<span class="hljs-string">&quot; &quot;</span>,y1,<span class="hljs-string">&quot; &quot;</span>,x2,<span class="hljs-string">&quot; &quot;</span>,y2)<br></code></pre></td></tr></table></figure><h3 id="自动微分机制"><a href="#自动微分机制" class="headerlink" title="自动微分机制"></a>自动微分机制</h3><p>目前大部分深度学习平台都支持自动微分（Automatic Differentiation），即根据<code>forward()</code>函数来自动构建<code>backward()</code>函数。自动微分的原理是将所有的数值计算都分解为基本的原子操作，并构建计算图DAG（可以理解为有向无环图）。在模型的构建阶段实时生成有关过程的图。</p><p><img src="/post/37103f0b/1262921-20190421193150562-620065206.gif" alt="img"></p><h3 id="预定义算子"><a href="#预定义算子" class="headerlink" title="预定义算子"></a>预定义算子</h3><p>在深度学习中，大多数模型都是以各种神经网络为主，由一系列层(Layer)组成，层是模型的基础逻辑执行单元。如paddle.nn.Layer、torch.nn.Layer类来方便快速地实现自己的层和模型。当我们实现的算子继承Layer类时，就不用再定义backward函数，自动微分机制可以自动完成反向传播过程，让我们只关注模型构建的前向过程，不必再进行烦琐的梯度求导。</p><p>完</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络与深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python课程作业二——生成密码</title>
      <link href="/post/5431c3b9.html"/>
      <url>/post/5431c3b9.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h1><p>要求：系统初始化为用户生成初始密码，生成的密码要求是随机生成的，并且生成的密码包含一串字符，其中一位数字和一位特殊字符。</p><h1 id="代码（有详细注释）"><a href="#代码（有详细注释）" class="headerlink" title="代码（有详细注释）"></a>代码（有详细注释）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> csv<br><span class="hljs-keyword">import</span> json<br><span class="hljs-comment">#字符的取值范围</span><br>special_char=<span class="hljs-string">&quot;!\&quot;#$%&amp;\&#x27;()*+,-./[\\]^_`&#123;|&#125;~?&quot;</span><br>char = <span class="hljs-string">&quot;abcdefghijklmnopqrstuvwxyz&quot;</span><br><span class="hljs-comment">#定义三个类型字符的获取函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_special_char</span>():<br>    <span class="hljs-keyword">return</span> special_char[random.randint(<span class="hljs-number">0</span>,<span class="hljs-number">25</span>)]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_number</span>():<br>    <span class="hljs-keyword">return</span> random.randint(<span class="hljs-number">0</span>,<span class="hljs-number">9</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_char</span>():<br>    <span class="hljs-keyword">return</span> char[random.randint(<span class="hljs-number">0</span>,<span class="hljs-number">25</span>)]<br><span class="hljs-comment">#根据位数生成代码</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_code</span>(<span class="hljs-params">num</span>):<br>    <span class="hljs-comment">#首先生成num范围中的两个随机位，用于存储数字和特殊字符</span><br>    <span class="hljs-comment">#第一个字符位，第二个数字位</span><br>    x,y = -<span class="hljs-number">1</span>,-<span class="hljs-number">1</span><br>    <span class="hljs-keyword">while</span> x==y:<br>        x = random.randint(<span class="hljs-number">0</span>,num-<span class="hljs-number">1</span>)<br>        y = random.randint(<span class="hljs-number">0</span>,num-<span class="hljs-number">1</span>)<br>    <span class="hljs-comment">#其次生成密码</span><br>    pwd = <span class="hljs-string">&quot;&quot;</span><br>    <span class="hljs-comment">#判断是哪种字符类型</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num):<br>        <span class="hljs-keyword">if</span> i == x:<br>            pwd += get_special_char()<br>        <span class="hljs-keyword">elif</span> i == y:<br>            pwd += <span class="hljs-built_in">str</span>(get_number())<br>        <span class="hljs-keyword">else</span>:<br>            pwd += get_char()<br><br>    <span class="hljs-keyword">return</span> pwd<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># 首先设置密码文件</span><br>    password_file = <span class="hljs-string">&#x27;pwd.txt&#x27;</span><br>    <span class="hljs-comment"># 设置最终的学号-密码 字典</span><br>    stu_pwd_dict = &#123;&#125;<br>    <span class="hljs-comment"># 读取文件</span><br>    files = <span class="hljs-built_in">open</span>(password_file, <span class="hljs-string">&#x27;r&#x27;</span>)<br>    <span class="hljs-comment"># 去掉\n</span><br>    stu_list = files.read().splitlines()<br>    <span class="hljs-comment"># 设置两种模式：1、全体指定相同的密码位数 2、从文件中读入密码位数，其中每一行按照&quot;XXXX,6&quot;的格式存储</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        pattern = <span class="hljs-built_in">str</span>(<span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;请输入您需要设定的位数模式，整体设置扣1，文件设置扣2&quot;</span>))<br>        <span class="hljs-keyword">if</span> pattern==<span class="hljs-string">&quot;1&quot;</span>:<br>            <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>                <span class="hljs-keyword">try</span>:<br>                    num = <span class="hljs-built_in">eval</span>(<span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;请输入您需要设置的密码位数，支持6-18位：&quot;</span>))<br>                    <span class="hljs-keyword">if</span> num <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>,<span class="hljs-number">19</span>):<br>                        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;密码生成中，请稍候……&quot;</span>)<br>                        <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> stu_list:<br>                            pwd = generate_code(num)<br>                            stu_pwd_dict[each] = pwd<br>                        <span class="hljs-keyword">break</span><br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您输入的密码位数有误，请重新输入：&quot;</span>)<br>                <span class="hljs-keyword">except</span>:<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您输入的数字格式有误，请重新输入：&quot;</span>)<br>            <span class="hljs-keyword">break</span><br>        <span class="hljs-keyword">elif</span> pattern==<span class="hljs-string">&quot;2&quot;</span>:<br>            <span class="hljs-comment"># 默认文件中的每一行都是以学号_位数为间隔</span><br>            <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> stu_list:<br>                <span class="hljs-built_in">id</span>,num = each.split(<span class="hljs-string">&quot;_&quot;</span>)<br>                pwd = generate_code(<span class="hljs-built_in">int</span>(num))<br>                stu_pwd_dict[<span class="hljs-built_in">id</span>] = pwd<br>            <span class="hljs-keyword">break</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您输入的模式代码有误，请重新输入：&quot;</span>)<br>            <span class="hljs-keyword">continue</span><br>    name = <span class="hljs-built_in">str</span>(<span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;您的密码已经生成完毕，请重命名输出文件：&quot;</span>))<br>    <span class="hljs-comment">#设置文件的存储格式，支持txt,csv和json三种格式</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-keyword">try</span>:<br>            mode = <span class="hljs-built_in">eval</span>(<span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;请选择生成密码文件的内部格式，按行存储扣1，csv文件存储扣2，json文件存储扣3：&quot;</span>))<br>            <span class="hljs-keyword">if</span> mode==<span class="hljs-number">1</span>:<br>                fileObject = <span class="hljs-built_in">open</span>(name+<span class="hljs-string">&#x27;.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br>                <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> stu_pwd_dict:<br>                    fileObject.write(each+<span class="hljs-string">&quot;_&quot;</span>+stu_pwd_dict[each])<br>                    fileObject.write(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>                fileObject.close()<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">elif</span> mode==<span class="hljs-number">2</span>:<br>                header = [<span class="hljs-string">&#x27;学号&#x27;</span>,<span class="hljs-string">&#x27;密码&#x27;</span>]<br>                fileObject = <span class="hljs-built_in">open</span>(name+<span class="hljs-string">&#x27;.csv&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>,encoding=<span class="hljs-string">&#x27;UTF8&#x27;</span>, newline=<span class="hljs-string">&#x27;&#x27;</span>)<br>                writer = csv.writer(fileObject)<br>                writer.writerow(header)<br>                <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> stu_pwd_dict:<br>                    <span class="hljs-built_in">list</span>=[each,stu_pwd_dict[each]]<br>                    writer.writerow(<span class="hljs-built_in">list</span>)<br>                fileObject.close()<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">elif</span> mode==<span class="hljs-number">3</span>:<br>                fileObject = <span class="hljs-built_in">open</span>(name+<span class="hljs-string">&#x27;.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br>                fileObject.write(json.dumps(stu_pwd_dict))<br>                fileObject.close()<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">else</span>:<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您的文件格式代码输入有误，请重新输入&quot;</span>)<br>        <span class="hljs-keyword">except</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您的文件格式代码输入有误，请重新输入&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;感谢您的使用&quot;</span>)<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python课程作业一——模拟电梯</title>
      <link href="/post/47f9b6bf.html"/>
      <url>/post/47f9b6bf.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h1><p>编程模拟电梯的运行，条件是电梯面板没有14层和18层，用户输入要去的楼层，输出实际到达的楼层（物理高度的楼层）</p><p>可以自己设定楼层高度等条件。</p><p>作业要求：</p><p>1、进一步完善流程图并提交到课堂派</p><p>2、编程实现，即提交python代码到课堂派（注意提交的代码文件格式为txt）</p><h1 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h1><p><img src="/post/47f9b6bf/6336dc8667aa8.jpg" alt="6336dc8667aa8.jpg"></p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">假设本楼最高可到30层，最低可到地下三层（-3层），以输入0为循环的跳出条件</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ReachedFloor</span>(<span class="hljs-params">elevator</span>):<br>    <span class="hljs-comment">#楼层的三段处理逻辑</span><br>    <span class="hljs-keyword">if</span> elevator &gt; <span class="hljs-number">14</span> <span class="hljs-keyword">and</span> elevator &lt; <span class="hljs-number">18</span>:<br>    elevator -= <span class="hljs-number">1</span><br>    <span class="hljs-keyword">elif</span> elevator &gt; <span class="hljs-number">18</span>:<br>    elevator -= <span class="hljs-number">2</span><br>    <span class="hljs-keyword">else</span>: <span class="hljs-keyword">pass</span><br>    <span class="hljs-keyword">return</span> elevator<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">OutOfRange</span>(<span class="hljs-params">elevator</span>):<br>    <span class="hljs-comment">#14和18不在面板上</span><br>    <span class="hljs-keyword">if</span> elevator==<span class="hljs-number">14</span> <span class="hljs-keyword">or</span> elevator==<span class="hljs-number">18</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The elevator doesn&#x27;t have this floor! Please retry!&quot;</span>)<br>    <span class="hljs-comment">#超出默认的楼层范围</span><br>    <span class="hljs-keyword">elif</span> elevator&gt;<span class="hljs-number">30</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The highest floor is 30, Please input the correct floor!&quot;</span>)<br>    <span class="hljs-keyword">elif</span> elevator&lt;-<span class="hljs-number">3</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The lowest floor is -3, Please input the correct floor!&quot;</span>)<br>    <span class="hljs-comment">#如果没有超出范围，返回False</span><br>    <span class="hljs-keyword">else</span>:<span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-comment">#否则，超出范围，返回True</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br><br><span class="hljs-comment">#注意：自定义为输入0时跳出循环</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment">#多次输入</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-comment">#输入</span><br>        <span class="hljs-keyword">try</span>:<br>        elevator=<span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;Please input the number of floor:\n&quot;</span>))<br>        <span class="hljs-comment">#非数字时的异常抛出</span><br>        <span class="hljs-keyword">except</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Please input an integer to represent the floor!&quot;</span>)<br>        <span class="hljs-keyword">continue</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment">#循环跳出条件</span><br>            <span class="hljs-keyword">if</span> elevator==<span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;byebye!&quot;</span>)<br>            <span class="hljs-keyword">break</span><br>            <span class="hljs-comment">#判断输入是否在范围内，若有误则提示重新输入</span><br>            <span class="hljs-keyword">elif</span> OutOfRange(elevator):<br>            <span class="hljs-keyword">continue</span><br>            <span class="hljs-comment">#输入处理逻辑</span><br>            <span class="hljs-keyword">else</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;You successfully reached floor:&quot;</span>+<span class="hljs-built_in">str</span>(ReachedFloor(elevator)))<br><br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>党支部书记竞选思路理顺</title>
      <link href="/post/7fc9300.html"/>
      <url>/post/7fc9300.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="What-Why-How"><a href="#What-Why-How" class="headerlink" title="What Why How"></a>What Why How</h1><p>对党支书的理解：</p><p>1、起到促进的作用：促进党支部的发展，保证大家的信息收集与政策学习，保证研究生党员生活的健康开展</p><p>2、起到组织协调的作用：将党支部成员团结到一起。</p><p>为什么想要做党支书？</p><p>1、服务性质：希望能够在研究生期间继续为同辈服务，在服务同学的过程中和大家熟悉，与大家共同成长。</p><p>2、管理性质：曾经当过班长，参与过本科生党支部的部分工作，党支部书记起到了上传下效、组织协调的作用，重点在于如何让大家如何在党的领导下，更为有效的传达信息，更有乐趣的学习党员基本知识，之前有一点做活动做管理的经验，也希望能够继续锻炼下去。</p><p>3、适配度：个人性格相对好说话好相处。</p><p>成为党支书之后如何做：</p><p>1、尽快进入工作状态，熟悉情况，积极学习党员知识，做合格党员。</p><p>2、通知信息的分层级、总整合：将所有信息整合到一起，减少全员艾特的次数，多做点对点的提示，流程类工作尽量成步骤。</p><p>3、活动的出彩：党日活动推陈出新，将活动的趣味性与党日的郑重性做巧妙地结合，展望一下可以有观影活动、联谊参观活动等。</p><p>4、时间的统一：保证会议时间的合理性。</p><p>表意愿：</p><p>希望大家投出宝贵的一票，也会积极听取大家的建议，给孩子一个锻炼的机会。</p>]]></content>
      
      
      <categories>
          
          <category> 学生工作记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 党支部工作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云计算虚拟机安装tips</title>
      <link href="/post/348fca29.html"/>
      <url>/post/348fca29.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="VMware安装系统时“无法创建新虚拟机-不具备执行此操作的权限“的解决方案"><a href="#VMware安装系统时“无法创建新虚拟机-不具备执行此操作的权限“的解决方案" class="headerlink" title="VMware安装系统时“无法创建新虚拟机: 不具备执行此操作的权限“的解决方案"></a>VMware安装系统时“无法创建新虚拟机: 不具备执行此操作的权限“的解决方案</h2><p>问题主要出在虚拟机文件的位置选择上，应该新建一个新的文件夹并进行存储，单独在位置选择一个新的文件夹不会生效。不应该选在VMware安装的文件夹里，应该重新选择文件夹位置，可以自己新建位置，比如D:\Ubuntu 7。这样问题就解决了。</p><h2 id="无法创建新虚拟机-无法打开配置文件“E-Linux-“-拒绝访问"><a href="#无法创建新虚拟机-无法打开配置文件“E-Linux-“-拒绝访问" class="headerlink" title="无法创建新虚拟机: 无法打开配置文件“E:\Linux\“: 拒绝访问"></a>无法创建新虚拟机: 无法打开配置文件“E:\Linux\“: 拒绝访问</h2><p>重新采用管理员身份开启Vmware</p>]]></content>
      
      
      <categories>
          
          <category> 日常tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 虚拟机 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Acwing 动态规划题解笔记</title>
      <link href="/post/c1d6b3e8.html"/>
      <url>/post/c1d6b3e8.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><h2 id="数字三角形模型"><a href="#数字三角形模型" class="headerlink" title="数字三角形模型"></a>数字三角形模型</h2><h3 id="方格取数"><a href="#方格取数" class="headerlink" title="方格取数"></a><a href="https://www.acwing.com/problem/content/1029/">方格取数</a></h3><p>与<a href="https://www.acwing.com/problem/content/1017/">摘花生</a>不同，本题要求的是路径走两次，且在走的过程中，若之前方格的数字中有被使用，则第二次不可使用。因此这道题的两个路径之间本身就有一定的关联性。</p><p>从摘花生的集合状态与属性出发，<b>f(i,j)</b>代表从开头(1,1)到(i,j)的最佳路径，因为此题走了两次，因此可以稍加推广，以<b>f(i1,j1,i2,j2)</b>表示第一次走到(i1,j1)，第二次走到(i2,j2)的数值最大值，<font color="red">这里的思路和平时不一样，同时处理两条路径的走向。</font></p><p>又根据题目要求，两个路径存在的主要矛盾是当路径重合使用时的数值使用问题，又因若使两个路径可能有重合，则两个路径的步数一定是一致的，因此设置一个同步变量<b>k &#x3D; i1+j1 &#x3D; i2+j2</b>，将集合状态换成<b>f(k,i1,i2)</b>，j1和j2均可通过此状态推导出来。<font color="red">这里可以形象的理解为状态从左上角到右下角成弧形扩散</font></p><p>∴以f(k,i1,i2)表示所有从(1,1)分别走到(i1,j1)和(i2,j2)的最大值。</p><p>转移时，因为上一个状态转到下一个状态的动作是”行走”动作，故可以将状态转移抽象成两个路径分别向<strong>（下,下）、（下,右）、（右,下）、（右,右）</strong>的转移。即</p><p><img src="/post/c1d6b3e8/image-20220912160548858.png" alt="image-20220912160548858"></p><p>对每一种情况的状态转移，首先需要判断是否走到一起。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//如果走到一起，则只记录一次(i1,j1)点的值，(i2,j2)点和(i1,j1)点重合</span><br><span class="hljs-keyword">if</span>(i1==i2) t = w[i1][j1];<br><span class="hljs-keyword">else</span> t = w[i1][j1]+w[i2][j2];<br></code></pre></td></tr></table></figure><p>以两个路径都向下为例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++">f[k][i1][j1]=<span class="hljs-built_in">max</span>(f[k][i1][i2],f[k<span class="hljs-number">-1</span>][i1<span class="hljs-number">-1</span>][i2<span class="hljs-number">-1</span>]+t);<br></code></pre></td></tr></table></figure><p>代码如下，注意注释内容</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N= <span class="hljs-number">25</span>;<br><span class="hljs-type">int</span> n,w[N][N],f[N*<span class="hljs-number">2</span>][N][N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<span class="hljs-type">int</span> r,c,p;<br>    <span class="hljs-keyword">while</span>(cin&gt;&gt;r&gt;&gt;c&gt;&gt;p &amp;&amp; r||c||p)&#123;<br>        w[r][c] = p;<br>    &#125;<br>    <span class="hljs-comment">//状态迭代，k从2开始，即(1,1)点开始</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> k = <span class="hljs-number">2</span>;k &lt;= n*<span class="hljs-number">2</span>;k++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i1 = <span class="hljs-number">1</span>;i1 &lt;= n;i1++)&#123;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i2 = <span class="hljs-number">1</span>;i2 &lt;= n;i2++)&#123;<br>                <span class="hljs-type">int</span> j1 = k-i1,j2 = k-i2;<br>                <span class="hljs-comment">//判断是否重合</span><br>                <span class="hljs-type">int</span> t = w[i1][j1];<br>                <span class="hljs-keyword">if</span>(i1!=i2) t+=w[i2][j2];<br>                <span class="hljs-type">int</span> &amp;x=f[k][i1][i2];<br>                <span class="hljs-comment">//状态转移</span><br>                x = <span class="hljs-built_in">max</span>(x,f[k<span class="hljs-number">-1</span>][i1<span class="hljs-number">-1</span>][i2<span class="hljs-number">-1</span>]+t);<br>                x = <span class="hljs-built_in">max</span>(x,f[k<span class="hljs-number">-1</span>][i1<span class="hljs-number">-1</span>][i2]+t);<br>                x = <span class="hljs-built_in">max</span>(x,f[k<span class="hljs-number">-1</span>][i1][i2<span class="hljs-number">-1</span>]+t);<br>                x = <span class="hljs-built_in">max</span>(x,f[k<span class="hljs-number">-1</span>][i1][i2]+t);<br>            &#125;<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; f[<span class="hljs-number">2</span>*n][n][n];<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="传纸条"><a href="#传纸条" class="headerlink" title="传纸条"></a><a href="https://www.acwing.com/problem/content/1029/">传纸条</a></h3><p>和方格取数一模一样，<a href="https://www.acwing.com/solution/content/51293/">证明如下：</a></p><h2 id="最长上升子序列模型（LIS）"><a href="#最长上升子序列模型（LIS）" class="headerlink" title="最长上升子序列模型（LIS）"></a>最长上升子序列模型（LIS）</h2><h3 id="怪盗基德的滑翔翼"><a href="#怪盗基德的滑翔翼" class="headerlink" title="怪盗基德的滑翔翼"></a><a href="https://www.acwing.com/problem/content/1019/">怪盗基德的滑翔翼</a></h3><p>此题可以抽象为正序、倒序各做一次最长上升子序列问题（LIS），这里主要给出两种可行的最长上升子序列求解办法。</p><h4 id="DP（O-n2-）"><a href="#DP（O-n2-）" class="headerlink" title="DP（O(n2)）"></a>DP（O(n2)）</h4><p>用一个数组F[i]代表以A[i]结尾的LIS的长度，状态转移时两层循环，最外层i循环每个数，内层循环j循环i前面的每个数，F[i] &#x3D; max{F[j]+1,F[i]}</p><p>代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= m;i++)&#123;<br>       <span class="hljs-comment">//g数组为dp数组，f数组为存值数组</span><br>           g[i]=<span class="hljs-number">1</span>;<br>           <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt; i;j++)&#123;<br>               <span class="hljs-keyword">if</span>(f[j]&lt;f[i]) g[i]=<span class="hljs-built_in">max</span>(g[i],g[j]+<span class="hljs-number">1</span>);<br>           &#125;<br>           res = <span class="hljs-built_in">max</span>(res,g[i]);<br>   &#125;<br></code></pre></td></tr></table></figure><h4 id="二分-贪心（O-nlogn-）"><a href="#二分-贪心（O-nlogn-）" class="headerlink" title="二分+贪心（O(nlogn)）"></a>二分+贪心（O(nlogn)）</h4><p>原理：</p><p>题解中最难理解的地方在于栈中序列虽然递增，但是每个元素在原串中对应的位置其实可能是乱的，那为什么这个栈还能用于计算最长子序列长度？<br>实际上这个栈【不用于记录最终的最长子序列】，<strong>而是【以stk[i]结尾的子串长度最长为i】</strong>或者说<strong>【长度为i的递增子串中，末尾元素最小的是stk[i]】</strong>。理解了这个问题以后就知道为什么新进来的元素要不就在末尾增加，要不就替代第一个大于等于它元素的位置。<br>这里的【替换】就蕴含了一个贪心的思想，对于同样长度的子串，我当然希望它的末端越小越好，这样以后我也有更多机会拓展。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-type">int</span> f[N],n;<br><span class="hljs-type">int</span> g[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)cin &gt;&gt; f[i];<br>    <span class="hljs-comment">//替换思想体现了贪心，我们希望序列每个位置的数都更小</span><br>    g[<span class="hljs-number">1</span>] = f[<span class="hljs-number">1</span>];<br>    <span class="hljs-type">int</span> t = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">2</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-comment">//如果当前序列的最大值小于f[n]，则加入数组</span><br>        <span class="hljs-keyword">if</span>(g[t] &lt; f[i])&#123;<br>            g[t+<span class="hljs-number">1</span>] = f[i];<br>            t++;<br>        &#125;<br>        <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-comment">//找到第一个大于等于f[n]的值</span><br>            <span class="hljs-type">int</span> l = <span class="hljs-number">1</span>,r = t;<br>            <span class="hljs-keyword">while</span>(l &lt; r)&#123;<br>                <span class="hljs-type">int</span> m = (l+r)/<span class="hljs-number">2</span>;<br>                <span class="hljs-keyword">if</span>(g[m] &gt;= f[i]) r = m;<br>                <span class="hljs-keyword">else</span> l = m+<span class="hljs-number">1</span>;<br>            &#125;<br>            g[l] = f[i];<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; t &lt;&lt; endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="登山-amp-合唱队形"><a href="#登山-amp-合唱队形" class="headerlink" title="登山 &amp; 合唱队形"></a><a href="https://www.acwing.com/problem/content/1016/">登山 </a>&amp; <a href="https://www.acwing.com/problem/content/484/">合唱队形</a></h3><p>此题是LIS问题的一个变型，需要求的是以中间为最大值，两端降低的子序列，我们直接从前到后和从后到前，一共两次求解最长上升和最长下降子序列，然后迭代每个位置求和即可。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1010</span>;<br><span class="hljs-type">int</span> n,f[N],g[N],k[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)cin &gt;&gt; f[i];<br>    <span class="hljs-comment">//最长上升子序列</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        g[i]=<span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt; i;j++)&#123;<br>            <span class="hljs-keyword">if</span>(f[j]&lt;f[i]) g[i]=<span class="hljs-built_in">max</span>(g[i],g[j]+<span class="hljs-number">1</span>);<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//最长下降子序列</span><br>    <span class="hljs-comment">//注意最外层的循环也要从n-1开始，因为需要有初值</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = n<span class="hljs-number">-1</span>;i &gt;= <span class="hljs-number">0</span>;i--)&#123;<br>        k[i]=<span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = n<span class="hljs-number">-1</span>;j&gt;i;j--)&#123;<br>            <span class="hljs-keyword">if</span>(f[j]&lt;f[i]) k[i]=<span class="hljs-built_in">max</span>(k[i],k[j]+<span class="hljs-number">1</span>);<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//计算终值</span><br>    <span class="hljs-type">int</span> res=<span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//将计算过两次的中间点去除</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        res=<span class="hljs-built_in">max</span>(res,g[i]+k[i]<span class="hljs-number">-1</span>);<br>    &#125;<br>    cout &lt;&lt; res;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="友好城市"><a href="#友好城市" class="headerlink" title="友好城市"></a><a href="https://www.acwing.com/problem/content/1014/">友好城市</a></h3><p>此题可以理解为对Pair求解LIS问题，首先对first部分进行排序，使其有一定的次序，然后对Pair的每一维都进行LIS问题的求解。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">5010</span>;<br><span class="hljs-keyword">typedef</span> pair&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt; PII;<br>vector&lt;PII&gt; v;<br><span class="hljs-type">int</span> res,g[N];<br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">cmp</span><span class="hljs-params">(<span class="hljs-type">const</span> PII a,<span class="hljs-type">const</span> PII b)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> a.first&lt;b.first;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n;cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> l,r;cin &gt;&gt; l &gt;&gt; r;<br>        v.<span class="hljs-built_in">push_back</span>(&#123;l,r&#125;);<br>    &#125;<br>    <span class="hljs-built_in">sort</span>(v.<span class="hljs-built_in">begin</span>(),v.<span class="hljs-built_in">end</span>(),cmp);<br>    <span class="hljs-comment">//对两维都进行LIS问题求解</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        g[i]=<span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt; i;j++)&#123;<br>            <span class="hljs-keyword">if</span>(v[j].first &lt; v[i].first &amp;&amp; v[j].second &lt; v[i].second) g[i]=<span class="hljs-built_in">max</span>(g[i],g[j]+<span class="hljs-number">1</span>);<br>        &#125;<br>        res=<span class="hljs-built_in">max</span>(res,g[i]);<br>    &#125;<br>    cout &lt;&lt; res &lt;&lt; endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="最大上升子序列和"><a href="#最大上升子序列和" class="headerlink" title="最大上升子序列和"></a><a href="https://www.acwing.com/problem/content/1018/">最大上升子序列和</a></h3><p>令LIS问题DP解法中的g[i]表示以f[i]为结尾的最长上升子序列和，这样LIS的转移方法仍然适用于这道题。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1010</span>;<br><span class="hljs-type">int</span> f[N],g[N],res;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i =<span class="hljs-number">0</span>;i &lt; n;i++)cin &gt;&gt; f[i];<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i =<span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-comment">//从这里进行求和，g[i]代表f[i]结尾的最长上升子序列和，默认为f[i]本身</span><br>        g[i]=f[i];<br>        <span class="hljs-comment">//DP求解</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt; i;j++)&#123;<br>            <span class="hljs-keyword">if</span>(f[j]&lt;f[i]) g[i]=<span class="hljs-built_in">max</span>(g[i],g[j]+f[i]);<br>        &#125;<br>        res=<span class="hljs-built_in">max</span>(res,g[i]);<br>    &#125;<br>    cout &lt;&lt; res;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="拦截导弹"><a href="#拦截导弹" class="headerlink" title="拦截导弹"></a><a href="https://www.acwing.com/problem/content/1012/">拦截导弹</a></h3><p>这里我们按照以下几个部分来讲：</p><p><b>问题的第一部分是最长下降子序列，直接代板子，不再赘述</b></p><p><b>问题的第二部分是找到最少个数的下降子序列将序列覆盖</b></p><p>第二问可以采用贪心的做法，这里可以复习最长上升子序列的<a href="https://www.acwing.com/problem/content/898/">贪心解法</a>。考虑一种贪心方案：<br>对于第i个数来说，把它加入前 i - 1 个数构成的下降子序列组中，所有结尾元素大于第i个数的数中最小的那个数，下给出对其的证明：</p><p>假设有一种最优策略不是按照贪心策略进行求解的，则可以通过有限步的调整将最优解调整成贪心解，且不改变原有条件要求。</p><p><img src="/post/c1d6b3e8/image-20220925233911877.png" alt="image-20220925233911877"></p><p>因此可以每次找到序列尾部一个大于等于当前值的最小值并加入，如果所有的序列最后一个都小于当前值，则另开一个新的序列，这保证了记录序列尾部的数组一定是单调递增的，因为每次加入的都是大于所有值的新数。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++">   <span class="hljs-comment">//计数器</span><br><span class="hljs-type">int</span> cnt=<span class="hljs-number">0</span>;<br><span class="hljs-comment">//对所有的值进行迭代，根据序列的要求，其数组一定是单调递增的。</span><br>   <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>       <span class="hljs-comment">//找到大于等于当前值的最小值</span><br>       <span class="hljs-type">int</span> t=<span class="hljs-number">0</span>;<br>       <span class="hljs-keyword">while</span>(t &lt;cnt &amp;&amp; g[t]&lt;f[i]) t++;<br>       <span class="hljs-comment">//赋值</span><br>       g[t]=f[i];<br>       <span class="hljs-comment">//如果加到了最后，则cnt加一，新加一个序列</span><br>       <span class="hljs-keyword">if</span>(t&gt;=cnt) cnt++;<br>   &#125;<br></code></pre></td></tr></table></figure><p>注：此道题由于满足<b>Dilworth定理</b>：偏序集的最少反链划分数等于最长链的长度，因此求最少的下降子序列划分数，等同于求上升子序列的最长长度，因此这里可以直接求一个最长上升子序列并给出解。</p><h3 id="最长公共上升子序列"><a href="#最长公共上升子序列" class="headerlink" title="最长公共上升子序列"></a><a href="https://www.acwing.com/problem/content/274/">最长公共上升子序列</a></h3><p>这是两个经典DP问题$（LIS和LCS）$的结合版，按照集合的概念理解DP</p><p>状态表示$f[i][j]$—集合：考虑 a 中前 i 个数字，b 中前 j 个数字 ，且当前以 b[j] 结尾的子序列的方案</p><p>状态表示$f[i][j]$—属性：该方案的子序列长度最大$max$</p><p>状态转移 ：</p><ul><li><p>a数组中前 i-1个数字，b数组中前 j 个数字 ，且当前以 b[j] 结尾的子序列的方案转移过来：$f_{i,j}&#x3D;max(f_{i,j},f_{i−1,j})$，此时$a[i]\ne b[j]$</p></li><li><p>a数组中前 i 个数字，b数组中前 k 个数字 ，且当前以 b[k] 结尾的子序列的方案转移过来：$f_{i,j}&#x3D;max(f_{i,j},f_{i−1,k+1})，k∈[0,j−1],a[i]&#x3D;&#x3D;b[j],bj&gt;bk$</p></li></ul><p>其中，对于第二种情况需要从j-1种可能的情况转移过来，难度较大。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">3010</span>;<br><span class="hljs-type">int</span> n,d[N][N];<br><span class="hljs-type">int</span> a[N],b[N];<br>vector&lt;<span class="hljs-type">int</span>&gt; s;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)cin &gt;&gt; a[i];<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)cin &gt;&gt; b[i];<br>    <span class="hljs-comment">//对a进行迭代</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-type">int</span> maxv=<span class="hljs-number">1</span>;<br>        <span class="hljs-comment">//对b进行迭代</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= n;j++)&#123;<br>            <span class="hljs-comment">//第一种情况：不取a[i]，</span><br>            d[i][j] = d[i<span class="hljs-number">-1</span>][j];<br>            <span class="hljs-comment">//第二种情况：取a[i]</span><br>            <span class="hljs-comment">//如果a[i]==b[j]了，结果应该是取和不取a[i]的情况的最大值</span><br>            <span class="hljs-keyword">if</span>(a[i]==b[j]) d[i][j] = <span class="hljs-built_in">max</span>(d[i][j],maxv);<br>            <span class="hljs-comment">//这个部分用来循环找到d[i-1][j]+1最大的</span><br>            <span class="hljs-keyword">if</span>(b[j]&lt;a[i]) maxv=<span class="hljs-built_in">max</span>(maxv,d[i<span class="hljs-number">-1</span>][j]+<span class="hljs-number">1</span>);<br>        &#125;<br>    &#125;<br>    <span class="hljs-type">int</span> res=<span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i&lt;=n;i++) res = <span class="hljs-built_in">max</span>(res,d[n][i]);<br>    cout &lt;&lt; res;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机试 </tag>
            
            <tag> 动态规划 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开学机试速通</title>
      <link href="/post/bf1e6b0b.html"/>
      <url>/post/bf1e6b0b.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="基础算法"><a href="#基础算法" class="headerlink" title="基础算法"></a>基础算法</h1><h2 id="快排"><a href="#快排" class="headerlink" title="快排"></a>快排</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">quicksort</span><span class="hljs-params">(<span class="hljs-type">int</span> num[],<span class="hljs-type">int</span> i,<span class="hljs-type">int</span> j)</span></span>&#123;<br>    <span class="hljs-comment">//特判</span><br>    <span class="hljs-keyword">if</span>(i &gt;= j) <span class="hljs-keyword">return</span>;<br>    <span class="hljs-comment">//两端赋值,中间为基准数</span><br>    <span class="hljs-type">int</span> l = i<span class="hljs-number">-1</span>,r = j+<span class="hljs-number">1</span>,m=num[(l+r)/<span class="hljs-number">2</span>];<br>    <span class="hljs-comment">//两端向中间移动，交换，使之满足基准数左边&lt;=m,基准数右边&gt;=m</span><br>    <span class="hljs-keyword">while</span>(l &lt; r)&#123;<br>        <span class="hljs-keyword">do</span> l++; <span class="hljs-keyword">while</span>(num[l]&lt;m);<br>        <span class="hljs-keyword">do</span> r--; <span class="hljs-keyword">while</span>(num[r]&gt;m);<br>        <span class="hljs-keyword">if</span>(l &lt; r) &#123;<br>            <span class="hljs-built_in">swap</span>(num[l], num[r]);<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//递归基准数两边</span><br>    <span class="hljs-built_in">quicksort</span>(num,i,r);<br>    <span class="hljs-built_in">quicksort</span>(num,r+<span class="hljs-number">1</span>,j);<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="归并"><a href="#归并" class="headerlink" title="归并"></a>归并</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">mergesort</span><span class="hljs-params">(<span class="hljs-type">int</span> num[],<span class="hljs-type">int</span> l,<span class="hljs-type">int</span> r)</span></span>&#123;<br>    <span class="hljs-comment">//特判</span><br>    <span class="hljs-keyword">if</span>(l &gt;= r)<span class="hljs-keyword">return</span>;<br>    <span class="hljs-comment">//取中间为分治点</span><br>    <span class="hljs-type">int</span> m = (l+r)/<span class="hljs-number">2</span>,tp[r-l+<span class="hljs-number">1</span>];<br>    <span class="hljs-comment">//两边递归分支</span><br>    <span class="hljs-built_in">mergesort</span>(num,l,m);<br>    <span class="hljs-built_in">mergesort</span>(num,m+<span class="hljs-number">1</span>,r);<br>    <span class="hljs-comment">//双指针归并</span><br>    <span class="hljs-type">int</span> i = l,j = m+<span class="hljs-number">1</span>,k = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">while</span>(i &lt;= m &amp;&amp; j &lt;= r)&#123;<br>        <span class="hljs-keyword">if</span>(num[i] &lt; num[j])&#123;<br>            tp[k] = num[i];i++;k++;<br>        &#125;<br>        <span class="hljs-keyword">else</span>&#123;<br>            tp[k] = num[j];j++;k++;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//收尾</span><br>    <span class="hljs-keyword">while</span>(i &lt;= m)&#123;<br>        tp[k] = num[i];i++;k++;<br>    &#125;<br>    <span class="hljs-keyword">while</span>(j &lt;= r)&#123;<br>        tp[k] = num[j];j++;k++;<br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; k;i++)&#123;<br>        num[l+i] = tp[i];<br>    &#125;<br>&#125; <br></code></pre></td></tr></table></figure><h4 id="逆序数思想："><a href="#逆序数思想：" class="headerlink" title="逆序数思想："></a>逆序数思想：</h4><p>res &#x3D; mergesort(l,m) + mergesort(m+1,r)+[当num[i]&gt;num[j]时，i到m的m-i+1个数均为j指向数的逆序数]</p><h2 id="二分法"><a href="#二分法" class="headerlink" title="二分法"></a>二分法</h2><p>本质：对边界情况的讨论，如下图</p><p><img src="/post/bf1e6b0b/image-20220831133428119.png" alt="image-20220831133428119"></p><p>模板：</p><p>1、确定二分中间值 m&#x3D;(l+r)&#x2F;2 或 m&#x3D;(l+r+1)&#x2F;2;</p><p>2、确定check函数</p><p>3、判断，移动端点位置</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//红色框的右端点:小于的最大值</span><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">check</span><span class="hljs-params">(<span class="hljs-type">int</span> m,<span class="hljs-type">int</span> q)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> num[m] &lt; q?<span class="hljs-literal">true</span>:<span class="hljs-literal">false</span>;<br>&#125;<br><span class="hljs-comment">//如果下一个值大于等于q,则l对应值就是小于的最大值</span><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">valid</span><span class="hljs-params">(<span class="hljs-type">int</span> l,<span class="hljs-type">int</span> q)</span></span>&#123;<br><span class="hljs-keyword">return</span> num[l+<span class="hljs-number">1</span>] &gt;= q?<span class="hljs-literal">true</span>:<span class="hljs-literal">false</span>;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">idx</span><span class="hljs-params">(<span class="hljs-type">int</span> l,<span class="hljs-type">int</span> r,<span class="hljs-type">int</span> q)</span></span>&#123;<br>    <span class="hljs-comment">//判断相遇</span><br>    <span class="hljs-keyword">while</span>(l &lt; r)&#123;<br>        <span class="hljs-comment">//确定中间值,因为l = m,所以用m=(l+r+1)/2,防止若m=r-1时，下取整会一直处在m == l的循环。</span><br>        m = l+r+<span class="hljs-number">1</span> &gt;&gt; <span class="hljs-number">1</span>;<br>        <span class="hljs-comment">//如果在红色框内，则将左边界向右推</span><br>        <span class="hljs-keyword">if</span>(<span class="hljs-built_in">check</span>(m,q)) l = m;<br>        <span class="hljs-comment">//否则右端点向左推，而且框内不包括m对应值</span><br>        <span class="hljs-keyword">else</span> r = m<span class="hljs-number">-1</span>;<br>    &#125;<br>    <span class="hljs-comment">//判断是否有效</span><br>    <span class="hljs-keyword">if</span>(<span class="hljs-built_in">valid</span>(l,q)) <span class="hljs-keyword">return</span> l;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>二分模板一共有两个，分别适用于不同情况。<br>算法思路：假设目标值在闭区间[l, r]中， 每次将区间长度缩小一半，当l &#x3D; r时，我们就找到了目标值。</p><p>版本1<br>当我们将区间[l, r]划分成[l, mid]和[mid + 1, r]时，其更新操作是r &#x3D; mid或者l &#x3D; mid + 1;，计算mid时不需要加1。</p><p>C++ 代码模板：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">bsearch_1</span><span class="hljs-params">(<span class="hljs-type">int</span> l, <span class="hljs-type">int</span> r)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">while</span> (l &lt; r)<br>    &#123;<br>        <span class="hljs-type">int</span> mid = l + r &gt;&gt; <span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">check</span>(mid)) r = mid;<br>        <span class="hljs-keyword">else</span> l = mid + <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> l;<br>&#125;<br></code></pre></td></tr></table></figure><p>版本2<br>当我们将区间[l, r]划分成[l, mid - 1]和[mid, r]时，其更新操作是r &#x3D; mid - 1或者l &#x3D; mid;，此时为了防止死循环，计算mid时需要加1。</p><p>C++ 代码模板：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">bsearch_2</span><span class="hljs-params">(<span class="hljs-type">int</span> l, <span class="hljs-type">int</span> r)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">while</span> (l &lt; r)<br>    &#123;<br>        <span class="hljs-type">int</span> mid = l + r + <span class="hljs-number">1</span> &gt;&gt; <span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">check</span>(mid)) l = mid;<br>        <span class="hljs-keyword">else</span> r = mid - <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> l;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="浮点数二分"><a href="#浮点数二分" class="headerlink" title="浮点数二分"></a>浮点数二分</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">double</span> n;cin &gt;&gt; n;<br>    <span class="hljs-comment">//确定两个边界</span><br>    <span class="hljs-type">double</span> l = <span class="hljs-number">-10000</span>,r = <span class="hljs-number">10000</span>,m;<br>    <span class="hljs-comment">//确定最小插值</span><br>    <span class="hljs-keyword">while</span>(r-l &gt;= <span class="hljs-number">1e-8</span>)&#123;<br>        m = (l+r)/<span class="hljs-number">2</span>;<br>        <span class="hljs-comment">//check函数</span><br>        <span class="hljs-keyword">if</span>(m*m*m &gt;= n) r = m;<br>        <span class="hljs-keyword">else</span> l = m;<br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%.6f&quot;</span>,m);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="高精度算数-记成所有方法都要去除前导零"><a href="#高精度算数-记成所有方法都要去除前导零" class="headerlink" title="高精度算数(记成所有方法都要去除前导零)"></a>高精度算数(记成所有方法都要去除前导零)</h2><h4 id="加法"><a href="#加法" class="headerlink" title="加法"></a>加法</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1e5</span>+<span class="hljs-number">10</span>;<br><span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">add</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt; va,vector&lt;<span class="hljs-type">int</span>&gt; vb)</span></span>&#123;<br>    <span class="hljs-type">int</span> c=<span class="hljs-number">0</span>,i=<span class="hljs-number">0</span>;vector&lt;<span class="hljs-type">int</span>&gt;res;<br>    <span class="hljs-comment">//每两位相加，c是进位，i是指针</span><br>    <span class="hljs-keyword">while</span>(i &lt; va.<span class="hljs-built_in">size</span>() || i &lt; vb.<span class="hljs-built_in">size</span>())&#123;<br>        <span class="hljs-keyword">if</span>(i &lt; va.<span class="hljs-built_in">size</span>()) c+=va[i];<br>        <span class="hljs-keyword">if</span>(i &lt; vb.<span class="hljs-built_in">size</span>()) c+=vb[i];<br>        res.<span class="hljs-built_in">push_back</span>(c%<span class="hljs-number">10</span>);<br>        c /= <span class="hljs-number">10</span>;i++;<br>    &#125;<br>    <span class="hljs-comment">//若c仍有进位，则将其压入</span><br>    <span class="hljs-keyword">if</span>(c)res.<span class="hljs-built_in">push_back</span>(c);<br>    <span class="hljs-comment">//前导零（其实没有也没关系）</span><br>    <span class="hljs-keyword">while</span>(res.<span class="hljs-built_in">size</span>()&gt;<span class="hljs-number">0</span> &amp;&amp; res.<span class="hljs-built_in">back</span>()==<span class="hljs-number">0</span>) res.<span class="hljs-built_in">pop_back</span>();<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">char</span> a[N],b[N];<br>    <span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%s%s&quot;</span>,a,b);<br>    vector&lt;<span class="hljs-type">int</span>&gt; va,vb;<br>    <span class="hljs-comment">//倒序存储</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-built_in">strlen</span>(a)<span class="hljs-number">-1</span>;i &gt;= <span class="hljs-number">0</span>;i--)va.<span class="hljs-built_in">push_back</span>(a[i]-<span class="hljs-string">&#x27;0&#x27;</span>);<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-built_in">strlen</span>(b)<span class="hljs-number">-1</span>;i &gt;= <span class="hljs-number">0</span>;i--)vb.<span class="hljs-built_in">push_back</span>(b[i]-<span class="hljs-string">&#x27;0&#x27;</span>);<br>    vector&lt;<span class="hljs-type">int</span>&gt; res = <span class="hljs-built_in">add</span>(va,vb);<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = res.<span class="hljs-built_in">size</span>()<span class="hljs-number">-1</span>;i &gt;= <span class="hljs-number">0</span>;i--)cout &lt;&lt; res[i];<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="减法"><a href="#减法" class="headerlink" title="减法"></a>减法</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1e5</span>+<span class="hljs-number">10</span>;<br><span class="hljs-comment">//减法首先保证大数减小数</span><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">upper</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt; va,vector&lt;<span class="hljs-type">int</span>&gt; vb)</span></span>&#123;<br>    <span class="hljs-comment">//size不一致</span><br>    <span class="hljs-keyword">if</span>(va.<span class="hljs-built_in">size</span>() != vb.<span class="hljs-built_in">size</span>()) <span class="hljs-keyword">return</span> va.<span class="hljs-built_in">size</span>() &gt; vb.<span class="hljs-built_in">size</span>();<br>    <span class="hljs-keyword">else</span>&#123;<br>        <span class="hljs-comment">//size一致，从高位到低位判断</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = va.<span class="hljs-built_in">size</span>()<span class="hljs-number">-1</span>;i &gt;= <span class="hljs-number">0</span>;i--)&#123;<br>            <span class="hljs-keyword">if</span>(va[i] != vb[i])<span class="hljs-keyword">return</span> va[i]&gt;vb[i];<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//相等</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>&#125;<br><span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">minus_</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt; va,vector&lt;<span class="hljs-type">int</span>&gt; vb)</span></span>&#123;<br>    vector&lt;<span class="hljs-type">int</span>&gt; res;<br>    <span class="hljs-comment">//借位和指针</span><br>    <span class="hljs-type">int</span> t=<span class="hljs-number">0</span>,i=<span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">while</span>(i &lt; va.<span class="hljs-built_in">size</span>())&#123;<br>        <span class="hljs-comment">//t=a-t-b</span><br>        t=va[i]-t;<br>        <span class="hljs-keyword">if</span>(i&lt;vb.<span class="hljs-built_in">size</span>())t-=vb[i];<br>        <span class="hljs-comment">//余位为(t+10)%10;</span><br>        res.<span class="hljs-built_in">push_back</span>((t+<span class="hljs-number">10</span>)%<span class="hljs-number">10</span>);<br>        t&lt;<span class="hljs-number">0</span>?t=<span class="hljs-number">1</span>:t=<span class="hljs-number">0</span>;i++;<br>    &#125;<br>    <span class="hljs-comment">//留个0</span><br>    <span class="hljs-keyword">while</span>(res.<span class="hljs-built_in">size</span>()&gt;<span class="hljs-number">1</span>&amp;&amp;res.<span class="hljs-built_in">back</span>()==<span class="hljs-number">0</span>)res.<span class="hljs-built_in">pop_back</span>();<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">char</span> a[N],b[N];<span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%s%s&quot;</span>,a,b);<br>    vector&lt;<span class="hljs-type">int</span>&gt; va,vb;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-built_in">strlen</span>(a)<span class="hljs-number">-1</span>;i &gt;= <span class="hljs-number">0</span>;i--) va.<span class="hljs-built_in">push_back</span>(a[i]-<span class="hljs-string">&#x27;0&#x27;</span>);<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-built_in">strlen</span>(b)<span class="hljs-number">-1</span>;i &gt;= <span class="hljs-number">0</span>;i--) vb.<span class="hljs-built_in">push_back</span>(b[i]-<span class="hljs-string">&#x27;0&#x27;</span>);<br>    vector&lt;<span class="hljs-type">int</span>&gt; res;<br>    <span class="hljs-keyword">if</span>(<span class="hljs-built_in">upper</span>(va,vb))res = <span class="hljs-built_in">minus_</span>(va,vb);<br>    <span class="hljs-keyword">else</span> res = <span class="hljs-built_in">minus_</span>(vb,va);<br>    <span class="hljs-keyword">if</span>(!<span class="hljs-built_in">upper</span>(va,vb))cout &lt;&lt; <span class="hljs-string">&quot;-&quot;</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = res.<span class="hljs-built_in">size</span>()<span class="hljs-number">-1</span>;i &gt;= <span class="hljs-number">0</span>;i--)cout &lt;&lt; res[i];<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="乘法"><a href="#乘法" class="headerlink" title="乘法"></a>乘法</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//大数用vector存，小数用int存</span><br><span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">mul</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt; a,<span class="hljs-type">int</span> b)</span></span>&#123;<br>    vector&lt;<span class="hljs-type">int</span>&gt; res;<br>    <span class="hljs-type">int</span> c=<span class="hljs-number">0</span>,i=<span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//进位(a*b+c)/10,本位(a*b+c)%10;</span><br>    <span class="hljs-keyword">while</span>(i &lt; a.<span class="hljs-built_in">size</span>())&#123;<br>        res.<span class="hljs-built_in">push_back</span>((a[i]*b+c)%<span class="hljs-number">10</span>);<br>        c = (a[i]*b+c)/<span class="hljs-number">10</span>;i++;<br>    &#125;<br>    <span class="hljs-comment">//收尾</span><br>    <span class="hljs-keyword">if</span>(c)res.<span class="hljs-built_in">push_back</span>(c);<br>    <span class="hljs-comment">//去除前导零，这里其实没必要去除，记成都去除一下吧</span><br>    <span class="hljs-keyword">while</span>(res.<span class="hljs-built_in">size</span>()&gt;<span class="hljs-number">1</span> &amp;&amp; res.<span class="hljs-built_in">back</span>()==<span class="hljs-number">0</span>)res.<span class="hljs-built_in">pop_back</span>();<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="除法（注意除法是正序处理，不用从个位开始倒序处理）"><a href="#除法（注意除法是正序处理，不用从个位开始倒序处理）" class="headerlink" title="除法（注意除法是正序处理，不用从个位开始倒序处理）"></a>除法（注意除法是正序处理，不用从个位开始倒序处理）</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//reverse的库</span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1e5</span>+<span class="hljs-number">10</span>;<br><span class="hljs-type">int</span> r = <span class="hljs-number">0</span>;<br><span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">divide</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt; a,<span class="hljs-type">int</span> b)</span></span>&#123;<br>    vector&lt;<span class="hljs-type">int</span>&gt; res;<br>    <span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//除商：(r*10+a)/b;余数(r*10+a)%b;</span><br>    <span class="hljs-keyword">while</span>(i &lt; a.<span class="hljs-built_in">size</span>())&#123;<br>        res.<span class="hljs-built_in">push_back</span>((r*<span class="hljs-number">10</span>+a[i])/b);<br>        r = (r*<span class="hljs-number">10</span>+a[i])%b;i++;<br>    &#125;<br>    <span class="hljs-comment">//倒过来，去除前导零</span><br>    <span class="hljs-built_in">reverse</span>(res.<span class="hljs-built_in">begin</span>(),res.<span class="hljs-built_in">end</span>());<br>    <span class="hljs-keyword">while</span>(res.<span class="hljs-built_in">size</span>()&gt;<span class="hljs-number">1</span>&amp;&amp;res.<span class="hljs-built_in">back</span>()==<span class="hljs-number">0</span>)res.<span class="hljs-built_in">pop_back</span>();<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="前缀和"><a href="#前缀和" class="headerlink" title="前缀和"></a>前缀和</h2><p>用于计算某个序列中一段子序列的和</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1e5</span>+<span class="hljs-number">10</span>;<br><span class="hljs-type">int</span> s[N],a[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n,m;<br>    cin &gt;&gt; n &gt;&gt; m;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        cin &gt;&gt; a[i];<br>        <span class="hljs-comment">//s[i]：前i个数的和，a[i]：第i个数</span><br>        s[i] = a[i]+s[i<span class="hljs-number">-1</span>];<br>    &#125;<br>    <span class="hljs-keyword">while</span>(m--)&#123;<br>        <span class="hljs-type">int</span> l,r;cin &gt;&gt; l &gt;&gt; r;<br>        cout &lt;&lt; s[r]-s[l<span class="hljs-number">-1</span>] &lt;&lt; endl;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>二维前缀和：</p><p><strong>s[i,j] &#x3D; s[i,j-1]+s[i-1,j] - s[i,j]+a[i,j];</strong></p><p>求解[x1,y1] -&gt; [x2,y2] ：s[x2,y2]-s[x1-1,y2]-s[x2,y1-1]+s[x1-1,y1-1]    <strong>&#x2F;&#x2F;注意x1-1,y1-1</strong></p><h2 id="差分"><a href="#差分" class="headerlink" title="差分"></a>差分</h2><p>构造一个数组b，使当前数组a是b的前缀和数组，我们只要有b数组，通过前缀和运算，就可以在O(n) 的时间内得到a数组 。</p><p>给定区间[l ,r ]，让我们把a数组中的[ l, r]区间中的每一个数都加上c,即 a[l] + c , a[l+1] + c , a[l+2] + c ,,,,,, a[r] + c;</p><p>始终要记得，a数组是b数组的前缀和数组，比如对b数组的b[i]的修改，会影响到a数组中从a[i]及往后的每一个数。</p><p>首先让差分b数组中的 b[l] + c ,a数组变成 **a[l] + c ,a[l+1] + c,,,,,, a[n] + c;**，对a[i]及之后的元素产生影响</p><p>然后我们打个补丁，b[r+1] - c, a数组变成 a[r+1] - c,a[r+2] - c,,,,,,,a[n] - c;</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1e5</span>+<span class="hljs-number">10</span>;<br><span class="hljs-type">int</span> a[N],b[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n,m;<br>    cin &gt;&gt; n &gt;&gt; m;a[<span class="hljs-number">0</span>]=<span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-comment">/*a[i]是b[i]的前缀和数组</span><br><span class="hljs-comment">        因为a[i] = a[i-1]+b[i]</span><br><span class="hljs-comment">        所以b[i] = a[i]-a[i-1]</span><br><span class="hljs-comment">        */</span><br>        cin &gt;&gt; a[i];<br>        b[i] = a[i]-a[i<span class="hljs-number">-1</span>];<br>    &#125;<br>    <span class="hljs-keyword">while</span>(m--)&#123;<br>        <span class="hljs-type">int</span> l,r,c;<br>        cin &gt;&gt; l &gt;&gt; r &gt;&gt; c;<br>        <span class="hljs-comment">//b[l]+=c使a[l]到最后的元素+c</span><br>        <span class="hljs-comment">//b[r+1]使a[r+1]到最后的元素-c</span><br>        b[l]+=c;b[r+<span class="hljs-number">1</span>]-=c;<br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        a[i] = a[i<span class="hljs-number">-1</span>]+b[i];<br>        cout &lt;&lt; a[i] &lt;&lt; <span class="hljs-string">&quot; &quot;</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="双指针算法"><a href="#双指针算法" class="headerlink" title="双指针算法"></a>双指针算法</h2><h4 id="板子："><a href="#板子：" class="headerlink" title="板子："></a>板子：</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>,j = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br><span class="hljs-keyword">while</span>(j&lt;i &amp;&amp; <span class="hljs-built_in">check</span>(i,j))j++;<br><span class="hljs-comment">//具体逻辑</span><br>&#125;<br></code></pre></td></tr></table></figure><h4 id="不重复连续子序列"><a href="#不重复连续子序列" class="headerlink" title="不重复连续子序列"></a>不重复连续子序列</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-type">int</span> c[N],f[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n,r=<span class="hljs-number">0</span>;cin &gt;&gt; n;<br>    <span class="hljs-comment">/*</span><br><span class="hljs-comment">    i指针用来遍历，j指针在i指针左边，j指向能够使从j到i的不重复连续子序列最远的点，</span><br><span class="hljs-comment">    */</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)cin &gt;&gt; f[i];<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>,j = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-comment">//计数</span><br>        c[f[i]]++;<br>        <span class="hljs-comment">//移动左指针，当i指向的点仍重复时，移动j指针直到i指向点不重复。</span><br>        <span class="hljs-keyword">while</span>(c[f[i]]&gt;<span class="hljs-number">1</span> &amp;&amp; j&lt;n)&#123;<br>            c[f[j]]--;j++;<br>        &#125;<br>        <span class="hljs-comment">//题目逻辑</span><br>        r = <span class="hljs-built_in">max</span>(r,i-j+<span class="hljs-number">1</span>);<br>    &#125;<br>    cout &lt;&lt; r;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="数组元素目标和"><a href="#数组元素目标和" class="headerlink" title="数组元素目标和"></a>数组元素目标和</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-type">int</span> c[N],f[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n,m,x;cin &gt;&gt; n &gt;&gt; m&gt;&gt;x;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)cin &gt;&gt; c[i];<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; m;i++)cin &gt;&gt; f[i];<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>,j = m<span class="hljs-number">-1</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-keyword">while</span>(c[i]+f[j] &gt; x)j--;<br>        <span class="hljs-keyword">if</span>(c[i]+f[j] == x)&#123;<br>            cout &lt;&lt; i&lt;&lt; <span class="hljs-string">&quot; &quot;</span> &lt;&lt; j;<br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="判断子序列"><a href="#判断子序列" class="headerlink" title="判断子序列"></a>判断子序列</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-type">int</span> c[N],f[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n,m;cin &gt;&gt; n &gt;&gt; m;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)cin &gt;&gt; c[i];<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; m;i++)cin &gt;&gt; f[i];<br>    <span class="hljs-type">int</span> cnt = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>,j = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-keyword">while</span>(c[i]!=f[j]&amp;&amp;j&lt;m)&#123;<br>            j++;<br>        &#125;<br>        <span class="hljs-comment">//cnt用于计数</span><br>        <span class="hljs-keyword">if</span>(j &lt; m)&#123;<br>            cnt++;j++;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span>(cnt==n)cout &lt;&lt; <span class="hljs-string">&quot;Yes&quot;</span>;<br>    <span class="hljs-keyword">else</span> cout &lt;&lt; <span class="hljs-string">&quot;No&quot;</span>;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h2><h4 id="二进制中1的个数"><a href="#二进制中1的个数" class="headerlink" title="二进制中1的个数"></a>二进制中1的个数</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n;cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> tp,cnt=<span class="hljs-number">0</span>;cin &gt;&gt; tp;<br>        <span class="hljs-keyword">while</span>(tp)&#123;<br>            <span class="hljs-comment">//循环判断</span><br>            <span class="hljs-keyword">if</span>(tp &amp; <span class="hljs-number">1</span>)cnt++;<br>            <span class="hljs-comment">//移位操作</span><br>            tp &gt;&gt;= <span class="hljs-number">1</span>;<br>        &#125;<br>        cout &lt;&lt; cnt &lt;&lt;<span class="hljs-string">&quot; &quot;</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="离散化"><a href="#离散化" class="headerlink" title="离散化"></a>离散化</h2><p><strong>特指整数、有序的离散化</strong></p><p>特点：值域很大，但数量很少，因此可以将数值从离散<strong>映射</strong>到连续，</p><p>注意：</p><p>①原数组中可能有重复值 ——&gt; <font color="red">去重</font></p><p>②如何算出x映射后的值 ——&gt;因为原数组有序，所以可以<font color="red">二分</font></p><p>过程：</p><p>1、排序，模拟数轴</p><p>2、去重 all.erase(unique(all.begin(),all.end()),all.end());</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//去重</span><br><span class="hljs-built_in">sort</span>(all.<span class="hljs-built_in">begin</span>(),all.<span class="hljs-built_in">end</span>());<br>all.<span class="hljs-built_in">erase</span>(<span class="hljs-built_in">unique</span>(all.<span class="hljs-built_in">begin</span>(),all.<span class="hljs-built_in">end</span>()),all.<span class="hljs-built_in">end</span>());<br></code></pre></td></tr></table></figure><p>3、对原位置的操作换成对二分之后的位置的操作</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//理解：**将x,l,r单独压入一个vector，访问时只使用vector中的下标进行访问**</span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-keyword">typedef</span> pair&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt; PII;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br>vector&lt;<span class="hljs-type">int</span>&gt; alls;<br>vector&lt;PII&gt; add,query;<br><span class="hljs-type">int</span> a[N],s[N];<br><span class="hljs-comment">//find函数通过二分确定映射位置</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">find</span><span class="hljs-params">(<span class="hljs-type">int</span> q)</span></span>&#123;<br>    <span class="hljs-type">int</span> l = <span class="hljs-number">0</span>, r = alls.<span class="hljs-built_in">size</span>()<span class="hljs-number">-1</span>;<br>    <span class="hljs-keyword">while</span>(l &lt; r)&#123;<br>        <span class="hljs-type">int</span> m = (l+r)/<span class="hljs-number">2</span>;<br>        <span class="hljs-keyword">if</span>(alls[m] &gt;= q) r = m;<br>        <span class="hljs-keyword">else</span> l = m+<span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> r;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n,m;cin &gt;&gt; n &gt;&gt; m;<br>    <span class="hljs-comment">//将离散的坐标x放到vector中，之后排序</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> x,c;cin &gt;&gt; x &gt;&gt; c;<br>        alls.<span class="hljs-built_in">push_back</span>(x);<br>        <span class="hljs-comment">//存储需要增加的值</span><br>        add.<span class="hljs-built_in">push_back</span>(&#123;x,c&#125;);<br>    &#125;<br>    <span class="hljs-comment">//将l,r也放到离散vector中</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; m;i++)&#123;<br>        <span class="hljs-type">int</span> l,r;cin &gt;&gt; l &gt;&gt; r;<br>        alls.<span class="hljs-built_in">push_back</span>(l);<br>        alls.<span class="hljs-built_in">push_back</span>(r);<br>        query.<span class="hljs-built_in">push_back</span>(&#123;l,r&#125;);<br>    &#125;<br>    <span class="hljs-comment">//离散化处理</span><br>    <span class="hljs-comment">//有序化</span><br>    <span class="hljs-built_in">sort</span>(alls.<span class="hljs-built_in">begin</span>(),alls.<span class="hljs-built_in">end</span>());<br>    <span class="hljs-comment">//去重</span><br>    alls.<span class="hljs-built_in">erase</span>(<span class="hljs-built_in">unique</span>(alls.<span class="hljs-built_in">begin</span>(),alls.<span class="hljs-built_in">end</span>()),alls.<span class="hljs-built_in">end</span>());<br>    <span class="hljs-comment">//对所有离散后的x所在坐标+=c</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> x = <span class="hljs-built_in">find</span>(add[i].first);<br>        a[x] += add[i].second;<br>    &#125;<br>    <span class="hljs-comment">//前缀和</span><br>    <span class="hljs-type">int</span> s[N];s[<span class="hljs-number">0</span>] = a[<span class="hljs-number">0</span>];<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt; alls.<span class="hljs-built_in">size</span>();i++)&#123;<br>        s[i] = a[i]+s[i<span class="hljs-number">-1</span>];<br>    &#125;<br>    <span class="hljs-comment">//输出</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; m;i++)&#123;<br>        <span class="hljs-type">int</span> l = <span class="hljs-built_in">find</span>(query[i].first);<br>        <span class="hljs-type">int</span> r = <span class="hljs-built_in">find</span>(query[i].second);<br>        <span class="hljs-keyword">if</span>(!l) cout &lt;&lt; s[r] &lt;&lt; endl;<br>        <span class="hljs-keyword">else</span> cout &lt;&lt; s[r]-s[l<span class="hljs-number">-1</span>] &lt;&lt; endl;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="区间合并"><a href="#区间合并" class="headerlink" title="区间合并"></a>区间合并</h2><p>左端点排序，判断两个区间之间能够存在的三种位置关系</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>,M = <span class="hljs-number">-1e9</span>;<br><span class="hljs-keyword">typedef</span> pair&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt; PII;<br>vector&lt;PII&gt; prd;<br><span class="hljs-comment">//自定义排序方法</span><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">cmp</span><span class="hljs-params">(<span class="hljs-type">const</span> PII &amp;a,<span class="hljs-type">const</span> PII &amp;b)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> a.first &lt;= b.first;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n;cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> l,r; cin &gt;&gt; l &gt;&gt; r;<br>        prd.<span class="hljs-built_in">push_back</span>(&#123;l,r&#125;);<br>    &#125;<br>    <span class="hljs-comment">//区间左端点排序</span><br>    <span class="hljs-built_in">sort</span>(prd.<span class="hljs-built_in">begin</span>(),prd.<span class="hljs-built_in">end</span>(),cmp);<br>    <span class="hljs-comment">//判断区间的三种情况</span><br>    <span class="hljs-type">int</span> i=<span class="hljs-number">1</span>,st=prd[<span class="hljs-number">0</span>].first,ed=prd[<span class="hljs-number">0</span>].second,cnt=<span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">while</span>(i &lt; n)&#123;<br>        <span class="hljs-type">int</span> l = prd[i].first,r = prd[i].second;i++;<br>        <span class="hljs-keyword">if</span>(l &lt;= ed &amp;&amp; r &lt;= ed)<span class="hljs-keyword">continue</span>;<br>        <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(l &lt;= ed &amp;&amp; r &gt; ed)&#123;<br>            ed=r;<span class="hljs-keyword">continue</span>;<br>        &#125;<br>        <span class="hljs-keyword">else</span>&#123;<br>            cnt++;st=l;ed=r;<br>        &#125;<br><br>    &#125;<br>    cout &lt;&lt; cnt;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h2><h4 id="单链表板子"><a href="#单链表板子" class="headerlink" title="单链表板子"></a>单链表板子</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//默认从1开始记录</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span>&#123;<br>    head=<span class="hljs-number">-1</span>;idx=<span class="hljs-number">1</span>;<br>&#125;<br><span class="hljs-comment">//插入头结点</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">add_to_head</span><span class="hljs-params">(<span class="hljs-type">int</span> x)</span></span>&#123;<br>    e[idx]=x;ne[idx]=head;head=idx;idx++;<br>&#125;<br><span class="hljs-comment">//插入</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> k,<span class="hljs-type">int</span> x)</span></span>&#123;<br>    e[idx]=x;ne[idx]=ne[k];ne[k]=idx;idx++;<br>&#125;<br><span class="hljs-comment">//删除</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">remove</span><span class="hljs-params">(<span class="hljs-type">int</span> k)</span></span>&#123;<br>    ne[k] = ne[ne[k]];<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="双链表板子"><a href="#双链表板子" class="headerlink" title="双链表板子"></a>双链表板子</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span>&#123;<br>    r[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span>;<br>    l[<span class="hljs-number">1</span>] = <span class="hljs-number">0</span>;<br>    idx = <span class="hljs-number">2</span>;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> k,<span class="hljs-type">int</span> x)</span></span>&#123;<br>    e[idx] = x;<br>    r[idx] = r[k];<br>    l[idx] = k;<br>    r[k] = idx;<br>    l[r[idx]] = idx;<br>    idx++;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">remove</span><span class="hljs-params">(<span class="hljs-type">int</span> k)</span></span>&#123;<br>    r[l[k]] = r[k];<br>    l[r[k]] = l[k];<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="表达式求值"><a href="#表达式求值" class="headerlink" title="表达式求值"></a>表达式求值</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;stack&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;unordered_map&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-keyword">typedef</span> pair&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt; PII;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br>stack&lt;<span class="hljs-type">char</span>&gt; d;<br>stack&lt;<span class="hljs-type">int</span>&gt; r;<br><span class="hljs-comment">//执行计算</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">eval</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">char</span> p = d.<span class="hljs-built_in">top</span>();d.<span class="hljs-built_in">pop</span>();<br>    <span class="hljs-type">int</span> b = r.<span class="hljs-built_in">top</span>();r.<span class="hljs-built_in">pop</span>();<br>    <span class="hljs-type">int</span> a = r.<span class="hljs-built_in">top</span>();r.<span class="hljs-built_in">pop</span>();<br>    <span class="hljs-keyword">switch</span> (p) &#123;<br>        <span class="hljs-keyword">case</span> <span class="hljs-string">&#x27;+&#x27;</span>: r.<span class="hljs-built_in">push</span>(a+b);<span class="hljs-keyword">break</span>;<br>        <span class="hljs-keyword">case</span> <span class="hljs-string">&#x27;-&#x27;</span>: r.<span class="hljs-built_in">push</span>(a-b);<span class="hljs-keyword">break</span>;<br>        <span class="hljs-keyword">case</span> <span class="hljs-string">&#x27;*&#x27;</span>: r.<span class="hljs-built_in">push</span>(a*b);<span class="hljs-keyword">break</span>;<br>        <span class="hljs-keyword">case</span> <span class="hljs-string">&#x27;/&#x27;</span>: r.<span class="hljs-built_in">push</span>(a/b);<span class="hljs-keyword">break</span>;<br>    &#125;<br>&#125;<br><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    unordered_map&lt;<span class="hljs-type">char</span>,<span class="hljs-type">int</span>&gt; m = &#123;&#123;<span class="hljs-string">&#x27;+&#x27;</span>,<span class="hljs-number">1</span>&#125;,&#123;<span class="hljs-string">&#x27;-&#x27;</span>,<span class="hljs-number">1</span>&#125;,&#123;<span class="hljs-string">&#x27;*&#x27;</span>,<span class="hljs-number">2</span>&#125;,&#123;<span class="hljs-string">&#x27;/&#x27;</span>,<span class="hljs-number">2</span>&#125;&#125;;<br>    <span class="hljs-type">char</span> s[N];<span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%s&quot;</span>,s);<br>    <span class="hljs-comment">//四种情况</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; <span class="hljs-built_in">strlen</span>(s);i++)&#123;<br>        <span class="hljs-keyword">if</span>(<span class="hljs-built_in">isdigit</span>(s[i]))&#123;<br>            <span class="hljs-type">int</span> num = <span class="hljs-number">0</span>;<br>            <span class="hljs-keyword">while</span>(<span class="hljs-built_in">isdigit</span>(s[i]))&#123;<br>                num = num*<span class="hljs-number">10</span>+s[i]-<span class="hljs-string">&#x27;0&#x27;</span>;i++;<br>            &#125;<br>            <span class="hljs-comment">//记得回退一个位置</span><br>            i--;r.<span class="hljs-built_in">push</span>(num);<br>        &#125;<br>        <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(s[i] == <span class="hljs-string">&#x27;(&#x27;</span>) d.<span class="hljs-built_in">push</span>(s[i]);<br>        <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(s[i] == <span class="hljs-string">&#x27;)&#x27;</span>)&#123;<br>            <span class="hljs-keyword">while</span>(d.<span class="hljs-built_in">top</span>()!=<span class="hljs-string">&#x27;(&#x27;</span>) <span class="hljs-built_in">eval</span>();<br>            <span class="hljs-comment">//注意去除&#x27;(&#x27;</span><br>            d.<span class="hljs-built_in">pop</span>();<br>        &#125;<br>        <span class="hljs-keyword">else</span>&#123;<br>            <span class="hljs-comment">//前缀表达式的优先级是&gt;=</span><br>            <span class="hljs-keyword">while</span>(d.<span class="hljs-built_in">size</span>() &amp;&amp; m[d.<span class="hljs-built_in">top</span>()] &gt;= m[s[i]])<span class="hljs-built_in">eval</span>();<br>            d.<span class="hljs-built_in">push</span>(s[i]);<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">while</span>(d.<span class="hljs-built_in">size</span>()) <span class="hljs-built_in">eval</span>();<br>    cout &lt;&lt; r.<span class="hljs-built_in">top</span>();<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure><h2 id="单调栈：左边最小的数"><a href="#单调栈：左边最小的数" class="headerlink" title="单调栈：左边最小的数"></a>单调栈：左边最小的数</h2><p>给定一个长度为 N 的整数数列，输出每个数左边第一个比它小的数，如果不存在则输出 −1。</p><p>思路：保证栈内的元素保序单调上升。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n;cin &gt;&gt; n;<br>    stack&lt;<span class="hljs-type">int</span>&gt; s;<br>    <span class="hljs-keyword">while</span>(n--)&#123;<br>        <span class="hljs-type">int</span> t;cin &gt;&gt; t;<br>        <span class="hljs-comment">//保序单调上升</span><br>        <span class="hljs-keyword">while</span> (!s.<span class="hljs-built_in">empty</span>() &amp;&amp; s.<span class="hljs-built_in">top</span>()&gt;=t)s.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-keyword">if</span>(s.<span class="hljs-built_in">empty</span>())cout &lt;&lt; <span class="hljs-string">&quot;-1&quot;</span> &lt;&lt; <span class="hljs-string">&quot; &quot;</span>;<br>        <span class="hljs-keyword">else</span> cout &lt;&lt; s.<span class="hljs-built_in">top</span>() &lt;&lt; <span class="hljs-string">&quot; &quot;</span>;<br>        s.<span class="hljs-built_in">push</span>(t);<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="单调队列：滑动窗口"><a href="#单调队列：滑动窗口" class="headerlink" title="单调队列：滑动窗口"></a>单调队列：滑动窗口</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-type">int</span> n,k;<br><span class="hljs-comment">//q数组存储元素下标所构成的队列</span><br><span class="hljs-type">int</span> num[N],q[N],fr=<span class="hljs-number">0</span>,rr=<span class="hljs-number">-1</span>;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span>&#123;<br>    fr = rr = <span class="hljs-number">0</span>;<br>    q[fr] = <span class="hljs-number">0</span>;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">empty</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-keyword">return</span> fr == rr;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n &gt;&gt; k;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        cin &gt;&gt; num[i];<br>    &#125;<br>    <span class="hljs-comment">//比如滑动窗口里有 3,-1，那么当-3进入到窗口之后，有3,-1的时候一定有-3，因此3和-1可以直接丢弃</span><br>    <span class="hljs-comment">//因此只需要保证队列里是单调递增的，即可以以O(1)的复杂度拿到最小值</span><br>    <span class="hljs-built_in">init</span>();<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-comment">//移动左端点，如果fr所指的下标对应的元素不在窗口里了，则右推</span><br>        <span class="hljs-keyword">if</span>(!<span class="hljs-built_in">empty</span>() &amp;&amp; q[fr]&lt;i-k+<span class="hljs-number">1</span>)fr++;<br>        <span class="hljs-comment">//当右端点的值大于i的值时，队列右端点左推</span><br>        <span class="hljs-keyword">while</span>(!<span class="hljs-built_in">empty</span>() &amp;&amp; num[q[rr<span class="hljs-number">-1</span>]] &gt; num[i]) rr--;<br>        <span class="hljs-comment">//压入队列</span><br>        q[rr]=i;rr++;<br>        <span class="hljs-comment">//输出</span><br>        <span class="hljs-keyword">if</span>(i &gt;= k<span class="hljs-number">-1</span>) cout &lt;&lt; num[q[fr]] &lt;&lt; <span class="hljs-string">&quot; &quot;</span>;<br>    &#125;<br>    <span class="hljs-built_in">init</span>();cout&lt;&lt;endl;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-keyword">if</span>(!<span class="hljs-built_in">empty</span>() &amp;&amp; q[fr]&lt;i-k+<span class="hljs-number">1</span>)fr++;<br>        <span class="hljs-keyword">while</span>(!<span class="hljs-built_in">empty</span>() &amp;&amp; num[q[rr<span class="hljs-number">-1</span>]] &lt; num[i])rr--;<br>        q[rr]=i;rr++;<br>        <span class="hljs-keyword">if</span>(i &gt;= k<span class="hljs-number">-1</span>) cout &lt;&lt; num[q[fr]] &lt;&lt; <span class="hljs-string">&quot; &quot;</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure><h2 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h2><h4 id="合并集合"><a href="#合并集合" class="headerlink" title="合并集合"></a>合并集合</h4><p>1、首先将每个数做成一个集合</p><p>2、合并的时候将p[a] &#x3D; b；</p><p><strong>三种操作：注意这里找父节点用的是find函数</strong><br>1、集合合并：<strong>p[find[a]] &#x3D; find[b]</strong><br>2、判断是否属于同一个集合：<strong>find[a] &#x3D;&#x3D; find[b]</strong><br>3、找到父节点：<strong>p[a] &#x3D; find(p[a])</strong></p><p><strong>并查集操作：</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">find</span><span class="hljs-params">(<span class="hljs-type">int</span> x)</span></span>&#123;<br>    <span class="hljs-comment">//如果不是根节点，则一直上溯，上溯到的一定是根节点，然后在这条路上的所有点的父节点都变成根节点（路径压缩）</span><br>    <span class="hljs-keyword">if</span>(p[x] != x) p[x] = <span class="hljs-built_in">find</span>(p[x]);<br>    <span class="hljs-keyword">return</span> p[x];<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h2><h4 id="排列数字"><a href="#排列数字" class="headerlink" title="排列数字"></a>排列数字</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">10</span>;<br><span class="hljs-type">int</span> st[N],p[N],n;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">DFS</span><span class="hljs-params">(<span class="hljs-type">int</span> cnt)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(cnt &gt; n)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>            cout &lt;&lt; p[i] &lt;&lt; <span class="hljs-string">&quot; &quot;</span>;<br>        &#125;<br>        cout &lt;&lt; endl;<br>    &#125;<br>    <span class="hljs-comment">//st数组用于记录是否已经使用，p数组按顺序存储已经使用过的数据回溯。</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">if</span>(!st[i])&#123;<br>            st[i]=<span class="hljs-number">1</span>;p[cnt]=i;<br>            <span class="hljs-built_in">DFS</span>(cnt+<span class="hljs-number">1</span>);<br>            st[i]=<span class="hljs-number">0</span>;p[cnt]=<span class="hljs-number">0</span>;<br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-built_in">DFS</span>(<span class="hljs-number">1</span>);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="八皇后"><a href="#八皇后" class="headerlink" title="八皇后"></a>八皇后</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//三种下棋的方向，r在迭代中有所表现</span><br><span class="hljs-type">int</span> c[N],d[M],u[M],n;<br><span class="hljs-comment">//记录下棋的方位</span><br><span class="hljs-type">char</span> s[N][N];<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">dfs</span><span class="hljs-params">(<span class="hljs-type">int</span> cnt)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(cnt &gt; n)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= n;j++)&#123;<br>                cout &lt;&lt; s[i][j];<br>            &#125;<br>            cout &lt;&lt; endl;<br>        &#125;<br>        cout &lt;&lt; endl;<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-comment">//注意这里的截距算法</span><br>        <span class="hljs-keyword">if</span>(!c[i] &amp;&amp; !d[cnt-i+n] &amp;&amp; !u[i+cnt])&#123;<br>            s[cnt][i] = <span class="hljs-string">&#x27;Q&#x27;</span>;<br>            c[i] = d[cnt-i+n] = u[i+cnt] = <span class="hljs-number">1</span>;<br>            <span class="hljs-built_in">dfs</span>(cnt+<span class="hljs-number">1</span>);<br>            s[cnt][i] = <span class="hljs-string">&#x27;.&#x27;</span>;<br>            c[i] = d[cnt-i+n] = u[i+cnt] = <span class="hljs-number">0</span>;<br>        &#125;<br>    &#125;<br>&#125;<br><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= n;j++)&#123;<br>            s[i][j] = <span class="hljs-string">&#x27;.&#x27;</span>;<br>        &#125;<br>    &#125;<br>    <span class="hljs-built_in">dfs</span>(<span class="hljs-number">1</span>);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="BFS"><a href="#BFS" class="headerlink" title="BFS"></a>BFS</h2><h4 id="走迷宫"><a href="#走迷宫" class="headerlink" title="走迷宫"></a>走迷宫</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;queue&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">110</span>;<br><span class="hljs-keyword">typedef</span> pair&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt; PII;<br>queue&lt;PII&gt; q;<span class="hljs-type">int</span> n,m,sum=<span class="hljs-number">0</span>;<br><span class="hljs-comment">//d记录最短距离，理论上第一次到达时的距离一定是最短距离</span><br><span class="hljs-type">int</span> dx[<span class="hljs-number">4</span>] = &#123;<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">-1</span>&#125;,dy[<span class="hljs-number">4</span>] = &#123;<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">-1</span>,<span class="hljs-number">0</span>&#125;,loc[N][N],d[N][N];<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">bfs</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">memset</span>(d,<span class="hljs-number">-1</span>,<span class="hljs-keyword">sizeof</span> d);<br>    d[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>]=<span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">while</span>(!q.<span class="hljs-built_in">empty</span>()) &#123;<br>        PII p = q.<span class="hljs-built_in">front</span>();<br>        q.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-type">int</span> x = p.first, y = p.second;<br>        <span class="hljs-keyword">if</span> (x == n &amp;&amp; y == m) &#123;<br>            cout &lt;&lt; d[x][y];<br>            <span class="hljs-keyword">return</span>;<br>        &#125;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;<br>            <span class="hljs-type">int</span> xx = x + dx[i], yy = y + dy[i];<br>            <span class="hljs-keyword">if</span> (xx &gt;= <span class="hljs-number">1</span> &amp;&amp; xx &lt;= n &amp;&amp; yy &gt;= <span class="hljs-number">1</span> &amp;&amp; yy &lt;= m &amp;&amp; !loc[xx][yy] &amp;&amp; d[xx][yy]==<span class="hljs-number">-1</span>) &#123;<br>                q.<span class="hljs-built_in">push</span>(&#123;xx, yy&#125;);d[xx][yy]=d[x][y]+<span class="hljs-number">1</span>;<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n &gt;&gt; m;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= m;j++)&#123;<br>            cin &gt;&gt; loc[i][j];<br>        &#125;<br>    &#125;<br>    q.<span class="hljs-built_in">push</span>(&#123;<span class="hljs-number">1</span>,<span class="hljs-number">1</span>&#125;);<br>    <span class="hljs-built_in">bfs</span>();<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure><h2 id="01背包"><a href="#01背包" class="headerlink" title="01背包"></a>01背包</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1010</span>;<br><span class="hljs-type">int</span> v[N],w[N],f[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n,m;cin &gt;&gt; n &gt;&gt;m;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        cin &gt;&gt; v[i] &gt;&gt; w[i];<br>    &#125;<br>    <span class="hljs-comment">//降到一维之后，原有的不选第i个物品就直接随着i的增加而自动迭代到了下一层</span><br>    <span class="hljs-comment">//但是，如果还是从小到大进行迭代，可能f[i][j]需要f[i-1][j-v[i]]的值，但是f[i-1][j-v[i]]在前面迭代成了f[i][j-v[i]]</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = m;j &gt;= v[i];j--)&#123;<br>            f[j] = <span class="hljs-built_in">max</span>(f[j],f[j-v[i]]+w[i]);<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; f[m] &lt;&lt; endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="完全背包"><a href="#完全背包" class="headerlink" title="完全背包"></a>完全背包</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1010</span>;<br><br><span class="hljs-type">int</span> v[N],w[N],f[N];<br><br><span class="hljs-comment">/*</span><br><span class="hljs-comment">    01背包的转移方程 f[i][j] = max(f[i-1][j],f[i-1][j-v[i]]+w[i])</span><br><span class="hljs-comment">    完全背包中选择第i件物品的推导</span><br><span class="hljs-comment">    f[i][j] = max(f[i-1][j-v[i]]+w[i],f[i-1][j-2v[i]]+2w[i]+...)</span><br><span class="hljs-comment">    减一个v</span><br><span class="hljs-comment">    f[i][j-v[i]] = max(f[i-1][j-2v[i]]+w[i],f[i-1][j-3v[i]]+2w[i]+...)</span><br><span class="hljs-comment">    所以</span><br><span class="hljs-comment">    完全背包转移方程 f[i][j] = max(f[i-1][j],f[i][j-v[i]]+w[i])</span><br><span class="hljs-comment">    </span><br><span class="hljs-comment">    所以01背包和完全背包的区别在于选择i时的更新是从i-1更新还是从i更新</span><br><span class="hljs-comment">    </span><br><span class="hljs-comment">    所以01背包从后往前遍历，防止i-1 --&gt; i</span><br><span class="hljs-comment">    而完全背包正需要将i-1 --&gt; i，所以从前向后遍历</span><br><span class="hljs-comment">    </span><br><span class="hljs-comment">*/</span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n,m;<br>    cin &gt;&gt; n &gt;&gt; m;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-type">int</span> a,b;<br>        cin &gt;&gt; a &gt;&gt; b;<br>        v[i] = a;<br>        w[i] = b;<br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-comment">//从v[i]开始</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = v[i];j &lt;= m;j++)&#123;<br>            f[j] = <span class="hljs-built_in">max</span>(f[j],f[j-v[i]]+w[i]);<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; f[m] &lt;&lt; endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="多重背包"><a href="#多重背包" class="headerlink" title="多重背包"></a>多重背包</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1010</span>;<br><span class="hljs-type">int</span> v[N],w[N],s[N],f[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n,m;cin &gt;&gt; n &gt;&gt;m;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        cin &gt;&gt; v[i] &gt;&gt; w[i] &gt;&gt; s[i];<br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = m;j &gt;= v[i];j--)&#123;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> k = <span class="hljs-number">1</span>;k &lt;= s[i];k++)&#123;<br>                <span class="hljs-keyword">if</span>(j &gt;= k*v[i]) f[j] = <span class="hljs-built_in">max</span>(f[j],f[j-k*v[i]]+k*w[i]);<br>            &#125;<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; f[m] &lt;&lt; endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="分组背包"><a href="#分组背包" class="headerlink" title="分组背包"></a>分组背包</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">110</span>;<br><br><span class="hljs-type">int</span> f[N];<br><span class="hljs-type">int</span> v[N][N],w[N][N],s[N];<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n,m;<br>    cin &gt;&gt; n &gt;&gt; m;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        cin &gt;&gt; s[i];<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt; s[i];j++)&#123;<br>            cin &gt;&gt; v[i][j] &gt;&gt; w[i][j];<br>        &#125;<br>    &#125;<br>    <br>    <span class="hljs-comment">//01背包</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = m;j &gt;= <span class="hljs-number">1</span>;j--)&#123;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> k = <span class="hljs-number">0</span>;k &lt; s[i];k++)&#123;<br>                <span class="hljs-keyword">if</span>(j &gt;= v[i][k])f[j] = <span class="hljs-built_in">max</span>(f[j],f[j-v[i][k]]+w[i][k]);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <br>    cout &lt;&lt; f[m] &lt;&lt; endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="线性DP"><a href="#线性DP" class="headerlink" title="线性DP"></a>线性DP</h2><h4 id="数字金字塔"><a href="#数字金字塔" class="headerlink" title="数字金字塔"></a>数字金字塔</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">510</span>;<br><span class="hljs-type">int</span> f[N][N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n;<br>    cin &gt;&gt; n;<br>    <span class="hljs-type">int</span> cnt=<span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">while</span>(cnt++ &lt; n)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= cnt;i++)&#123;<br>            cin &gt;&gt; f[cnt][i];<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//从下向上可以解决分支问题</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = n<span class="hljs-number">-1</span>;i &gt; <span class="hljs-number">0</span>;i--)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= i;j++)&#123;<br>            f[i][j] = <span class="hljs-built_in">max</span>(f[i+<span class="hljs-number">1</span>][j],f[i+<span class="hljs-number">1</span>][j+<span class="hljs-number">1</span>])+f[i][j];<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; f[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>];<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="上升子序列（优化到O-nlogn-）"><a href="#上升子序列（优化到O-nlogn-）" class="headerlink" title="上升子序列（优化到O(nlogn)）"></a>上升子序列（优化到O(nlogn)）</h4><p><strong>O(n^2)做法：</strong>第n个元素所在的上升子序列的最大值取决于倒数第二个元素是什么，找到倒数第二个元素所对应的最长子序列，然后+1即可。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-type">int</span> a[N],f[N];<br><span class="hljs-comment">//题的思路是这么来的</span><br><span class="hljs-comment">//数据冗余：相同的最长上升子序列，但是当前值不一样，我们期待使用更小的值而不是更大的值，因为值越小其后面更可能有比这个数大的数</span><br><span class="hljs-comment">//所以定义一个数组f存储每次迭代到的最长为k的序列对应的当前位置m</span><br><span class="hljs-comment">//当找到一个f[i-1]&lt;a&lt;f[i]的时候，证明a一定是能够在i-1之后加上</span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)cin &gt;&gt; a[i];<br>    <span class="hljs-type">int</span> len = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//保证有一个数小于a[i]</span><br>    f[<span class="hljs-number">0</span>] = <span class="hljs-number">-2e9</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-comment">//找到一个小于a[i]的最大的f值</span><br>        <span class="hljs-comment">//复习二分</span><br>        <span class="hljs-type">int</span> l = <span class="hljs-number">0</span>,r = len;<br>        <span class="hljs-keyword">while</span>(l &lt; r)&#123;<br>            <span class="hljs-type">int</span> mid = (l+r+<span class="hljs-number">1</span>) &gt;&gt; <span class="hljs-number">1</span>;<br>            <span class="hljs-keyword">if</span>(f[mid] &lt; a[i]) l = mid;<br>            <span class="hljs-keyword">else</span> r = mid<span class="hljs-number">-1</span>;<br>        &#125;<br>        <span class="hljs-comment">//对应位置r+1是找到的f值对应的距离点</span><br>        <span class="hljs-comment">//更新最大距离</span><br>        len = <span class="hljs-built_in">max</span>(len,r+<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//如果f[r]&lt;a[i]&lt;f[r+1]，在对f[r]后面加一个a[i]一定会有f[r+1]再次变小，我们期待使用更小的数</span><br>        <span class="hljs-comment">//所以需要更新f[r+1]</span><br>        f[r+<span class="hljs-number">1</span>] = a[i];<br>    &#125;<br>    cout &lt;&lt; len;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="最长公共子序列"><a href="#最长公共子序列" class="headerlink" title="最长公共子序列"></a>最长公共子序列</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">/*</span><br><span class="hljs-comment">    分析</span><br><span class="hljs-comment">    状态表示 f[i][j]</span><br><span class="hljs-comment">            定义：表示a的前i个字符和b的前j个字符所组成的所有子序列</span><br><span class="hljs-comment">            属性：长度最大值</span><br><span class="hljs-comment">    状态计算：看成集合</span><br><span class="hljs-comment">            这个状态下产生不同的原因是是否有a[i]或者b[j]</span><br><span class="hljs-comment">            f[i][j]的计算针对于a[i],b[j]可以由以下几个部分组成：</span><br><span class="hljs-comment">                1、有a[i]和b[j]     if(a[i] == b[j]) f[i][j] = f[i-1][j-1]+1;</span><br><span class="hljs-comment">                2、没有a[i]或b[j]    f[i][j] = f[i-1][j-1]</span><br><span class="hljs-comment">                3、有a[i]没b[j]   f[i][j-1]表示的是a的前i个字符和b的前j-1个字符的子序列最大值，包含了当前情况</span><br><span class="hljs-comment">                                  而且f[i][j-1]是不超过集合f[i][j]的，即f[i][j-1]集合的最大值包含当前情况且没有出界</span><br><span class="hljs-comment">                                  所以可以用f[i][j-1]代替本情况</span><br><span class="hljs-comment">                4、有b[i]没a[i]   同上,用f[i-1][j]代替本情况</span><br><span class="hljs-comment">*/</span> <br><span class="hljs-type">int</span> f[N][N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br><br>    <span class="hljs-type">int</span> n,m;<br>    <span class="hljs-type">char</span> a[N],b[N];<br>    cin &gt;&gt; n &gt;&gt; m &gt;&gt; a+<span class="hljs-number">1</span> &gt;&gt; b+<span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= m;j++)&#123;<br>            f[i][j] = <span class="hljs-built_in">max</span>(f[i<span class="hljs-number">-1</span>][j],f[i][j<span class="hljs-number">-1</span>]);<br>            <span class="hljs-keyword">if</span>(a[i]==b[j]) f[i][j] = <span class="hljs-built_in">max</span>(f[i][j],f[i<span class="hljs-number">-1</span>][j<span class="hljs-number">-1</span>]+<span class="hljs-number">1</span>);<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; f[n][m];<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="最短编辑距离"><a href="#最短编辑距离" class="headerlink" title="最短编辑距离"></a>最短编辑距离</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">/*</span><br><span class="hljs-comment">    1、状态表示：f[i][j]</span><br><span class="hljs-comment">        定义：从a[1..i]到b[1..j]的操作</span><br><span class="hljs-comment">        属性：求操作次数最小值</span><br><span class="hljs-comment">    2、状态计算：（从最后的操作出发）</span><br><span class="hljs-comment">        对于一个f[i][j]，求解的情况有三种（注意这里，对a的删除理论上和对b的增加一致，所以只考虑对a的操作）</span><br><span class="hljs-comment">        a.删除   f[i][j] = f[i-1][j]+1 （从a的后面删除一个字符）</span><br><span class="hljs-comment">        b.增加   f[i][j] = f[i][j-1]+1 （在a的后面增加一个字符）</span><br><span class="hljs-comment">            最后一步是增加：若最后一步是增加，为了使a[1 ~ i] = b[1 ~ j]那最后一步增加的一定是b[j]。</span><br><span class="hljs-comment">            那么f[i][j]就可以看成是，首先将a[1 ~ i]转化成b[1 ~ j - 1]，这时候所操作的步骤数为f[i][j - 1]，下一步在b[j - 1]后面添加一个b[j]，那么f[i][j] = f[i][j - 1] + 1。</span><br><span class="hljs-comment">            也就是说，我们的转化过程为：a[1 ~ i] -&gt; b[1 ~ j - 1] -&gt; b[1 ~ j]，</span><br><span class="hljs-comment">            f[i][j]的意义是将a[1 ~ i]转化为b[1 ~ j]的步骤数，不能理解为直接在a[i]后面添加一个数，这是一个动态的变化的过程。</span><br><span class="hljs-comment">        c.修改   f[i][j] = if(a[i] == b[j])f[i-1][j-1]（不用改） else f[i-1][j-1]+1（加上修改操作）</span><br><span class="hljs-comment">*/</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n,m;<br>    <span class="hljs-type">char</span> a[N],b[N];<br>    cin &gt;&gt; n &gt;&gt; a+<span class="hljs-number">1</span> &gt;&gt; m &gt;&gt; b+<span class="hljs-number">1</span>;<br><br>    <span class="hljs-comment">//判断一下是否需要边界条件</span><br>    <span class="hljs-comment">//只进行增加操作</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt;= m;i++)f[<span class="hljs-number">0</span>][i] = i;<br>    <span class="hljs-comment">//只进行删除操作</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt;= n;i++)f[i][<span class="hljs-number">0</span>] = i;<br><br>    <span class="hljs-comment">//dp过程</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= m;j++)&#123;<br>            <span class="hljs-comment">//增加和删除</span><br>            f[i][j] = <span class="hljs-built_in">min</span>(f[i<span class="hljs-number">-1</span>][j]+<span class="hljs-number">1</span>,f[i][j<span class="hljs-number">-1</span>]+<span class="hljs-number">1</span>);<br>            <span class="hljs-comment">//修改判断</span><br>            <span class="hljs-keyword">if</span>(a[i] == b[j]) f[i][j] = <span class="hljs-built_in">min</span>(f[i][j],f[i<span class="hljs-number">-1</span>][j<span class="hljs-number">-1</span>]);<br>            <span class="hljs-keyword">else</span> f[i][j] = <span class="hljs-built_in">min</span>(f[i][j],f[i<span class="hljs-number">-1</span>][j<span class="hljs-number">-1</span>]+<span class="hljs-number">1</span>);<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; f[n][m];<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="区间DP"><a href="#区间DP" class="headerlink" title="区间DP"></a>区间DP</h2><h4 id="石子合并"><a href="#石子合并" class="headerlink" title="石子合并"></a>石子合并</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs c++"><br><span class="hljs-comment">/*</span><br><span class="hljs-comment">    1、集合表示f[i][j]</span><br><span class="hljs-comment">        定义：从第i个石子到第j个石子的所有合并情况</span><br><span class="hljs-comment">        属性：代价最小值</span><br><span class="hljs-comment">    2、集合计算</span><br><span class="hljs-comment">        对于一个从i到j的集合，可以从[i,i],[i+1,j] / [i,i+1],[i+2,j] / ... / [i,k],[k+1,j] / ... / [i,j-1],[j,j]这样来枚举</span><br><span class="hljs-comment">        即对每一个可能的合并情况进行处理</span><br><span class="hljs-comment">*/</span><br><br><span class="hljs-type">int</span> n;<br><span class="hljs-type">int</span> s[N],a[N],f[N][N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)cin &gt;&gt; a[i];<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)s[i] = a[i]+s[i<span class="hljs-number">-1</span>];<br><br>    <span class="hljs-comment">//长度为1时不需要合并，为0</span><br>    <span class="hljs-comment">// for(int i = 1;i &lt;= n;i++)f[i][i] = a[i];</span><br>    <span class="hljs-comment">//区间dp的第一步一般为对区间长度进行枚举</span><br>    <span class="hljs-comment">//O(n^3)的时间复杂度</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> len = <span class="hljs-number">2</span>;len &lt;= n;len++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i + len<span class="hljs-number">-1</span> &lt;= n;i++)&#123;<br>            <span class="hljs-type">int</span> j = i+len<span class="hljs-number">-1</span>;<br>            f[i][j] = <span class="hljs-number">1e7</span>;<br>            <span class="hljs-comment">//转移方程</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> k = i;k &lt;= j<span class="hljs-number">-1</span>;k++)&#123;<br>                f[i][j] = <span class="hljs-built_in">min</span>(f[i][j],f[i][k]+f[k+<span class="hljs-number">1</span>][j]);<br>            &#125;<br>            <span class="hljs-comment">//f[i][j] = f[i][k]+f[k][j]+s[j]-s[i-1];</span><br>            f[i][j] += s[j]-s[i<span class="hljs-number">-1</span>];<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; f[<span class="hljs-number">1</span>][n];<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="计数类DP"><a href="#计数类DP" class="headerlink" title="计数类DP"></a>计数类DP</h2><h4 id="整数划分"><a href="#整数划分" class="headerlink" title="整数划分"></a>整数划分</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-comment">/*</span><br><span class="hljs-comment">    1、状态表示f[i][j]：</span><br><span class="hljs-comment">        定义：总和为i，划分个数为j的方案</span><br><span class="hljs-comment">        属性：方案数量</span><br><span class="hljs-comment">    2、状态计算</span><br><span class="hljs-comment">        初始化：</span><br><span class="hljs-comment">        f[i][0] = 0;</span><br><span class="hljs-comment">        f[0][i] = 0;</span><br><span class="hljs-comment">        f[0][0] = 1;</span><br><span class="hljs-comment">        集合划分：将f[i][j]划分成方案中有1和方案中没有1</span><br><span class="hljs-comment">            对于方案中有1 f[i][j] = f[i-1][j-1]</span><br><span class="hljs-comment">            对于方案中没有1 f[i][j] = f[i-j][j]</span><br><span class="hljs-comment">        所以</span><br><span class="hljs-comment">            f[i][j] = f[i-1][j-1]+f[i-j][j]</span><br><span class="hljs-comment">        最后需要把所有情况加和</span><br><span class="hljs-comment"></span><br><span class="hljs-comment">*/</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1010</span>,mod = <span class="hljs-number">1e9</span>+<span class="hljs-number">7</span>;<br><span class="hljs-type">int</span> f[N][N];<br><span class="hljs-type">int</span> n;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    f[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= i;j++)&#123;<br>            f[i][j] = (f[i<span class="hljs-number">-1</span>][j<span class="hljs-number">-1</span>]+f[i-j][j])%mod;<br>        &#125; <br>    &#125;<br>    <span class="hljs-type">int</span> r = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++) r = (r+f[n][i]) % mod;<br>    cout &lt;&lt; r;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="状态DP"><a href="#状态DP" class="headerlink" title="状态DP"></a>状态DP</h2><h4 id="哈密特路径"><a href="#哈密特路径" class="headerlink" title="哈密特路径"></a>哈密特路径</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">21</span>,M = <span class="hljs-number">1</span> &lt;&lt; <span class="hljs-number">20</span>;<br><span class="hljs-type">int</span> n;<br><span class="hljs-type">int</span> f[M][N],w[N][N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt; n;j++)&#123;<br>            cin &gt;&gt; w[i][j];<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//因为求最小值，所以初始化每个点是无限大</span><br>    <span class="hljs-built_in">memset</span>(f,<span class="hljs-number">0x3f</span>,<span class="hljs-keyword">sizeof</span> f);<br>    <span class="hljs-comment">//从1开始走</span><br>    f[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//从0到111...1进行状态遍历</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; <span class="hljs-number">1</span>&lt;&lt;n;i++)&#123;<br>        <span class="hljs-comment">//判断当前状态下是否有每个点j</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt; n;j++)&#123;  <br>            <span class="hljs-comment">//如果存在j</span><br>            <span class="hljs-keyword">if</span>(i &gt;&gt; j &amp; <span class="hljs-number">1</span>)&#123;<br>                <span class="hljs-comment">//则判断去掉j点之后是否含k点，有的话代表可以从k点转移到j点</span><br>                <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> k = <span class="hljs-number">0</span>;k &lt; n;k++)&#123;<br>                    <span class="hljs-keyword">if</span>(i ^ (<span class="hljs-number">1</span> &lt;&lt; j) &amp; k)&#123;<br>                        <span class="hljs-comment">//可以转移的话则判断是否可以取到最小值</span><br>                        f[i][j] = <span class="hljs-built_in">min</span>(f[i][j],f[i ^ (<span class="hljs-number">1</span> &lt;&lt; j)][k]+w[k][j]);<br>                    &#125; <br>                &#125;<br>            &#125;<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; f[(<span class="hljs-number">1</span>&lt;&lt;n)<span class="hljs-number">-1</span>][n<span class="hljs-number">-1</span>];<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="树形DP"><a href="#树形DP" class="headerlink" title="树形DP"></a>树形DP</h2><h4 id="没有上司的酒会"><a href="#没有上司的酒会" class="headerlink" title="没有上司的酒会"></a>没有上司的酒会</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">6010</span>;<br><span class="hljs-type">int</span> h[N],e[N],ne[N],hp[N],up[N],n,idx;<br><span class="hljs-type">int</span> f[N][<span class="hljs-number">2</span>];<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> a,<span class="hljs-type">int</span> b)</span></span>&#123;<br>    e[idx] = b;ne[idx]=h[a];h[a]=idx++;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">dfs</span><span class="hljs-params">(<span class="hljs-type">int</span> n)</span></span>&#123;<br>    <span class="hljs-comment">//选择n点</span><br>    f[n][<span class="hljs-number">1</span>] = hp[n];<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = h[n];i != <span class="hljs-number">-1</span>;i = ne[i])&#123;<br>        <span class="hljs-comment">//dfs</span><br>        <span class="hljs-type">int</span> d = e[i];<span class="hljs-built_in">dfs</span>(d);<br>        <span class="hljs-comment">//如果不选择n点，则d可选可不选</span><br>        f[n][<span class="hljs-number">0</span>] += <span class="hljs-built_in">max</span>(f[d][<span class="hljs-number">0</span>],f[d][<span class="hljs-number">1</span>]);<br>        <span class="hljs-comment">//如果选择n点，则d必不选</span><br>        f[n][<span class="hljs-number">1</span>] += f[d][<span class="hljs-number">0</span>];<br>    &#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-comment">//初始化</span><br>    cin &gt;&gt; n;<span class="hljs-built_in">memset</span>(h,<span class="hljs-number">-1</span>,<span class="hljs-keyword">sizeof</span> h);<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        cin &gt;&gt; hp[i];<br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n<span class="hljs-number">-1</span>;i++)&#123;<br>        <span class="hljs-type">int</span> a,b;cin &gt;&gt; a &gt;&gt; b;<br>        up[a]=<span class="hljs-number">1</span>;<span class="hljs-built_in">add</span>(b,a);<br>    &#125;<br>    <span class="hljs-comment">//找到根节点</span><br>    <span class="hljs-type">int</span> root=<span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">while</span>(up[root]) root++;<br>    <span class="hljs-built_in">dfs</span>(root);<br>    cout &lt;&lt; <span class="hljs-built_in">max</span>(f[root][<span class="hljs-number">0</span>],f[root][<span class="hljs-number">1</span>]);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="记忆化路径"><a href="#记忆化路径" class="headerlink" title="记忆化路径"></a>记忆化路径</h2><h4 id="滑雪"><a href="#滑雪" class="headerlink" title="滑雪"></a>滑雪</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">310</span>;<br><span class="hljs-type">int</span> n,m;<br><span class="hljs-type">int</span> f[N][N];<br><span class="hljs-type">int</span> h[N][N];<br><span class="hljs-type">int</span> dx[<span class="hljs-number">4</span>] = &#123;<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">-1</span>&#125;;<br><span class="hljs-type">int</span> dy[<span class="hljs-number">4</span>] = &#123;<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">-1</span>,<span class="hljs-number">0</span>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">dp</span><span class="hljs-params">(<span class="hljs-type">int</span> x,<span class="hljs-type">int</span> y)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(f[x][y]) <span class="hljs-keyword">return</span> f[x][y];<span class="hljs-comment">//如果已经记录过了，就不用再算了 </span><br>    f[x][y]=<span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;<span class="hljs-number">4</span>;i++)<br>    &#123;<br>        <span class="hljs-type">int</span> xx=x+dx[i];<span class="hljs-comment">//到下一个点 </span><br>        <span class="hljs-type">int</span> yy=y+dy[i];<br>        <span class="hljs-keyword">if</span>(xx&gt;=<span class="hljs-number">1</span>&amp;&amp;xx&lt;=n&amp;&amp;yy&gt;=<span class="hljs-number">1</span>&amp;&amp;yy&lt;=m&amp;&amp;h[x][y]&gt;h[xx][yy])<span class="hljs-comment">//点在范围内，且此点高度，比刚才的点矮就滑过去 </span><br>        &#123;<br>            f[x][y]=<span class="hljs-built_in">max</span>(f[x][y],<span class="hljs-built_in">dp</span>(xx,yy)+<span class="hljs-number">1</span>);<span class="hljs-comment">//比较大小，取出最大的 </span><br>        &#125;                                       <span class="hljs-comment">//+1，是因为如果到从两个点的滑雪距离一样的话，再滑一步，就滑到x，y这个点了，所以+1 </span><br>    &#125;<br>    <span class="hljs-keyword">return</span> f[x][y];<span class="hljs-comment">//返回最终值 </span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n &gt;&gt; m;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= m;j++)&#123;<br>            cin &gt;&gt; h[i][j];<br>        &#125;<br>    &#125;<br>    <span class="hljs-type">int</span> r = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= m;j++)&#123;<br>            r = <span class="hljs-built_in">max</span>(r,<span class="hljs-built_in">dp</span>(i,j));<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; r;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="区间问题"><a href="#区间问题" class="headerlink" title="区间问题"></a>区间问题</h2><h4 id="区间选点-x2F-最大不相交区间数量"><a href="#区间选点-x2F-最大不相交区间数量" class="headerlink" title="区间选点&#x2F;最大不相交区间数量"></a>区间选点&#x2F;最大不相交区间数量</h4><p>右排序-&gt;判断左端点</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-keyword">typedef</span> pair&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt; PII;<br>vector&lt;PII&gt; v;<br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">cmp</span><span class="hljs-params">(PII a,PII b)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> a.second &lt;= b.second;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n;cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i =<span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> l,r;cin &gt;&gt; l &gt;&gt; r;<br>        v.<span class="hljs-built_in">push_back</span>(&#123;l,r&#125;);<br>    &#125;<br>    <span class="hljs-built_in">sort</span>(v.<span class="hljs-built_in">begin</span>(),v.<span class="hljs-built_in">end</span>(),cmp);<br>    <span class="hljs-type">int</span> r = <span class="hljs-number">-2e9</span>,sum = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> l = v[i].first;<br>        <span class="hljs-keyword">if</span>(r &lt; l)&#123;<br>            sum++;r = v[i].second;<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; sum;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="区间分组"><a href="#区间分组" class="headerlink" title="区间分组"></a>区间分组</h4><p>左端点排序-&gt;小根堆存储每个分组的最右端点。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;queue&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-keyword">typedef</span> pair&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt; PII;<br>vector&lt;PII&gt; v;<br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">cmp</span><span class="hljs-params">(PII a,PII b)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> a.first &lt;= b.first;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n;cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> l,r;cin &gt;&gt; l &gt;&gt; r;<br>        v.<span class="hljs-built_in">push_back</span>(&#123;l,r&#125;);<br>    &#125;<br>    <span class="hljs-built_in">sort</span>(v.<span class="hljs-built_in">begin</span>(),v.<span class="hljs-built_in">end</span>(),cmp);<br>    priority_queue&lt;<span class="hljs-type">int</span>,vector&lt;<span class="hljs-type">int</span>&gt;,greater&lt;<span class="hljs-type">int</span>&gt;&gt; q;<br>    <span class="hljs-type">int</span> sum;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> l = v[i].first,r = v[i].second;<br>        <span class="hljs-keyword">if</span>(q.<span class="hljs-built_in">empty</span>() || q.<span class="hljs-built_in">top</span>() &gt;= l)&#123;<br>            sum++;q.<span class="hljs-built_in">push</span>(r);<br>        &#125;<br>        <span class="hljs-keyword">else</span>&#123;<br>            q.<span class="hljs-built_in">pop</span>();q.<span class="hljs-built_in">push</span>(r);<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; sum;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="区间覆盖"><a href="#区间覆盖" class="headerlink" title="区间覆盖"></a>区间覆盖</h4><p>左端点排序-&gt;寻找最远的右端点</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;queue&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-keyword">typedef</span> pair&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt; PII;<br>vector&lt;PII&gt; v;<br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">cmp</span><span class="hljs-params">(PII a,PII b)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> a.first &lt;= b.first;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> s,t;cin &gt;&gt; s &gt;&gt; t;<br>    <span class="hljs-type">int</span> n;cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> l,r;cin &gt;&gt; l &gt;&gt; r;<br>        v.<span class="hljs-built_in">push_back</span>(&#123;l,r&#125;);<br>    &#125;<br>    <span class="hljs-built_in">sort</span>(v.<span class="hljs-built_in">begin</span>(),v.<span class="hljs-built_in">end</span>(),cmp);<br>    <span class="hljs-type">int</span> ss = <span class="hljs-number">-2e9</span>;<span class="hljs-type">int</span> sum = <span class="hljs-number">0</span>;<span class="hljs-type">bool</span> c = <span class="hljs-literal">false</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> r=<span class="hljs-number">-2e9</span>,j=i;<br>        <span class="hljs-keyword">for</span>(;j &lt; n;j++)&#123;<br>            <span class="hljs-keyword">if</span>(v[j].first &lt;= s)&#123;<br>                r = <span class="hljs-built_in">max</span>(r,v[j].second);<br>            &#125;<br>            <span class="hljs-keyword">else</span> <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-keyword">if</span>(r &lt; s)<span class="hljs-keyword">break</span>;sum++;<br>        <span class="hljs-keyword">if</span>(r &gt;= t)&#123;<br>            c=<span class="hljs-literal">true</span>;<span class="hljs-keyword">break</span>;<br>        &#125;<br>        s=r;i=j<span class="hljs-number">-1</span>;<br>    &#125;<br>    <span class="hljs-keyword">if</span>(c)cout &lt;&lt; sum;<br>    <span class="hljs-keyword">else</span> cout &lt;&lt; <span class="hljs-string">&quot;-1&quot;</span>;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>区间选点问题：<strong>右排序</strong>，优先选择最右点</p><p>区间分组问题：<strong>左排序</strong>，如果最小分组的max_r都大于当前区间的l，则代表区间与所有分组有冲突，新开一组</p><p>区间覆盖问题：<strong>左排序</strong>，如果l&lt;s，则找到其符合条件区间的最右端点并更新成s，注意判断断点和结束点</p><h2 id="绝对值不等式"><a href="#绝对值不等式" class="headerlink" title="绝对值不等式"></a>绝对值不等式</h2><h4 id="货仓选址"><a href="#货仓选址" class="headerlink" title="货仓选址"></a>货仓选址</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">int</span> n;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-type">int</span> f[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)cin &gt;&gt; f[i];<br>    <span class="hljs-built_in">sort</span>(f,f+n);<br>    <span class="hljs-type">int</span> r = <span class="hljs-number">0</span>;<br>    <span class="hljs-type">int</span> m = n/<span class="hljs-number">2</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        r += <span class="hljs-built_in">abs</span>(f[i]-f[m]);<br>    &#125;<br>    cout &lt;&lt; r;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机试复习</title>
      <link href="/post/48f084a8.html"/>
      <url>/post/48f084a8.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="SF"><a href="#SF" class="headerlink" title="SF"></a>SF</h2><p>1、数组越界；2、数组开小了</p><h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><p>1、特判</p><p>2、取l,r为端点外边</p><p>3、循环(l&lt;r)，使用do-while</p><p>4、递归调用，其中使用最后的r作为分割点</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">quicksort</span><span class="hljs-params">(<span class="hljs-type">int</span> num[],<span class="hljs-type">int</span> left,<span class="hljs-type">int</span> right)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(left &gt;= right)<span class="hljs-keyword">return</span>;<br>    <span class="hljs-comment">//1、这里的l和r在外围，t是中间数，left+right-1的话，在分治的时候要用r作为边界点 </span><br>    <span class="hljs-type">int</span> l = left<span class="hljs-number">-1</span>;<span class="hljs-type">int</span> r = right+<span class="hljs-number">1</span>;<span class="hljs-type">int</span> t = num[(left+right<span class="hljs-number">-1</span>)/<span class="hljs-number">2</span>];<br>    <span class="hljs-keyword">while</span>(l &lt; r)&#123;<br>        <span class="hljs-comment">//2、先移动，后判断</span><br>        <span class="hljs-comment">//移动左边</span><br>        <span class="hljs-keyword">do</span> l++; <span class="hljs-keyword">while</span>(num[l] &lt; t);<br>        <span class="hljs-comment">//移动右边</span><br>        <span class="hljs-keyword">do</span> r--; <span class="hljs-keyword">while</span>(num[r] &gt; t);<br>        <span class="hljs-comment">//3、如果没有相遇，则交换l和r</span><br>        <span class="hljs-keyword">if</span>(l &lt; r)&#123;<br>            <span class="hljs-type">int</span> tmp = num[l];<br>            num[l] = num[r];<br>            num[r] = tmp;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//4、分治,r为分界点</span><br>    <span class="hljs-built_in">quicksort</span>(num,left,r);<br>    <span class="hljs-built_in">quicksort</span>(num,r+<span class="hljs-number">1</span>,right);<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><p>1、特判</p><p>2、先对两边sort</p><p>3、再对两边循环，merge</p><p>4、如果两段中有未完成循环的接到最后</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">mergesort</span><span class="hljs-params">(<span class="hljs-type">int</span> num[],<span class="hljs-type">int</span> l,<span class="hljs-type">int</span> r)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(l &gt;= r)<span class="hljs-keyword">return</span>;<br>    <span class="hljs-type">int</span> m = (l+r)/<span class="hljs-number">2</span>;<br>    <span class="hljs-type">int</span> tmp[r-l+<span class="hljs-number">1</span>];<br>    <span class="hljs-comment">//分治法将两端进行排序</span><br>    <span class="hljs-built_in">mergesort</span>(num,l,m);<br>    <span class="hljs-built_in">mergesort</span>(num,m+<span class="hljs-number">1</span>,r);<br>    <span class="hljs-comment">//双指针将两端数组进行归并</span><br>    <span class="hljs-type">int</span> i = l;<span class="hljs-type">int</span> j = m+<span class="hljs-number">1</span>;<span class="hljs-type">int</span> k = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">while</span>(i &lt;= m&amp;&amp;j &lt;= r)&#123;<br>        <span class="hljs-keyword">if</span>(num[i] &lt; num[j])&#123;<br>            tmp[k] = num[i];i++;<br>        &#125;<br>        <span class="hljs-keyword">else</span>&#123;<br>            tmp[k] = num[j];j++;<br>        &#125;<br>        k++;<br>    &#125;<br>    <span class="hljs-comment">//处理一段归并完成但另一段没有的情况</span><br>    <span class="hljs-keyword">while</span>(i &lt;= m)&#123;<br>        tmp[k] = num[i];i++;k++;<br>    &#125;<br>    <span class="hljs-keyword">while</span>(j &lt;= r)&#123;<br>        tmp[k] = num[j];j++;k++;<br>    &#125;<br>    k = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//赋值</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> a = l;a &lt;= r;a++)&#123;<br>        num[a] = tmp[k];k++;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="二分排序"><a href="#二分排序" class="headerlink" title="二分排序"></a>二分排序</h2><p>1、确定判断条件：如小于等于k的最大值</p><p>2、while(l&lt;r){</p><p>​<strong>int m &#x3D; (l+r+1)&#x2F;2; &#x2F;&#x2F;取中点 ,如果有l&#x3D;m则需取上分界</strong></p><p>​&#x2F;&#x2F;小于等于：&lt;&#x3D;</p><p>​if(f[m] &lt;&#x3D; k) l &#x3D; m;&#x2F;&#x2F;判断条件：我们需要找到<strong>最大值</strong>，因此需要尽量往右推，因为等于k的时候满足条件，所以右推的时候是l &#x3D; m，<strong>不去除m对应的点防止丢失</strong></p><p>​else r &#x3D; m+1;</p><p>}</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">divide_right</span><span class="hljs-params">(<span class="hljs-type">int</span> num[],<span class="hljs-type">int</span> q,<span class="hljs-type">int</span> l,<span class="hljs-type">int</span> r)</span></span>&#123;<br>    <span class="hljs-comment">//右端点的特点是在右端点的左边的所有数都满足x &lt;= q;</span><br>    <span class="hljs-keyword">while</span>(l &lt; r)&#123;<br>        <span class="hljs-comment">//在这里需要取中点上界，防止死循环</span><br>        <span class="hljs-type">int</span> m = (l+r+<span class="hljs-number">1</span>)/<span class="hljs-number">2</span>;<br>        <span class="hljs-comment">//如果在右端点左边，说明x在num[m]和r之间，且可能num[m]==x;</span><br>        <span class="hljs-keyword">if</span>(num[m] &lt;= q) l = m;<br>        <span class="hljs-comment">//否则m对应的数一定不在范围里，直接跳到下一个</span><br>        <span class="hljs-keyword">else</span> r = m<span class="hljs-number">-1</span>;<br>    &#125;<br>    <span class="hljs-keyword">if</span>(num[l] != q)<span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>    <span class="hljs-keyword">return</span> l;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="浮点数二分"><a href="#浮点数二分" class="headerlink" title="浮点数二分"></a>浮点数二分</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">double</span> n;cin &gt;&gt; n;<br>    <span class="hljs-type">double</span> l = <span class="hljs-number">-10000</span>;<span class="hljs-type">double</span> r = <span class="hljs-number">10000</span>;<br>    <span class="hljs-keyword">while</span>(r-l &gt; <span class="hljs-number">1e-8</span>)&#123;        <br><span class="hljs-type">double</span> m = (l+r)/<span class="hljs-number">2</span>;<br>        <span class="hljs-keyword">if</span>(m * m * m &gt;= n)r = m;       <br><span class="hljs-keyword">else</span> l = m;<br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%1f&quot;</span>,l);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="高精度计算-记成所有方法都要去除前导零"><a href="#高精度计算-记成所有方法都要去除前导零" class="headerlink" title="高精度计算(记成所有方法都要去除前导零)"></a>高精度计算(记成所有方法都要去除前导零)</h2><h4 id="加法："><a href="#加法：" class="headerlink" title="加法："></a>加法：</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs c++">首先**倒序**将各个数字存到vector里（使用a[i]-<span class="hljs-string">&#x27;0&#x27;</span>）<br>然后对两个序列同时进行处理<br><span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">add</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt; a,vector&lt;<span class="hljs-type">int</span>&gt; b)</span></span>&#123;<br>    vector&lt;<span class="hljs-type">int</span>&gt; r;<br>    <span class="hljs-type">int</span> c = <span class="hljs-number">0</span>;<br>    <span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//对两个序列进行迭代</span><br>    <span class="hljs-keyword">while</span>(i &lt; a.<span class="hljs-built_in">size</span>() || i &lt; b.<span class="hljs-built_in">size</span>())&#123;<br>        <span class="hljs-keyword">if</span>(i &lt; a.<span class="hljs-built_in">size</span>()) c+=a[i];<br>        <span class="hljs-keyword">if</span>(i &lt; b.<span class="hljs-built_in">size</span>()) c+=b[i];<br>        <span class="hljs-comment">//结果位</span><br>        r.<span class="hljs-built_in">push_back</span>(c%<span class="hljs-number">10</span>);<br>        <span class="hljs-comment">//值进位</span><br>        c = c/<span class="hljs-number">10</span>;i++;<br>    &#125;<br>    <span class="hljs-keyword">if</span>(c) r.<span class="hljs-built_in">push_back</span>(c);<br>    <span class="hljs-keyword">return</span> r;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="减法："><a href="#减法：" class="headerlink" title="减法："></a>减法：</h4><p>减法我觉得有一些些恶心</p><p>首先，减法的操作是用大数减小数，所以首先要判断一个两个数的大小</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">cmp</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;a, vector&lt;<span class="hljs-type">int</span>&gt; b)</span></span>&#123;<br>    <span class="hljs-comment">//如果a和b的size不同，返回长的</span><br>    <span class="hljs-keyword">if</span>(a.<span class="hljs-built_in">size</span>() != b.<span class="hljs-built_in">size</span>()) <span class="hljs-keyword">return</span> a.<span class="hljs-built_in">size</span>() &gt;= b.<span class="hljs-built_in">size</span>();<br>    <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-comment">//从大位到小位判断，返回大的</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = a.<span class="hljs-built_in">size</span>()<span class="hljs-number">-1</span>;i &gt;= <span class="hljs-number">0</span>;i--)&#123;<br>            <span class="hljs-keyword">if</span>(a[i] != b[i])<span class="hljs-keyword">return</span> a[i] &gt; b[i];<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>然后进行减法操作，操作如下：</p><p>1、设置一个t存储当前的借位，默认为0；    </p><p>2、对两个序列a,b进行操作，注意借位</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c++">a-t-b,然后(t+<span class="hljs-number">10</span>)%<span class="hljs-number">10</span>判断是否借位<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; a.<span class="hljs-built_in">size</span>();i++)&#123;<br>        <span class="hljs-comment">//a大于b</span><br>        <span class="hljs-comment">//减去借位</span><br>        t = a[i] - t;<br>    <span class="hljs-comment">//如果b还有，则减b</span><br>        <span class="hljs-keyword">if</span>(i &lt; b.<span class="hljs-built_in">size</span>()) t -= b[i];<br>    <span class="hljs-comment">//加上借位</span><br>        r.<span class="hljs-built_in">push_back</span>((t+<span class="hljs-number">10</span>)%<span class="hljs-number">10</span>);<br>    <span class="hljs-comment">//判断是否借位</span><br>        <span class="hljs-keyword">if</span>(t&lt;<span class="hljs-number">0</span>) t = <span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">else</span> t = <span class="hljs-number">0</span>;<br>    &#125;<br></code></pre></td></tr></table></figure><p>3、去除前导0</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">while</span>(r.<span class="hljs-built_in">size</span>()&gt;<span class="hljs-number">1</span> &amp;&amp; r.<span class="hljs-built_in">back</span>()==<span class="hljs-number">0</span>) r.<span class="hljs-built_in">pop_back</span>();<br></code></pre></td></tr></table></figure><h4 id="乘法"><a href="#乘法" class="headerlink" title="乘法"></a>乘法</h4><p>乘法的思路相对简单一些：</p><p>1、首先读入的值是一个vector和一个int，<strong>vector存高精度数</strong></p><p>2、迭代的时候需要多迭代一次，<strong>即从0-size，因为到i&#x3D;&#x3D;size的时候t可能有数字（超过现在的位数）</strong></p><p>​思路：如果i&lt;size，则当前位为(a[i]*b+t)%10，进位为(a[i] * b+t)&#x2F;10；</p><p>3、去除前导零</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//多迭代一次</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt;= a.<span class="hljs-built_in">size</span>();i++)&#123;<br>        <span class="hljs-keyword">if</span>(i &lt; a.<span class="hljs-built_in">size</span>())&#123;<br>            r.<span class="hljs-built_in">push_back</span>((a[i]*b+t) % <span class="hljs-number">10</span>);<br>            t = (a[i]*b+t)/<span class="hljs-number">10</span>;<br>        &#125;<br>        <span class="hljs-keyword">else</span>&#123;<br>            r.<span class="hljs-built_in">push_back</span>(t);<br>            t = <span class="hljs-number">0</span>;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">while</span>(r.<span class="hljs-built_in">size</span>() &gt; <span class="hljs-number">1</span> &amp;&amp; r.<span class="hljs-built_in">back</span>() == <span class="hljs-number">0</span>)r.<span class="hljs-built_in">pop_back</span>();<br>    <span class="hljs-keyword">return</span> r;<br><br><br><span class="hljs-type">int</span> t = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt;= a.<span class="hljs-built_in">size</span>();i++)&#123;<br>    <span class="hljs-keyword">if</span>(i &lt; a.<span class="hljs-built_in">size</span>())&#123;<br>        r.<span class="hljs-built_in">push_back</span>((a[i]*b+t)%<span class="hljs-number">10</span>);<br>        t = (a[i]*b+t)/<span class="hljs-number">10</span>;<br>    &#125;<br>    <span class="hljs-keyword">else</span>&#123;<br>        r.<span class="hljs-built_in">push_back</span>(t);<br>        t = <span class="hljs-number">0</span>;<br>    &#125;<br>&#125;<br><span class="hljs-keyword">while</span>(r.<span class="hljs-built_in">size</span>() &gt; <span class="hljs-number">1</span>&amp;&amp;r.<span class="hljs-built_in">back</span>()==<span class="hljs-number">0</span>)r.<span class="hljs-built_in">pop_back</span>();<br></code></pre></td></tr></table></figure><h4 id="除法"><a href="#除法" class="headerlink" title="除法"></a>除法</h4><p>与前面不同，<strong>除法操作是从高位开始的，</strong>所以读入时需要注意从高位开始压入</p><p><strong>存余数的话可以使用一个地址变量不断更新</strong></p><p>除法的算法如下：</p><p>1、获取当前被除数值的大小 t &#x3D; r*10+a[i];</p><p>2、获取结果与余数</p><p>​<strong>结果 res.push_back(t &#x2F; b);</strong></p><p>​<strong>余数 r &#x3D; t%b;</strong></p><p>使用int &amp;r将余数返回到主函数</p><p><strong>3、倒转结果</strong></p><p>4、去除前导零</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">div</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt; a,<span class="hljs-type">int</span> b,<span class="hljs-type">int</span> &amp;r)</span></span>&#123;<br>    vector&lt;<span class="hljs-type">int</span>&gt; res;<br>    r = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; a.<span class="hljs-built_in">size</span>();i++)&#123;<br>        <span class="hljs-type">int</span> t = r*<span class="hljs-number">10</span>+a[i];<br>        res.<span class="hljs-built_in">push_back</span>(t/b);<br>        r = t%b;<br>    &#125;<br>    <span class="hljs-built_in">reverse</span>(res.<span class="hljs-built_in">begin</span>(),res.<span class="hljs-built_in">end</span>());<br>    <span class="hljs-keyword">while</span>(res.<span class="hljs-built_in">size</span>() &gt; <span class="hljs-number">1</span>&amp;&amp;res.<span class="hljs-built_in">back</span>() == <span class="hljs-number">0</span>)res.<span class="hljs-built_in">pop_back</span>();<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="前缀和"><a href="#前缀和" class="headerlink" title="前缀和"></a>前缀和</h2><h4 id="一维前缀和"><a href="#一维前缀和" class="headerlink" title="一维前缀和"></a>一维前缀和</h4><p>s[i] &#x3D; a[i] + s[i-1]从1开始</p><p><strong>求解[l,r]</strong> ：s[r]-s[l-1]    <strong>&#x2F;&#x2F;注意是l-1</strong></p><h4 id="二维前缀和"><a href="#二维前缀和" class="headerlink" title="二维前缀和"></a>二维前缀和</h4><p><strong>s[i,j] &#x3D; s[i,j-1]+s[i-1,j] - s[i,j]+a[i,j];</strong></p><p>求解[x1,y1] -&gt; [x2,y2] ：s[x2,y2]-s[x1-1,y2]-s[x2,y1-1]+s[x1-1,y1-1]    <strong>&#x2F;&#x2F;注意x1-1,y1-1</strong></p><h2 id="差分"><a href="#差分" class="headerlink" title="差分"></a>差分</h2><h4 id="一维差分：求某一段的和"><a href="#一维差分：求某一段的和" class="headerlink" title="一维差分：求某一段的和"></a>一维差分：求某一段的和</h4><p>a[i] &#x3D; s[i] - s[i-1];</p><p>求解在[l,r]范围+c：a[l] +&#x3D; c;a[r-1]-&#x3D;c;</p><p><strong>理解：对a[l]的操作在l之后的所有元素都产生影响</strong></p><h4 id="二维差分"><a href="#二维差分" class="headerlink" title="二维差分"></a>二维差分</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">inv</span><span class="hljs-params">(<span class="hljs-type">int</span> x1,<span class="hljs-type">int</span> y1,<span class="hljs-type">int</span> x2,<span class="hljs-type">int</span> y2,<span class="hljs-type">int</span> w)</span></span>&#123;<br>    a[x1][y1] += w;<br>    a[x2+<span class="hljs-number">1</span>][y1] -= w;<br>    a[x1][y2+<span class="hljs-number">1</span>] -= w;<br>    a[x2+<span class="hljs-number">1</span>][y2+<span class="hljs-number">1</span>] += w;<br>&#125;<br></code></pre></td></tr></table></figure><p>步骤：</p><p>1、输入</p><p>2、inv(i,j,i,j,s[i,j]);</p><p>3、inv(x1,y1,x2,y2,w);</p><p>4、求和：a[i,j] +&#x3D; a[i,j-1]+a[i-1,j]-a[i-1,j-1];</p><h2 id="双指针算法"><a href="#双指针算法" class="headerlink" title="双指针算法"></a>双指针算法</h2><p>板子：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//1、注意i,j的起始（从小到大/从大到小）</span><br><span class="hljs-comment">//2、check条件</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = a,j = b;i &lt;= n;i++)&#123;<br>    <span class="hljs-comment">//dealing</span><br>    <span class="hljs-keyword">while</span>(c[f[i]] &gt; <span class="hljs-number">1</span>)&#123;<br>        c[f[j]]--;j++;<br>    &#125;<br> <span class="hljs-comment">//dealing</span><br>&#125;<br></code></pre></td></tr></table></figure><h4 id="最长不连续子序列"><a href="#最长不连续子序列" class="headerlink" title="最长不连续子序列"></a>最长不连续子序列</h4><p>快慢指针法：i指向右端点，j指向左端点</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>,j = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>    c[f[i]]++;<br>    <span class="hljs-keyword">while</span>(c[f[i]] &gt; <span class="hljs-number">1</span>)&#123;<br>        c[f[j]]--;j++;<br>    &#125;<br>    r = <span class="hljs-built_in">max</span>(r,i-j+<span class="hljs-number">1</span>);<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="数组元素的目标和"><a href="#数组元素的目标和" class="headerlink" title="数组元素的目标和"></a>数组元素的目标和</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//从大到小</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>,j = m<span class="hljs-number">-1</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-keyword">while</span>(a[i]+b[j] &gt; x) j--;<br>    <span class="hljs-keyword">if</span>(a[i] + b[j] == x)&#123;<br>        cout &lt;&lt; i &lt;&lt; <span class="hljs-string">&quot; &quot;</span> &lt;&lt; j;<span class="hljs-keyword">break</span>;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="判断子序列"><a href="#判断子序列" class="headerlink" title="判断子序列"></a>判断子序列</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>,j = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-keyword">while</span>(a[i] != b[j] &amp;&amp; j&lt;=m) j++;<br>    <span class="hljs-keyword">if</span>(j &lt;= m) &#123;<br>        r++;j++;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="离散化"><a href="#离散化" class="headerlink" title="离散化"></a>离散化</h2><p>理解：<strong>将x,l,r单独压入一个vector，访问时只使用vector中的下标进行访问</strong></p><p>如何找到对应index：<strong>使用二分查找</strong></p><p>1、排序，模拟数轴</p><p>2、去重 all.erase(unique(all.begin(),all.end()),all.end());</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//去重</span><br><span class="hljs-built_in">sort</span>(all.<span class="hljs-built_in">begin</span>(),all.<span class="hljs-built_in">end</span>());<br>all.<span class="hljs-built_in">erase</span>(<span class="hljs-built_in">unique</span>(all.<span class="hljs-built_in">begin</span>(),all.<span class="hljs-built_in">end</span>()),all.<span class="hljs-built_in">end</span>());<br></code></pre></td></tr></table></figure><p>3、对原位置的操作换成对二分之后的位置的操作</p><p>4、前缀</p><h2 id="区间合并"><a href="#区间合并" class="headerlink" title="区间合并"></a>区间合并</h2><p>1、按左边排序</p><p>2、两种判断情况：包含与交叉</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//区间合并算法</span><br><span class="hljs-function">vector&lt;PII&gt; <span class="hljs-title">merge</span><span class="hljs-params">(vector&lt;PII&gt; &amp;a)</span></span>&#123;<br>    vector&lt;PII&gt; res;<br>    <span class="hljs-comment">//首先按照左边的位置进行排序</span><br>    <span class="hljs-built_in">sort</span>(a.<span class="hljs-built_in">begin</span>(),a.<span class="hljs-built_in">end</span>());<br>    <span class="hljs-comment">//保证第一个区间能够被正确处理，r&lt;-1e9即可</span><br>    <span class="hljs-type">int</span> l = <span class="hljs-number">-2e9</span>;<span class="hljs-type">int</span> r = <span class="hljs-number">-2e9</span>;<br>    <span class="hljs-comment">//针对包含、交叉、错位三种情况进行处理</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> item:a)&#123;<br>        <span class="hljs-comment">//错位的情况</span><br>        <span class="hljs-keyword">if</span>(r &lt; item.first)&#123;<br>            <span class="hljs-comment">//如果不是第一次</span><br>            <span class="hljs-keyword">if</span>(l != <span class="hljs-number">-2e9</span>)res.<span class="hljs-built_in">push_back</span>(&#123;l,r&#125;);<br>            l = item.first;<br>            r = item.second;<br>        &#125;<br>        <span class="hljs-comment">//包含与交叉的情况，左边因排序过，l一定是l，r取最大</span><br>        <span class="hljs-keyword">else</span> r = <span class="hljs-built_in">max</span>(r,item.second);<br>    &#125;<br>    <span class="hljs-comment">//处理最后一个区间</span><br>    <span class="hljs-keyword">if</span>(l != <span class="hljs-number">-2e9</span>)res.<span class="hljs-built_in">push_back</span>(&#123;l,r&#125;);<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h3><h4 id="单链表"><a href="#单链表" class="headerlink" title="单链表"></a>单链表</h4><p>e[idx] &#x3D; b;ne[idx] &#x3D; h[a];h[a] &#x3D; idx++;</p><p>删除：ne[k] &#x3D; ne[ne[k]]; </p><h4 id="双链表"><a href="#双链表" class="headerlink" title="双链表"></a>双链表</h4><p>初始化：r[0] &#x3D; 1;r[1] &#x3D; 0;idx&#x3D;2;</p><p>传入左节点地址k和值x</p><p>e[idx] &#x3D; x;r[idx] &#x3D; r[k];l[idx] &#x3D; k;l[r[k]] &#x3D; idx;r[k] &#x3D; idx;</p><p>删除中间结点k</p><p>r[l[k]] &#x3D; r[k];</p><p>l[r[k]] &#x3D; l[k];</p><p>艾海舟，孙立峰，袁春</p><h2 id="模拟线性表"><a href="#模拟线性表" class="headerlink" title="模拟线性表"></a>模拟线性表</h2><h4 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h4><p>数组模拟，top指针，从数组高位操作</p><h4 id="表达式求值"><a href="#表达式求值" class="headerlink" title="表达式求值"></a>表达式求值</h4><p>1、建表</p><p>2、建计算</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">eval</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">char</span> c = operators.<span class="hljs-built_in">top</span>();operators.<span class="hljs-built_in">pop</span>();<br>    <span class="hljs-type">int</span> b = operant.<span class="hljs-built_in">top</span>();operant.<span class="hljs-built_in">pop</span>();<br>    <span class="hljs-type">int</span> a = operant.<span class="hljs-built_in">top</span>();operant.<span class="hljs-built_in">pop</span>();<br>    <span class="hljs-keyword">switch</span>(c)&#123;<br>        <span class="hljs-keyword">case</span> <span class="hljs-string">&#x27;+&#x27;</span> : operant.<span class="hljs-built_in">push</span>(a+b);<span class="hljs-keyword">break</span>;<br>        <span class="hljs-keyword">case</span> <span class="hljs-string">&#x27;-&#x27;</span> : operant.<span class="hljs-built_in">push</span>(a-b);<span class="hljs-keyword">break</span>;<br>        <span class="hljs-keyword">case</span> <span class="hljs-string">&#x27;*&#x27;</span> : operant.<span class="hljs-built_in">push</span>(a*b);<span class="hljs-keyword">break</span>;<br>        <span class="hljs-keyword">case</span> <span class="hljs-string">&#x27;/&#x27;</span> : operant.<span class="hljs-built_in">push</span>(a/b);<span class="hljs-keyword">break</span>;<br>        <span class="hljs-keyword">default</span>:<span class="hljs-keyword">break</span>;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>3、判断条件</p><p>①是数值 ②是左括号 ③是右括号 ④判断是否需要计算</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">if</span>(<span class="hljs-built_in">isdigit</span>(s[i]))&#123;<br>           <span class="hljs-type">int</span> t = <span class="hljs-number">0</span>;<br>           <span class="hljs-keyword">while</span>(<span class="hljs-built_in">isdigit</span>(s[i]))&#123;<br>               t = t*<span class="hljs-number">10</span>+s[i]-<span class="hljs-string">&#x27;0&#x27;</span>;i++;<br>           &#125;<br>           i--;operant.<span class="hljs-built_in">push</span>(t);<br>       &#125;<br>       <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(s[i] == <span class="hljs-string">&#x27;(&#x27;</span>)operators.<span class="hljs-built_in">push</span>(<span class="hljs-string">&#x27;(&#x27;</span>);<br>       <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(s[i] == <span class="hljs-string">&#x27;)&#x27;</span>)&#123;<br>           <span class="hljs-keyword">while</span>(operators.<span class="hljs-built_in">top</span>() != <span class="hljs-string">&#x27;(&#x27;</span>)&#123;<br>               <span class="hljs-built_in">eval</span>();<br>           &#125;<br>           operators.<span class="hljs-built_in">pop</span>();<br>       &#125;<br>       <span class="hljs-keyword">else</span>&#123;<br>           <span class="hljs-comment">//注意这里：要先判断size()条件，每次都是这样，否则可能出现数组越界的问题</span><br>           <span class="hljs-keyword">while</span>(operators.<span class="hljs-built_in">size</span>() &amp;&amp; p[operators.<span class="hljs-built_in">top</span>()] &gt;= p[s[i]]) <span class="hljs-built_in">eval</span>();<br>           operators.<span class="hljs-built_in">push</span>(s[i]);<br>       &#125;<br></code></pre></td></tr></table></figure><h4 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h4><p>数组模拟，双指针，从数组高位操作push，低位操作pop</p><h4 id="单调栈与单调队列"><a href="#单调栈与单调队列" class="headerlink" title="单调栈与单调队列"></a>单调栈与单调队列</h4><p>单调栈：判断栈顶是否大于当前值即可</p><h6 id="给定一个长度为-N-的整数数列，输出每个数左边第一个比它小的数，如果不存在则输出-−1。"><a href="#给定一个长度为-N-的整数数列，输出每个数左边第一个比它小的数，如果不存在则输出-−1。" class="headerlink" title="给定一个长度为 N 的整数数列，输出每个数左边第一个比它小的数，如果不存在则输出 −1。"></a>给定一个长度为 N 的整数数列，输出每个数左边第一个比它小的数，如果不存在则输出 −1。</h6><h2 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a><em><u>滑动窗口</u></em></h2><p>注意！！！这道题用数组的<strong>单调队列</strong>做</p><p>对于<strong>每次操作的算法</strong>：</p><p>1、判断fr对应的值是否已经出栈</p><p>2、判断栈尾的值是否大于当前值，如果大于则栈尾的值一定不是最小值，出栈</p><p>3、将当前值压栈</p><p>4、输出栈顶值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs c++">  <span class="hljs-comment">//初始fr == 0,rr == -1,让第一个数能直接入队</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-comment">//首先判断最左边的点是否在窗口里</span><br>        <span class="hljs-keyword">if</span>(fr &lt;= rr &amp;&amp; q[fr] &lt; i-m+<span class="hljs-number">1</span>) fr++;<br>        <span class="hljs-comment">//其次判断是否大于栈尾，如果小于则压入，否则栈尾的元素一定不会是最小值，</span><br>        <span class="hljs-keyword">while</span>(fr &lt;= rr &amp;&amp; f[q[rr]] &gt;= f[i]) &#123;<br>            rr--;<br>        &#125;<br>        q[++rr] = i;<br>        <span class="hljs-keyword">if</span>(i &gt;= m<span class="hljs-number">-1</span>) cout &lt;&lt; f[q[fr]] &lt;&lt; <span class="hljs-string">&quot; &quot;</span> ;<br>    &#125;<br><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>    <span class="hljs-keyword">if</span>(fr&lt;=rr &amp;&amp; q[fr]&lt;i-m+<span class="hljs-number">1</span>)fr++;<br>    <span class="hljs-keyword">while</span>(fr&lt;=rr &amp;&amp; f[q[rr]]&gt;=f[i]) rr--;<br>    q[++rr] = i;<br>    <span class="hljs-keyword">if</span>(i &gt; m)cout &lt;&lt; f[q[fr]] &lt;&lt; <span class="hljs-string">&quot; &quot;</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="KMP"><a href="#KMP" class="headerlink" title="KMP"></a><em><u>KMP</u></em></h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c++">cin &gt;&gt; m &gt;&gt; p+<span class="hljs-number">1</span> &gt;&gt; n &gt;&gt; s+<span class="hljs-number">1</span>;<br><span class="hljs-comment">//这里对p串的匹配从2开始是因为i==1时，i == j+1在开始一定是成立的，但是我们想要的是非平凡子串，ne[1] == 0，所以直接从2开始</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">2</span>,j = <span class="hljs-number">0</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-comment">//当未匹配成功时则使用next向后跳跃</span><br>    <span class="hljs-keyword">while</span>(j &amp;&amp; p[i] != p[j+<span class="hljs-number">1</span>]) j = ne[j];<br>    <span class="hljs-comment">//判断是哪个条件跳出的，如果是i == j+1则匹配成功，j++指到下一个需要处理的位置；如果是j到头则不处理</span><br>    <span class="hljs-keyword">if</span>(p[i] == p[j+<span class="hljs-number">1</span>])j++;<br>    <span class="hljs-comment">//对每个i的位置进行赋值。</span><br>    ne[i] = j;<br>&#125;<br><span class="hljs-comment">//s串与p串的匹配，过程和next数组求解基本一致，不过next数组是p与p的匹配。</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>,j = <span class="hljs-number">0</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-keyword">while</span>(j &amp;&amp; s[i] != p[j+<span class="hljs-number">1</span>]) j = ne[j];<br>    <span class="hljs-keyword">if</span>(s[i] == p[j+<span class="hljs-number">1</span>])j++;<br>    <span class="hljs-comment">//判断是否整串完成匹配</span><br>    <span class="hljs-keyword">if</span>(j == m)&#123;<br>        cout &lt;&lt; i-m &lt;&lt; <span class="hljs-string">&quot; &quot;</span>;<br>        j = ne[j];<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Trie"><a href="#Trie" class="headerlink" title="Trie"></a>Trie</h2><p>&#x2F;&#x2F;原理：使用一棵树来存储字符串</p><p>使用son[N,26]存储树，cnt[N]数组存每个位置的计数，对应不同的字符串</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> cnt[N],son[N][<span class="hljs-number">26</span>],idx;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">insert</span><span class="hljs-params">(<span class="hljs-type">char</span> s[])</span></span>&#123;<br>    <span class="hljs-type">int</span> p = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;s[i];i++)&#123;<br>        <span class="hljs-comment">//转义</span><br>        <span class="hljs-type">int</span> t = s[i]-<span class="hljs-string">&#x27;a&#x27;</span>;<br>        <span class="hljs-comment">//如果没有创建则创建</span><br>        <span class="hljs-keyword">if</span>(!son[p][t]) son[p][t]=++idx;<br>        <span class="hljs-comment">//下转</span><br>        p = son[p][t];<br>    &#125;<br>    <span class="hljs-comment">//计数增加</span><br>    cnt[p]++;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">query</span><span class="hljs-params">(<span class="hljs-type">char</span> s[])</span></span>&#123;<br>    <span class="hljs-type">int</span> p = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;s[i];i++)&#123;<br>        <span class="hljs-type">int</span> t = s[i] - <span class="hljs-string">&#x27;a&#x27;</span>;<br>        <span class="hljs-keyword">if</span>(!son[p][t]) <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>        p = son[p][t];<br>    &#125;<br>    <span class="hljs-keyword">return</span> cnt[p];<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">query</span><span class="hljs-params">(<span class="hljs-type">char</span> s[])</span></span>&#123;<br>    <span class="hljs-type">int</span> p = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;s[i];i++)&#123;<br>        <span class="hljs-type">int</span> t = s[i]-<span class="hljs-string">&#x27;a&#x27;</span>;<br>        <span class="hljs-keyword">if</span>(!son[p][t])<span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>        p = son[p][t];<br>    &#125;<br>    <span class="hljs-keyword">return</span> cnt[p];<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="最大异或对"><a href="#最大异或对" class="headerlink" title="最大异或对"></a>最大异或对</h4><p>原理：下溯的时候溯与当前位相反的数（保证异或最大）</p><p>注意：建树的时候从高到低</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">query</span><span class="hljs-params">(<span class="hljs-type">int</span> s)</span></span>&#123;<br>    <span class="hljs-type">int</span> r = <span class="hljs-number">0</span>,p = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">30</span>;i &gt;= <span class="hljs-number">0</span>;i--)&#123;<br>        <span class="hljs-type">int</span> t = s &gt;&gt; i &amp; <span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">if</span>(son[p][!t])&#123;<br>            r += <span class="hljs-number">1</span> &lt;&lt; i;<br>            p = son[p][!t];<br>        &#125;<br>        <span class="hljs-keyword">else</span> p = son[p][t];<br>    &#125;<br>    <span class="hljs-keyword">return</span> r;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="并查集"><a href="#并查集" class="headerlink" title="并查集***"></a>并查集***</h2><p>并查集可以用来进行集合处理</p><h4 id="合并集合"><a href="#合并集合" class="headerlink" title="合并集合"></a>合并集合</h4><p>1、首先将每个数做成一个集合</p><p>2、合并的时候将p[a] &#x3D; b；</p><p><strong>三种操作：注意这里找父节点用的是find函数</strong><br>1、集合合并：<strong>p[find[a]] &#x3D; find[b]</strong><br>2、判断是否属于同一个集合：<strong>find[a] &#x3D;&#x3D; find[b]</strong><br>3、找到父节点：<strong>p[a] &#x3D; find(p[a])</strong></p><p><strong>并查集操作：</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">find</span><span class="hljs-params">(<span class="hljs-type">int</span> x)</span></span>&#123;<br>    <span class="hljs-comment">//如果不是根节点，则一直上溯，上溯到的一定是根节点，然后在这条路上的所有点的父节点都变成根节点（路径压缩）</span><br>    <span class="hljs-keyword">if</span>(p[x] != x) p[x] = <span class="hljs-built_in">find</span>(p[x]);<br>    <span class="hljs-keyword">return</span> p[x];<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="连通块中点的数量"><a href="#连通块中点的数量" class="headerlink" title="连通块中点的数量"></a>连通块中点的数量</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">if</span>(s[<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;C&#x27;</span>)&#123;<br>          cin &gt;&gt; a &gt;&gt; b;<br>          <span class="hljs-comment">//注意这里：需要判定是否相等，否则会重复加</span><br>          <span class="hljs-keyword">if</span>(<span class="hljs-built_in">find</span>(a) != <span class="hljs-built_in">find</span>(b))&#123;<br>              cnt[<span class="hljs-built_in">find</span>(b)] += cnt[<span class="hljs-built_in">find</span>(a)];<br>              p[<span class="hljs-built_in">find</span>(a)] = <span class="hljs-built_in">find</span>(b);               <br>          &#125;<br>      &#125;<br>      <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(s[<span class="hljs-number">1</span>] == <span class="hljs-string">&#x27;1&#x27;</span>)&#123;<br>          cin &gt;&gt; a &gt;&gt; b;<br>          <span class="hljs-keyword">if</span>(<span class="hljs-built_in">find</span>(a)==<span class="hljs-built_in">find</span>(b)) cout &lt;&lt; <span class="hljs-string">&quot;Yes&quot;</span> &lt;&lt; endl;<br>          <span class="hljs-keyword">else</span> cout &lt;&lt; <span class="hljs-string">&quot;No&quot;</span> &lt;&lt; endl;<br>      &#125;<br>      <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(s[<span class="hljs-number">1</span>] == <span class="hljs-string">&#x27;2&#x27;</span>)&#123;<br>          cin &gt;&gt; a;<br>          cout &lt;&lt; cnt[<span class="hljs-built_in">find</span>(a)] &lt;&lt; endl;<br>      &#125;<br></code></pre></td></tr></table></figure><h2 id="模拟堆"><a href="#模拟堆" class="headerlink" title="模拟堆"></a>模拟堆</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs c++">    <span class="hljs-keyword">if</span>(!<span class="hljs-built_in">strcmp</span>(op,<span class="hljs-string">&quot;I&quot;</span>))&#123;<br>        <span class="hljs-type">int</span> t;<span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%d&quot;</span>, &amp;t);<br>        m++;s++;<span class="hljs-comment">//这里的m出错了</span><br>        <span class="hljs-comment">//尾部插入</span><br>        h[s] = t;<br>        hp[s] = m;ph[m] = s;<br>        <span class="hljs-comment">//上处理，重新刷新堆</span><br>        <span class="hljs-built_in">up</span>(s);<br>    &#125;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(!<span class="hljs-built_in">strcmp</span>(op,<span class="hljs-string">&quot;PM&quot;</span>))&#123;<br>        cout &lt;&lt; h[<span class="hljs-number">1</span>] &lt;&lt; endl;<br>    &#125;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(!<span class="hljs-built_in">strcmp</span>(op,<span class="hljs-string">&quot;DM&quot;</span>))&#123;<br>        <span class="hljs-built_in">heap_swap</span>(<span class="hljs-number">1</span>,s);<br>        s--;<span class="hljs-built_in">down</span>(<span class="hljs-number">1</span>);<br>    &#125;<br><br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">up</span><span class="hljs-params">(<span class="hljs-type">int</span> x)</span></span>&#123;<br>    <span class="hljs-keyword">while</span>(x/<span class="hljs-number">2</span> &gt; <span class="hljs-number">0</span> &amp;&amp; h[x/<span class="hljs-number">2</span>] &gt; h[x]) <span class="hljs-built_in">swap</span>(h[x/<span class="hljs-number">2</span>],h[x]);<br>    x /= <span class="hljs-number">2</span>;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">down</span><span class="hljs-params">(<span class="hljs-type">int</span> x)</span></span>&#123;<br>    <span class="hljs-type">int</span> t = x;<br>    <span class="hljs-keyword">if</span>(<span class="hljs-number">2</span>*x&lt;=s &amp;&amp; h[<span class="hljs-number">2</span>*x] &lt; h[t]) t = <span class="hljs-number">2</span>*x;<br>    <span class="hljs-keyword">if</span>(<span class="hljs-number">2</span>*x+<span class="hljs-number">1</span> &lt;=s&amp;&amp; h[<span class="hljs-number">2</span>*x+<span class="hljs-number">1</span>] &lt; h[t]) t = <span class="hljs-number">2</span>*x+<span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">if</span>(t != x)&#123;<br>        <span class="hljs-comment">//这里使用的是下标，不是h！！！！！！！！！！！！</span><br>        <span class="hljs-built_in">heap_swap</span>(t,x);<br>        <span class="hljs-built_in">down</span>(t);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="模拟哈希"><a href="#模拟哈希" class="headerlink" title="模拟哈希"></a>模拟哈希</h2><p><strong>拉链法</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//求质数</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100003</span>;<br><span class="hljs-comment">//开槽</span><br><span class="hljs-type">int</span> h[N],e[N],ne[N],idx;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">insert</span><span class="hljs-params">(<span class="hljs-type">int</span> x)</span></span>&#123;<br><span class="hljs-comment">//转成正数</span><br><span class="hljs-type">int</span> k = (x%N+N)%N;<br>e[idx] = x;<br>ne[idx] = h[k];<br>h[k] = idx++;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">find</span><span class="hljs-params">(<span class="hljs-type">int</span> x)</span></span>&#123;<br><span class="hljs-type">int</span> k = (x%N+N)%N;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = h[k];i != <span class="hljs-number">-1</span>;i = ne[i])&#123;<br><span class="hljs-keyword">if</span>(e[i] == x) <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>注意要把槽设为<span class="hljs-number">-1</span><br>    <span class="hljs-type">int</span> k = (x%N+N)%N;<br><span class="hljs-built_in">memset</span>(h,<span class="hljs-number">-1</span>,<span class="hljs-keyword">sizeof</span> h);<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="字符串哈希"><a href="#字符串哈希" class="headerlink" title="字符串哈希"></a>字符串哈希</h4><p><img src="/post/48f084a8/image-20210822154642276.png" alt="image-20210822154642276"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//P = 131或13331</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> P = <span class="hljs-number">131</span>;<br><br><span class="hljs-keyword">typedef</span> <span class="hljs-type">unsigned</span> <span class="hljs-type">long</span> <span class="hljs-type">long</span> ULL; <br><br><span class="hljs-comment">//h[i]:存1~i位置对应的哈希。p[i]:存第i位对应的p值。</span><br><span class="hljs-type">int</span> h[N],p[N];<br><br><span class="hljs-function">ULL <span class="hljs-title">get</span><span class="hljs-params">(<span class="hljs-type">int</span> l,<span class="hljs-type">int</span> r)</span></span>&#123;<br>    <span class="hljs-comment">//h[r]:1~r;h[l-1]:1~l-1;</span><br>    <span class="hljs-comment">//p[r-l+1]:对齐高位（ABCDE-ABC00 = DE）</span><br>    <span class="hljs-keyword">return</span> h[r] - h[l<span class="hljs-number">-1</span>] * p[r-l+<span class="hljs-number">1</span>];<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br><span class="hljs-comment">//从h[1]开始存储数字，代表前1个字符的哈希值，h[0]=0;</span><br><span class="hljs-comment">//p[0]=1代表开始的时候没有指数增长。</span><br>    h[<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>;p[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span>;<br>    <br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++) &#123;<br>        cin &gt;&gt; str[i];<br>        <span class="hljs-comment">//字符串的高位同样也是哈希的高位</span><br>        <span class="hljs-comment">//h = h*p+str[i]</span><br>        h[i] = h[i<span class="hljs-number">-1</span>] * P + str[i];<br>        <span class="hljs-comment">//p指数增长</span><br>        p[i] = p[i<span class="hljs-number">-1</span>] * P;<br>    &#125;<br>    <br>    <span class="hljs-keyword">while</span>(m--)&#123;<br>        <span class="hljs-type">int</span> l1,r1,l2,r2;<br>        cin &gt;&gt; l1 &gt;&gt; r1 &gt;&gt; l2 &gt;&gt; r2;<br>        <span class="hljs-keyword">if</span>(<span class="hljs-built_in">get</span>(l1,r1) == <span class="hljs-built_in">get</span>(l2,r2)) cout &lt;&lt; <span class="hljs-string">&quot;Yes&quot;</span> &lt;&lt; endl;<br>        <span class="hljs-keyword">else</span> cout &lt;&lt; <span class="hljs-string">&quot;No&quot;</span> &lt;&lt; endl;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h2><h3 id="排列数字"><a href="#排列数字" class="headerlink" title="排列数字"></a>排列数字</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//s[N]:存储是否使用，p[N]:按序存储DFS结果</span><br><span class="hljs-type">int</span> n,s[N],p[N];<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">DFS</span><span class="hljs-params">(<span class="hljs-type">int</span> u)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(u &gt; n)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>            cout &lt;&lt; p[i] &lt;&lt; <span class="hljs-string">&quot; &quot;</span>;<br>        &#125;<br>        cout &lt;&lt; endl;<br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">if</span>(s[i] == <span class="hljs-number">0</span>)&#123;<br>            s[i] = u;<br>            p[u] = i;<br>            <span class="hljs-built_in">DFS</span>(u+<span class="hljs-number">1</span>);<br>            <span class="hljs-comment">//恢复现场</span><br>            p[u] = <span class="hljs-number">0</span>;<br>            s[i] = <span class="hljs-number">0</span>;<br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-built_in">DFS</span>(<span class="hljs-number">1</span>);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="N皇后"><a href="#N皇后" class="headerlink" title="N皇后"></a>N皇后</h3><p>只记迭代每个位置的方法</p><p><img src="/post/48f084a8/image-20210822213659047.png" alt="image-20210822213659047"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">DFS</span><span class="hljs-params">(<span class="hljs-type">int</span> r,<span class="hljs-type">int</span> c,<span class="hljs-type">int</span> t)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(c &gt; n)&#123;<br>        c = <span class="hljs-number">1</span>;<br>        r++;<br>    &#125;<br>    <span class="hljs-keyword">if</span>(r &gt; n)&#123;<br>        <span class="hljs-keyword">if</span>(t == n)&#123;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>                <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= n;j++)&#123;<br>                    cout &lt;&lt; s[i][j];<br>                &#125;<br>                cout &lt;&lt; endl;<br>            &#125;<br>            cout&lt;&lt;endl;<br>        &#125;<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-comment">//注意这里先走不放棋子的分支</span><br>    <span class="hljs-comment">//注意！！！！！！！！！！</span><br>    <span class="hljs-built_in">DFS</span>(r,c+<span class="hljs-number">1</span>,t);<br>    <span class="hljs-comment">//判断横竖，对角线次对角线有没有被占</span><br>    <span class="hljs-keyword">if</span>(!row[r] &amp;&amp; !col[c] &amp;&amp; !dg[r+c] &amp;&amp; !udg[c-r+n])&#123;<br>        row[r] = col[c] = dg[r+c] = udg[c-r+n] = <span class="hljs-number">1</span>;<br>        s[r][c] = <span class="hljs-string">&#x27;Q&#x27;</span>;<br>        <span class="hljs-built_in">DFS</span>(r,c+<span class="hljs-number">1</span>,t+<span class="hljs-number">1</span>);<br>        row[r] = col[c] = dg[r+c] = udg[c-r+n] = <span class="hljs-number">0</span>;<br>        s[r][c] = <span class="hljs-string">&#x27;.&#x27;</span>;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="BFS"><a href="#BFS" class="headerlink" title="BFS"></a>BFS</h2><h3 id="走迷宫"><a href="#走迷宫" class="headerlink" title="走迷宫"></a>走迷宫</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;queue&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;unordered_map&gt;</span></span><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">110</span>,M = <span class="hljs-number">110</span>;<br><span class="hljs-keyword">typedef</span> pair&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt; MM;<br><br><span class="hljs-type">int</span> n,m;<br><span class="hljs-type">int</span> h[N][M];<br><span class="hljs-comment">//d表示与起点的距离</span><br><span class="hljs-type">int</span> d[N][M];<br><span class="hljs-comment">//偏移数组</span><br><span class="hljs-type">int</span> dx[<span class="hljs-number">4</span>]=&#123;<span class="hljs-number">0</span>,<span class="hljs-number">-1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>&#125;;<br><span class="hljs-type">int</span> dy[<span class="hljs-number">4</span>]=&#123;<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">-1</span>,<span class="hljs-number">0</span>&#125;;<br>queue&lt;MM&gt; q;<br><span class="hljs-comment">//在一个队列里完成层次遍历，直到队列再次为空，队列中先访问的点的距离一定是相对小的</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">BFS</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-comment">//首先初始化d，所有点都没有走到</span><br>    <span class="hljs-built_in">memset</span>(d,<span class="hljs-number">-1</span>,<span class="hljs-keyword">sizeof</span> d);<br>    <span class="hljs-comment">//从(1,1)开始</span><br>    d[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>]=<span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">while</span>(!q.<span class="hljs-built_in">empty</span>())&#123;<br>        <span class="hljs-comment">//首先判断是否到终点</span><br>        <span class="hljs-type">int</span> x = q.<span class="hljs-built_in">front</span>().first;<br>        <span class="hljs-type">int</span> y = q.<span class="hljs-built_in">front</span>().second;<br>        q.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//如果到终点则跳出</span><br>        <span class="hljs-keyword">if</span>(x == n &amp;&amp; y == m)&#123;<br>            cout &lt;&lt; d[n][m];<br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//否则按照四周进行移动</span><br>        <span class="hljs-keyword">else</span>&#123;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; <span class="hljs-number">4</span>;i++)&#123;<br>                <span class="hljs-type">int</span> px = x+dx[i];<br>                <span class="hljs-type">int</span> py = y+dy[i];<br>                <span class="hljs-comment">//判断是否是有效位置</span><br>                <span class="hljs-keyword">if</span>(px &gt; <span class="hljs-number">0</span> &amp;&amp; px &lt;= n &amp;&amp; py &gt; <span class="hljs-number">0</span> &amp;&amp; py &lt;= m &amp;&amp; h[px][py] != <span class="hljs-number">1</span> &amp;&amp; d[px][py]== <span class="hljs-number">-1</span>)&#123;<br>                    d[px][py] = d[x][y]+<span class="hljs-number">1</span>;<br>                    q.<span class="hljs-built_in">push</span>(&#123;px,py&#125;);<br>                &#125;<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n &gt;&gt; m;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= m;j++)&#123;<br>            cin &gt;&gt; h[i][j];<br>        &#125;<br>    &#125;<br>    q.<span class="hljs-built_in">push</span>(&#123;<span class="hljs-number">1</span>,<span class="hljs-number">1</span>&#125;);<span class="hljs-built_in">BFS</span>();<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="八数码"><a href="#八数码" class="headerlink" title="八数码"></a>八数码</h3><p>特征：矩阵移动性</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">bfs</span><span class="hljs-params">(string s)</span></span>&#123;<br>    string e = <span class="hljs-string">&quot;12345678x&quot;</span>;<br>    <span class="hljs-comment">//入队</span><br>    q.<span class="hljs-built_in">push</span>(s);<br>    <span class="hljs-comment">//从初始串的位置开始</span><br>    d[s] = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//偏移向量</span><br>    <span class="hljs-type">int</span> dx[<span class="hljs-number">4</span>] = &#123;<span class="hljs-number">1</span>,<span class="hljs-number">-1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>&#125;;<br>    <span class="hljs-type">int</span> dy[<span class="hljs-number">4</span>] = &#123;<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">-1</span>&#125;;<br>    <span class="hljs-comment">//推理</span><br>    <span class="hljs-keyword">while</span>(!q.<span class="hljs-built_in">empty</span>())&#123;、<br>        <span class="hljs-comment">//出队</span><br>        string t = q.<span class="hljs-built_in">front</span>();<br>        q.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-comment">//判断当前位置是否已经是结果</span><br>        <span class="hljs-type">int</span> dt = d[t];<br>        <span class="hljs-keyword">if</span>(t == e) &#123;<br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>        <span class="hljs-comment">//找到X的位置</span><br>        <span class="hljs-type">int</span> xx = t.<span class="hljs-built_in">find</span>(<span class="hljs-string">&#x27;x&#x27;</span>);<br>        <span class="hljs-comment">//转化为二维坐标</span><br>        <span class="hljs-type">int</span> tx = xx / <span class="hljs-number">3</span>;<br>        <span class="hljs-type">int</span> ty = xx % <span class="hljs-number">3</span>;<br>        <span class="hljs-comment">//推理到四周的位置</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; <span class="hljs-number">4</span>;i++)&#123;<br>            <span class="hljs-type">int</span> px = tx + dx[i];<br>            <span class="hljs-type">int</span> py = ty + dy[i];<br>            <span class="hljs-comment">//如果是合法位置</span><br>            <span class="hljs-keyword">if</span>(px &gt;= <span class="hljs-number">0</span> &amp;&amp; px &lt; <span class="hljs-number">3</span> &amp;&amp; py &gt;= <span class="hljs-number">0</span> &amp;&amp; py &lt; <span class="hljs-number">3</span>)&#123;<br>                <span class="hljs-comment">//交换当前位置</span><br>                <span class="hljs-type">int</span> dp = d[t]+<span class="hljs-number">1</span>;<br>                <span class="hljs-built_in">swap</span>(t[px*<span class="hljs-number">3</span>+py],t[xx]);<br>                <span class="hljs-comment">//如果这个情况没有被走过</span><br>                <span class="hljs-keyword">if</span>(!d.<span class="hljs-built_in">count</span>(t))&#123;<br>                    d[t] = dp;<br>                    q.<span class="hljs-built_in">push</span>(t);<br>                &#125;<br>                <span class="hljs-comment">//恢复现场</span><br>                <span class="hljs-built_in">swap</span>(t[px*<span class="hljs-number">3</span>+py],t[xx]);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//判断最终情况是否有记录</span><br>    <span class="hljs-keyword">if</span>(d.<span class="hljs-built_in">count</span>(e)) cout &lt;&lt; d[e];<br>    <span class="hljs-keyword">else</span> cout &lt;&lt; <span class="hljs-number">-1</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="树与图的深度优先遍历"><a href="#树与图的深度优先遍历" class="headerlink" title="树与图的深度优先遍历"></a>树与图的深度优先遍历</h2><h3 id="树的重心"><a href="#树的重心" class="headerlink" title="树的重心"></a>树的重心</h3><p><img src="/post/48f084a8/image-20210823235144263.png" alt="image-20210823235144263"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//返回以u为根的子树中节点的个数，包括u节点</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">dfs</span><span class="hljs-params">(<span class="hljs-type">int</span> u)</span> </span>&#123;<br>    <span class="hljs-type">int</span> res = <span class="hljs-number">0</span>; <span class="hljs-comment">//存储删掉某个节点之后，最大的连通子图节点数</span><br>    st[u] = <span class="hljs-literal">true</span>; <span class="hljs-comment">//标记访问过u节点</span><br>    <span class="hljs-type">int</span> sum = <span class="hljs-number">1</span>; <span class="hljs-comment">//存储 以u为根的树的节点数, 包括u，如图中的4号节点</span><br>    <span class="hljs-comment">//访问u的每个子节点</span><br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = h[u]; i != <span class="hljs-number">-1</span>; i = ne[i]) &#123;<br>        <span class="hljs-type">int</span> j = e[i];<br>        <span class="hljs-comment">//因为每个节点的编号都是不一样的，所以 用编号为下标来标记是否被访问过</span><br>        <span class="hljs-keyword">if</span> (!st[j]) &#123;<br>            <span class="hljs-type">int</span> s = <span class="hljs-built_in">dfs</span>(j);  <span class="hljs-comment">// u节点的单棵子树节点数如图中的size值</span><br>            res = <span class="hljs-built_in">max</span>(res, s); <span class="hljs-comment">// 记录最大联通子图的节点数</span><br>            sum += s; <span class="hljs-comment">//以j为根的树的节点数</span><br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">//n-sum 如图中的n-size值，不包括根节点4；</span><br>    res = <span class="hljs-built_in">max</span>(res, n - sum); <span class="hljs-comment">// 选择u节点为重心，最大的连通子图节点数</span><br>    ans = <span class="hljs-built_in">min</span>(res, ans); <span class="hljs-comment">//遍历过的假设重心中，最小的最大联通子图的节点数</span><br>    <span class="hljs-keyword">return</span> sum;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="树与图的广度优先遍历"><a href="#树与图的广度优先遍历" class="headerlink" title="树与图的广度优先遍历"></a>树与图的广度优先遍历</h2><h3 id="图中点的层次"><a href="#图中点的层次" class="headerlink" title="图中点的层次"></a>图中点的层次</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">bfs</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">memset</span>(d,<span class="hljs-number">-1</span>,<span class="hljs-keyword">sizeof</span> d);<br>    d[<span class="hljs-number">1</span>] = <span class="hljs-number">0</span>;<br>    q.<span class="hljs-built_in">push</span>(<span class="hljs-number">1</span>);<br>    <span class="hljs-keyword">while</span>(q.<span class="hljs-built_in">size</span>())&#123;<br>        <span class="hljs-type">int</span> t = q.<span class="hljs-built_in">front</span>();<br>        q.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = h[t];i != <span class="hljs-number">-1</span>;i = ne[i])&#123;<br>            <span class="hljs-keyword">if</span>(d[e[i]] == <span class="hljs-number">-1</span>)&#123;<br>                d[e[i]]=d[t]+<span class="hljs-number">1</span>;<br>                q.<span class="hljs-built_in">push</span>(e[i]);<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">if</span>(t == n)&#123;<br>            cout &lt;&lt; d[t];<span class="hljs-keyword">return</span>;<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; <span class="hljs-number">-1</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="图的排序与最短路"><a href="#图的排序与最短路" class="headerlink" title="图的排序与最短路"></a>图的排序与最短路</h2><h3 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h3><p><img src="/post/48f084a8/image-20210824144744931.png" alt="image-20210824144744931"></p><p>注意：图中可能存在重边和自环</p><p>策略：宽搜+入度计算</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//先建图，记录入度</span><br><span class="hljs-keyword">while</span>(m--)&#123;<br>    <span class="hljs-type">int</span> a,b;<br>    cin &gt;&gt; a &gt;&gt; b;<br>    <span class="hljs-built_in">add</span>(a,b);<br>    <span class="hljs-comment">//用一个d数组记入度</span><br>    d[b]++;<br>&#125;<br><span class="hljs-comment">//然后从入度为0的点开始进行处理</span><br><span class="hljs-comment">//宽搜</span><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">topsort</span><span class="hljs-params">()</span></span>&#123;<br>   <span class="hljs-type">int</span> count = <span class="hljs-number">0</span>;<br>   <span class="hljs-comment">//首先找到所有入度为0的点</span><br>   <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>       <span class="hljs-keyword">if</span>(d[i] == <span class="hljs-number">0</span>)q.<span class="hljs-built_in">push</span>(i);<br>   &#125;<br>   <span class="hljs-comment">//宽搜</span><br>   <span class="hljs-keyword">while</span>(q.<span class="hljs-built_in">size</span>())&#123;<br>       <span class="hljs-type">int</span> tmp = q.<span class="hljs-built_in">front</span>();<br>       q.<span class="hljs-built_in">pop</span>();count++;<br>       res.<span class="hljs-built_in">push_back</span>(tmp);<br>       <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = h[tmp];i != <span class="hljs-number">-1</span>;i = ne[i])&#123;<br>           <span class="hljs-type">int</span> link = e[i];<br>           d[link]--;<br>           <span class="hljs-keyword">if</span>(d[link] == <span class="hljs-number">0</span>)q.<span class="hljs-built_in">push</span>(link);<br>       &#125;<br>   &#125;<br>   <span class="hljs-comment">//如果所有点都走完了，这个图的拓扑序列有效</span><br>   <span class="hljs-keyword">if</span>(count == n)<span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>   <span class="hljs-keyword">else</span> <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p><img src="/post/48f084a8/image-20210825001507160.png" alt="image-20210825001507160"></p><h3 id="Dijkstra算法：非负权，有自环重边"><a href="#Dijkstra算法：非负权，有自环重边" class="headerlink" title="Dijkstra算法：非负权，有自环重边"></a>Dijkstra算法：非负权，有自环重边</h3><p>Dijkstra 的整体思路比较清晰<br>即<strong>进行n（n为n的个数）次迭代</strong>去确定<strong>每个点到起点的最小值</strong>，最后输出的终点的即为我们要找的最短路的距离</p><p>步骤</p><p>迭代n次，每次迭代都找到<strong>一个符合条件的点加入</strong>，确定一个点的最短路的值，符合的条件为：</p><p>1、没有被处理过（即没有加入当前的连通分量）</p><p>2、距离最短</p><p>找到j点之后将use数组置1，然后对于所有和j点相连的点更新最短路。</p><p>初始化memset时无限大是0x3f,memset是按字节填充 int 4 个字节，判断的时候所以是0x3f3f3f3f</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//稠密图需要使用邻接矩阵存储图</span><br><span class="hljs-type">int</span> g[N][N];<br><span class="hljs-comment">//记录1点与其他点的距离</span><br><span class="hljs-type">int</span> dist[N];<br><span class="hljs-comment">//判断是否使用过</span><br><span class="hljs-type">int</span> use[N];<br><span class="hljs-type">int</span> n,m;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">dijkstr</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">memset</span>(dist,<span class="hljs-number">0x3f</span>,<span class="hljs-keyword">sizeof</span> dist);<br>    <span class="hljs-built_in">memset</span>(use,<span class="hljs-number">0</span>,<span class="hljs-keyword">sizeof</span> use);<br>    <span class="hljs-comment">//从1号点开始，所以dist[1] = 0;</span><br>    dist[<span class="hljs-number">1</span>] = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//迭代所有点</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-comment">//设置一个t = -1，保证至少有一个点需要用到</span><br>        <span class="hljs-type">int</span> t = <span class="hljs-number">-1</span>;<br>        <span class="hljs-comment">//判断当前点是否是有效点（未被用过的最短距离）</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= n;j++)&#123;<br>            <span class="hljs-keyword">if</span>(!use[j] &amp;&amp; (t == <span class="hljs-number">-1</span> || dist[t] &gt; dist[j])) t = j;<br>        &#125;<br>        <span class="hljs-comment">//使用当前的点</span><br>        use[t] = <span class="hljs-number">1</span>;<br>        <span class="hljs-comment">//更新从当前点到其他点的最小值</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= n;j++)&#123;<br>            <span class="hljs-keyword">if</span>(dist[j] &gt; dist[t]+g[t][j])&#123;<br>                dist[j] = dist[t]+g[t][j];<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//如果更新到了n，则现在的n所存的就是最短路值</span><br>    <span class="hljs-keyword">if</span>(dist[n] != <span class="hljs-number">0x3f3f3f3f</span>) <span class="hljs-keyword">return</span> dist[n];<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-comment">//默认无穷大，即没有边</span><br>    <span class="hljs-built_in">memset</span>(g,<span class="hljs-number">0x3f</span>,<span class="hljs-keyword">sizeof</span> g);<br>    cin &gt;&gt; n &gt;&gt; m;<br>    <span class="hljs-type">int</span> a,b,w;<br>    <span class="hljs-keyword">while</span>(m--)&#123;<br>        cin &gt;&gt; a &gt;&gt; b &gt;&gt; w;<br>        <span class="hljs-comment">//注意这里只保存最短边</span><br>        g[a][b] = <span class="hljs-built_in">min</span>(g[a][b],w);<br>    &#125;<br>    <span class="hljs-type">int</span> r = <span class="hljs-built_in">dijkstr</span>();<br>    cout &lt;&lt; r;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="堆优化Dijkstra"><a href="#堆优化Dijkstra" class="headerlink" title="堆优化Dijkstra"></a>堆优化Dijkstra</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">disj</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">memset</span>(dist,<span class="hljs-number">0x3f</span>,<span class="hljs-keyword">sizeof</span> dist);<br>    q.<span class="hljs-built_in">push</span>(&#123;<span class="hljs-number">0</span>,<span class="hljs-number">1</span>&#125;);<br>    <span class="hljs-keyword">while</span>(!q.<span class="hljs-built_in">empty</span>())&#123;<br>        <span class="hljs-keyword">auto</span> t = q.<span class="hljs-built_in">top</span>();q.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-type">int</span> d = t.first;<br>        <span class="hljs-type">int</span> v = t.second;<br>        <span class="hljs-comment">//优先处理的是最短路，所以如果碰到了第二个相同点的距离，则该距离一定是大的，因此直接continue即可。</span><br>        <span class="hljs-keyword">if</span>(use[v])<span class="hljs-keyword">continue</span>;<br>        use[v] = <span class="hljs-literal">true</span>;<br>        <span class="hljs-comment">// cout &lt;&lt; d &lt;&lt; &quot;  &quot; &lt;&lt; v &lt;&lt; endl;</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = h[v];i != <span class="hljs-number">-1</span>;i = ne[i])&#123;<br>            <span class="hljs-type">int</span> k = e[i];<br>            <span class="hljs-comment">//重新压入</span><br>            <span class="hljs-keyword">if</span>(dist[k] &gt; d+w[i])&#123;<br>                q.<span class="hljs-built_in">push</span>(&#123;d+w[i],k&#125;);<br>                dist[k] = d+w[i];<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span>(dist[n] == <span class="hljs-number">0x3f3f3f3f</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>    <span class="hljs-keyword">return</span> dist[n];<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="贝尔曼算法：含负权，有重边自环，图中可能-存在负权回路-。"><a href="#贝尔曼算法：含负权，有重边自环，图中可能-存在负权回路-。" class="headerlink" title="贝尔曼算法：含负权，有重边自环，图中可能 存在负权回路 。"></a>贝尔曼算法：含负权，有重边自环，图中可能 <strong>存在负权回路</strong> 。</h3><p>步骤：</p><p>如果限制最短路上只有k条边，则最外层迭代k次，内层迭代m（边数）次，然后对每个边{a,b,w}，判断dist[b] &#x3D; min(dist[b],dist[a]+w)，即松弛操作。</p><p>由于在代码中前面的操作可能会影响后面的操作，因此需要先保存上一次的处理结果，即对dist数组进行备份（注意语法）</p><p><img src="/post/48f084a8/image-20210824165312352.png" alt="image-20210824165312352"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//所有边使用结构体存更方便</span><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">edge</span>&#123;<br>    <span class="hljs-type">int</span> a,b,w;<br>&#125; e[N];<br><span class="hljs-type">int</span> n,m,k;<br><span class="hljs-type">int</span> dist[N],backup[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">bellman_ford</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-comment">//首先设置所有点的距离为0x3f(无限大)</span><br>    <span class="hljs-built_in">memset</span>(dist,<span class="hljs-number">0x3f</span>,<span class="hljs-keyword">sizeof</span> dist);<br>    <span class="hljs-comment">//从1开始</span><br>    dist[<span class="hljs-number">1</span>] = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//因为最多不经过k条边，因此迭代k次</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; k;i++)&#123;<br>        <span class="hljs-comment">//备份，防止出现串联现象,注意这个语法</span><br>        <span class="hljs-built_in">memcpy</span>(backup,dist,<span class="hljs-keyword">sizeof</span> dist);<br>        <span class="hljs-comment">//每次对所有边进行迭代</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt; m;j++)&#123;<br>            <span class="hljs-type">int</span> a = e[j].a;<br>            <span class="hljs-type">int</span> b = e[j].b;<br>            <span class="hljs-type">int</span> w = e[j].w;<br>            <span class="hljs-comment">//松弛操作，判断三角不等式</span><br>            dist[b] = <span class="hljs-built_in">min</span>(dist[b],backup[a]+w);<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//由于含有负权，因此在处理最短路的时候dist可能会小于0x3f，但是仍然是无限大的值，因此除以2</span><br>    <span class="hljs-keyword">if</span>(dist[n] &gt; <span class="hljs-number">0x3f3f3f3f</span>/<span class="hljs-number">2</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">0x3f</span>;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">return</span> dist[n];<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n &gt;&gt; m &gt;&gt; k;<br>    <span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; m;i++)&#123;<br>        <span class="hljs-type">int</span> a,b,w;<br>        cin &gt;&gt; a &gt;&gt; b &gt;&gt; w;<br>        <span class="hljs-comment">//注意结构体赋值操作</span><br>        e[i] = &#123;a,b,w&#125;;<br>    &#125;<br>    <span class="hljs-type">int</span> r = <span class="hljs-built_in">bellman_ford</span>();<br>    <span class="hljs-keyword">if</span>(r == <span class="hljs-number">0x3f</span>)cout &lt;&lt; <span class="hljs-string">&quot;impossible&quot;</span>;<br>    <span class="hljs-keyword">else</span> cout &lt;&lt; r;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="spfa算法：含负权，可能存在重边和自环，判断是否有负环"><a href="#spfa算法：含负权，可能存在重边和自环，判断是否有负环" class="headerlink" title="spfa算法：含负权，可能存在重边和自环，判断是否有负环"></a>spfa算法：含负权，可能存在重边和自环，判断是否有负环</h3><h4 id="求最短路"><a href="#求最短路" class="headerlink" title="求最短路"></a>求最短路</h4><p>spfa是根据贝尔曼算法所做的优化，Bellman_ford算法会遍历所有的边，但是有很多的边遍历了其实没有什么意义，<strong>我们只用遍历那些到源点距离变小的点所连接的边即可，只有当一个点的前驱结点更新了，该节点才会得到更新；</strong>因此考虑到这一点，我们将创建一个队列每一次加入距离被更新的结点。</p><h5 id="值得注意的是"><a href="#值得注意的是" class="headerlink" title="值得注意的是"></a>值得注意的是</h5><p><strong>1) st数组的作用：</strong>判断当前的点是否已经加入到队列当中了；<strong>已经加入队列的结点就不需要反复的把该点加入到队列中了</strong>，就算此次还是会更新到源点的距离，那只用更新一下数值而不用加入到队列当中。<br><strong>即便不使用st数组最终也没有什么关系</strong>，但是使用的好处在于可以提升效率。</p><ol start="2"><li>SPFA算法看上去和Dijstra算法长得有一些像但是其中的意义还是相差甚远的:</li></ol><p>Dijkstra算法里使用的是<strong>优先队列保存的是当前未确定最小距离的点</strong>，目的是快速的取出当前到源点距离最小的点；<strong>SPFA算法中使用的是队列(你也可以使用别的数据结构)</strong>,目的只是记录一下当前发生过更新的点。</p><ol start="3"><li><p>⭐️Bellman_ford算法里最后return-1的判断条件写的是dist[n]&gt;0x3f3f3f3f&#x2F;2;而spfa算法写的是dist[n]&#x3D;&#x3D;0x3f3f3f3f;<strong>其原因在于Bellman_ford算法会遍历所有的边，因此不管是不是和源点连通的边它都会得到更新；</strong>但是SPFA算法不一样，它相当于采用了BFS，因此遍历到的结点都是与源点连通的，因此如果你要求的n和源点不连通，它不会得到更新，还是保持的0x3f3f3f3f。</p></li><li><p>⭐️ Bellman_ford算法<strong>可以存在负权回路，是因为其循环的次数是有限制的</strong>因此最终不会发生死循环；但是SPFA算法不可以，由于用了队列来存储，只要发生了更新就会不断的入队，因此假如有负权回路请你不要用SPFA否则会死循环。</p></li><li><p>⭐️<strong>求负环一般使用SPFA算法</strong>，方法是用一个cnt数组记录每个点到源点的边数，一个点被更新一次就+1，一旦有点的边数达到了n那就证明存在了负环。</p></li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//距离,st存储点是否在队列中，队列每个点只需要存储一个（注意一下）</span><br><span class="hljs-type">int</span> d[N],st[N];<br><span class="hljs-comment">//队列</span><br>queue&lt;<span class="hljs-type">int</span>&gt; q;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> a,<span class="hljs-type">int</span> b,<span class="hljs-type">int</span> c)</span></span>&#123;<br>    e[idx] = b;w[idx] = c;ne[idx] = h[a];h[a] = idx++;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">spfa</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-comment">//使用宽搜的方法来优化贝尔曼算法,注意初始化是用0x3f初始化</span><br>    <span class="hljs-built_in">memset</span>(d,<span class="hljs-number">0x3f</span>,<span class="hljs-keyword">sizeof</span> d);<br>    <span class="hljs-comment">//初始化</span><br>    d[<span class="hljs-number">1</span>]=<span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//从1点开始</span><br>    q.<span class="hljs-built_in">push</span>(<span class="hljs-number">1</span>);<br>    <span class="hljs-comment">//当前点入队</span><br>    st[<span class="hljs-number">1</span>] = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">while</span>(q.<span class="hljs-built_in">size</span>())&#123;<br>        <span class="hljs-type">int</span> t = q.<span class="hljs-built_in">front</span>();q.<span class="hljs-built_in">pop</span>();<br>        st[t] = <span class="hljs-number">0</span>;<br>        <span class="hljs-comment">//判断t点所连的边是否有更新</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = h[t];i != <span class="hljs-number">-1</span>;i = ne[i])&#123;<br>            <span class="hljs-type">int</span> et = e[i];<br>            <span class="hljs-type">int</span> wt = w[i];<br>            <span class="hljs-comment">//spfa算法认为只有在当前被修改小之后的点才会影响之后连接的点,使其越来越小</span><br>            <span class="hljs-keyword">if</span>(d[et] &gt; d[t]+wt)&#123;<br>                <span class="hljs-comment">//更新距离</span><br>                d[et] = d[t]+wt;<br>                <span class="hljs-comment">//如果当前点没有入队</span><br>                <span class="hljs-keyword">if</span>(!st[et])&#123;<br>                    q.<span class="hljs-built_in">push</span>(et);<br>                    st[et] = <span class="hljs-number">1</span>;<br>                &#125;<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span>(d[n] == <span class="hljs-number">0x3f3f3f3f</span>)<span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>    <span class="hljs-keyword">return</span> d[n];<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="求负环（最常用）"><a href="#求负环（最常用）" class="headerlink" title="求负环（最常用）"></a>求负环（最常用）</h4><p>1、初始的时候将所有点入队，因为负权回路可能是从任意一个点开始</p><p>2、在更新点的时候设置一个cnt数组存储<strong>x点到虚拟源点最短路的边数</strong>，初始每个点到虚拟源点的距离为0，只要他能再走n步，即cnt[x] &gt;&#x3D; n，则表示该图中一定存在负环，由于从虚拟源点到x至少经过n条边时，则说明图中至少有n + 1个点，表示一定有点是重复使用</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">spfa</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">memset</span>(d,<span class="hljs-number">0x3f</span>,<span class="hljs-keyword">sizeof</span> d);<br>    <span class="hljs-comment">//需要设置一个位置开始迭代,因为1是最开始存进去的,所以先初始化d</span><br>    d[<span class="hljs-number">1</span>] = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//将所有点加入队列</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        q.<span class="hljs-built_in">push</span>(i);st[i] = <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-keyword">while</span>(q.<span class="hljs-built_in">size</span>())&#123;<br>        <span class="hljs-type">int</span> t = q.<span class="hljs-built_in">front</span>();q.<span class="hljs-built_in">pop</span>();<br>        st[t] = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = h[t];i != <span class="hljs-number">-1</span>;i = ne[i])&#123;<br>            <span class="hljs-type">int</span> et = e[i];<br>            <span class="hljs-type">int</span> wt = w[i];<br>            <span class="hljs-keyword">if</span>(d[et] &gt; d[t] + wt)&#123;<br>                d[et] = d[t] + wt;<br>                <span class="hljs-comment">//更新cnt</span><br>                cnt[et] = cnt[t]+<span class="hljs-number">1</span>;<br>                <span class="hljs-keyword">if</span>(!st[et])&#123;<br>                    q.<span class="hljs-built_in">push</span>(et);<br>                    st[et] = <span class="hljs-number">1</span>;<br>                &#125;<br>            &#125;<br>            <span class="hljs-comment">//抽屉原理判断负环</span><br>            <span class="hljs-keyword">if</span>(cnt[et] &gt;= n) <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="Floyd算法：多源最短路"><a href="#Floyd算法：多源最短路" class="headerlink" title="Floyd算法：多源最短路"></a>Floyd算法：多源最短路</h3><p>三层循环，原理是dp</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">floyd</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> a = <span class="hljs-number">1</span>;a &lt;= n;a++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> b = <span class="hljs-number">1</span>;b &lt;= n;b++)&#123;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> c = <span class="hljs-number">1</span>;c &lt;= n;c++)&#123;<br>                d[b][c] = <span class="hljs-built_in">min</span>(d[b][c],d[b][a]+d[a][c]);<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>注意使用邻接矩阵处理含重边和自环的图时需要进行预处理</p><p>重边：d[a,b]  &#x3D;  min(d[a,b],w)</p><p>自环（默认自环为0）：d[a,a] &#x3D; 0；</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= n; i++)<br>     <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>; j &lt;= n; j++)<br>         <span class="hljs-keyword">if</span>(i == j) d[i][j] = <span class="hljs-number">0</span>;<br>         <span class="hljs-keyword">else</span> d[i][j] = INF;<br> <span class="hljs-keyword">while</span>(m--) &#123;<br>     cin &gt;&gt; x &gt;&gt; y &gt;&gt; z;<br>     d[x][y] = <span class="hljs-built_in">min</span>(d[x][y], z);<br>     <span class="hljs-comment">//注意保存最小的边</span><br> &#125;<br></code></pre></td></tr></table></figure><h3 id="图的处理总结"><a href="#图的处理总结" class="headerlink" title="图的处理总结"></a>图的处理总结</h3><p><img src="/post/48f084a8/image-20210825001903536.png" alt="image-20210825001903536"></p><h2 id="最小生成树"><a href="#最小生成树" class="headerlink" title="最小生成树"></a>最小生成树</h2><h3 id="prim算法"><a href="#prim算法" class="headerlink" title="prim算法"></a>prim算法</h3><p>和dijkstra算法十分像。</p><p>联系：Dijkstra算法是更新到起始点的距离，Prim是更新到集合S的距离</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">prim</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-comment">//初始化距离</span><br>    <span class="hljs-built_in">memset</span>(d,<span class="hljs-number">0x3f</span>,<span class="hljs-keyword">sizeof</span> d);<br>    <span class="hljs-comment">//不妨从1开始</span><br>    <span class="hljs-comment">//注意这里不要st[1],让循环能选到1进行</span><br>    d[<span class="hljs-number">1</span>] = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//结果</span><br>    <span class="hljs-type">int</span> r = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> t = <span class="hljs-number">-1</span>;<br>        <span class="hljs-comment">//找到最近点</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= n;j++)&#123;<br>            <span class="hljs-keyword">if</span>(!st[j] &amp;&amp; (t==<span class="hljs-number">-1</span> || d[t] &gt; d[j]))&#123;<br>                t = j;<br>            &#125;<br>        &#125;<br>        <span class="hljs-comment">//不同点1:判断是否连通,注意这里需要特判i,防止最开始的时候没有可连通的点</span><br>        <span class="hljs-keyword">if</span>(d[t] == INF) <span class="hljs-keyword">return</span> INF;<br>        r += d[t];<br>        st[t] = <span class="hljs-number">1</span>;      <br>        <span class="hljs-comment">//更新其他点的距离</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= n;j++)&#123;<br>            <span class="hljs-comment">//不同点2:距离更新的时候是更新和当前连通子图的距离,不是和虚拟源点的距离</span><br>            d[j] = <span class="hljs-built_in">min</span>(d[j],g[t][j]);<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> r;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="Kruskal算法"><a href="#Kruskal算法" class="headerlink" title="Kruskal算法"></a>Kruskal算法</h3><p>1、对边按照权值从小到大排序<br>2、迭代所有边，判断是否边的两端连通，如果不连通则将该边加入<br>3、判断条件：所有点已经加入（count）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> p[N];<br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Edge</span>&#123;<br>    <span class="hljs-type">int</span> a,b,w;<br>&#125;e[N];<br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">cmp</span><span class="hljs-params">(<span class="hljs-type">const</span> Edge&amp; a,<span class="hljs-type">const</span> Edge&amp;b)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> a.w &lt; b.w;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">find</span><span class="hljs-params">(<span class="hljs-type">int</span> a)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(p[a]!=a) p[a] = <span class="hljs-built_in">find</span>(p[a]);<br>    <span class="hljs-keyword">return</span> p[a];<br>&#125;<br><br><br><span class="hljs-comment">//初始化并查集</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)p[i] = i;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; m;i++)&#123;<br>    <span class="hljs-type">int</span> a,b,w;<br>    cin &gt;&gt; a &gt;&gt; b &gt;&gt; w;<br>    e[i] = &#123;a,b,w&#125;;<br>&#125;<br><span class="hljs-comment">//1、边排序</span><br><span class="hljs-built_in">sort</span>(e,e+m,cmp);<br><span class="hljs-comment">//2、从小到大判断是否两端连通</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; m;i++)&#123;<br>    <span class="hljs-type">int</span> a = e[i].a;<br>    <span class="hljs-type">int</span> b = e[i].b;<br>    <span class="hljs-type">int</span> w = e[i].w;<br>    <span class="hljs-keyword">if</span>(<span class="hljs-built_in">find</span>(a) != <span class="hljs-built_in">find</span>(b))&#123;<br>        p[<span class="hljs-built_in">find</span>(a)] = <span class="hljs-built_in">find</span>(b);<br>        r+=w;<br>        cnt++;<br>    &#125;<br>&#125;<br><span class="hljs-comment">//3、判断是否是最小生成树</span><br><span class="hljs-keyword">if</span>(cnt == n)cout &lt;&lt; r;<br><span class="hljs-keyword">else</span> cout &lt;&lt; <span class="hljs-string">&quot;impossible&quot;</span>;<br></code></pre></td></tr></table></figure><h2 id="染色法判定二分图"><a href="#染色法判定二分图" class="headerlink" title="染色法判定二分图"></a>染色法判定二分图</h2><p><img src="/post/48f084a8/image-20210825124328622.png" alt="image-20210825124328622"></p><p><img src="/post/48f084a8/image-20210825124536399.png" alt="image-20210825124536399"></p><p><strong>遍历所有点，对于每个点为起点进行dfs。</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">bool</span> j = <span class="hljs-literal">true</span>;<br><span class="hljs-comment">//保证每个点都能作为起始染色点</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-comment">//如果没有染色</span><br>    <span class="hljs-keyword">if</span>(!color[i])&#123;<br>        <span class="hljs-comment">//如果从i开始染1号点不成功</span><br>        <span class="hljs-keyword">if</span>(!<span class="hljs-built_in">dfs</span>(i,<span class="hljs-number">1</span>)) &#123;<br>            j = <span class="hljs-literal">false</span>;<br>            <span class="hljs-keyword">break</span>;<br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">dfs</span><span class="hljs-params">(<span class="hljs-type">int</span> k,<span class="hljs-type">int</span> c)</span></span>&#123;<br><span class="hljs-comment">//先染色</span><br>    color[k] = c;<br>    <span class="hljs-comment">//对连接的所有边判断情况是否成立</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = h[k];i!=<span class="hljs-number">-1</span>;i = ne[i])&#123;<br>        <span class="hljs-comment">//两种情况</span><br>        <span class="hljs-keyword">if</span>(color[e[i]] == <span class="hljs-number">0</span>)&#123;<br>            <span class="hljs-keyword">if</span>(!<span class="hljs-built_in">dfs</span>(e[i],<span class="hljs-number">3</span>-c))<span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>        &#125;<br>        <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(color[e[i]]==c)<span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>&#125;<br><br><br></code></pre></td></tr></table></figure><h2 id="匈牙利算法判断二分图的最大匹配"><a href="#匈牙利算法判断二分图的最大匹配" class="headerlink" title="匈牙利算法判断二分图的最大匹配"></a>匈牙利算法判断二分图的最大匹配</h2><p><img src="/post/48f084a8/image-20210825133131114.png" alt="image-20210825133131114"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> g[N][N];<br><span class="hljs-comment">//st用于判断当前考虑到了哪个点</span><br><span class="hljs-type">int</span> st[N];<br><span class="hljs-comment">//match用于判断当前右边的点是否已有对应</span><br><span class="hljs-type">int</span> match[N];<br><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">find</span><span class="hljs-params">(<span class="hljs-type">int</span> x)</span></span>&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n2;i++)&#123;<br>        <span class="hljs-keyword">if</span>(!st[i] &amp;&amp; g[x][i])&#123;<br>            <span class="hljs-comment">//注意这里</span><br>            <span class="hljs-comment">//st数组用来保证本次匹配过程中，第二个集合中的每个点只被遍历一次，防止死循环。</span><br>            st[i] = <span class="hljs-literal">true</span>;<br>            <span class="hljs-comment">//两种情况：第一种是右边的点空匹配，则匹配该点；第二种是右边的点匹配到的左边的点还有别的点可以匹配，则也可以匹配该点</span><br>            <span class="hljs-keyword">if</span>(!match[i] || <span class="hljs-built_in">find</span>(match[i]))&#123;<br>                match[i] = x;<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>&#125;<br>或<br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">find</span><span class="hljs-params">(<span class="hljs-type">int</span> a)</span></span>&#123;<br>    <span class="hljs-comment">//对a所连接的边进行迭代</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = h[a];i != <span class="hljs-number">-1</span>;i = ne[i])&#123;<br>        <span class="hljs-type">int</span> et = e[i];<br>        <span class="hljs-keyword">if</span>(!st[et])&#123;<br>            st[et] = <span class="hljs-literal">true</span>;<br>            <span class="hljs-keyword">if</span>(!match[et] || <span class="hljs-built_in">find</span>(match[et]))&#123;<br>                match[et] = a;<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>    <br>&#125;<br></code></pre></td></tr></table></figure><h2 id="质数求解"><a href="#质数求解" class="headerlink" title="质数求解"></a>质数求解</h2><h3 id="试除法"><a href="#试除法" class="headerlink" title="试除法"></a>试除法</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//试除法是对定义的应用</span><br><span class="hljs-comment">/*</span><br><span class="hljs-comment">            两个条件：</span><br><span class="hljs-comment">            1、m是1，则No</span><br><span class="hljs-comment">            2、m是2，则判断从2到根号n是否有可以整除的数字</span><br><span class="hljs-comment">                原理：若d|n,则(n/d)|n </span><br><span class="hljs-comment">        */</span><br><span class="hljs-keyword">if</span>(m&lt;<span class="hljs-number">2</span>) &#123;<br>    cout &lt;&lt; <span class="hljs-string">&quot;No&quot;</span> &lt;&lt; endl;<span class="hljs-keyword">continue</span>;<br>&#125;<br><span class="hljs-comment">//这里注意一下i &lt;= m/i，只能用这种，i*i&lt;=m有溢出风险，i&lt;=sqrt(m)太慢</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">2</span>;i &lt;= m/i;i++)&#123;<br>    <span class="hljs-keyword">if</span>(m % i == <span class="hljs-number">0</span>)&#123;<br>        cout &lt;&lt; <span class="hljs-string">&quot;No&quot;</span> &lt;&lt; endl;<br>        k=<span class="hljs-number">0</span>;<span class="hljs-keyword">break</span>;<br>    &#125;<br>&#125;<br><span class="hljs-keyword">if</span>(k)cout &lt;&lt; <span class="hljs-string">&quot;Yes&quot;</span> &lt;&lt; endl;<br></code></pre></td></tr></table></figure><h3 id="筛质数"><a href="#筛质数" class="headerlink" title="筛质数"></a>筛质数</h3><h4 id="埃式筛法"><a href="#埃式筛法" class="headerlink" title="埃式筛法"></a>埃式筛法</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//埃式筛法</span><br>    <span class="hljs-type">int</span> c= <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//从2开始筛</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">2</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">if</span>(!st[i])&#123;<br>            c++;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = i;j &lt;= n;j+=i)&#123;<br>                st[j] = <span class="hljs-number">1</span>;<br>            &#125;<br>        &#125;<br>    &#125;<br><br></code></pre></td></tr></table></figure><h4 id="线性筛法"><a href="#线性筛法" class="headerlink" title="线性筛法"></a>线性筛法</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//线性筛法:拿质数筛质数</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">2</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-comment">//找到质数</span><br>    <span class="hljs-keyword">if</span>(!st[i])&#123;<br>         <span class="hljs-comment">//假设primes[0]为n最小的质因子,i为最大的因数，</span><br>        <span class="hljs-comment">//易知若primes[i]中i&gt;0,则会进入循环后产生多余的标记</span><br>        p[c++] = i;<br>        st[i] = <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-comment">//以当前质数为因子，之前的质数为另一个因子，筛数，质数*质数一定是不一样的</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;p[j] &lt;= n/i;j++)&#123;            <br>        <span class="hljs-comment">//标记;primes[j]一定是primes[j]*i的最小质因子</span><br>        st[p[j]*i] = <span class="hljs-number">1</span>;<br>        <span class="hljs-comment">//表明primes[j]一定是i的最小质因子,没有必要再遍历,primes要小于等于i的最小质因子</span><br>        <span class="hljs-comment">//这样能保证每个数遍历一遍,而没有重复</span><br>        <span class="hljs-keyword">if</span>(i % p[j] == <span class="hljs-number">0</span>)<span class="hljs-keyword">break</span>;<br>    &#125;<br>&#125;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">2</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-keyword">if</span>(!st[i])&#123;<br>        st[i]=<span class="hljs-number">1</span>;<br>        prime[cnt++] =i;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;prime[j]&lt;=n/i;j++)&#123;<br>            st[prime[j]*i]=<span class="hljs-number">1</span>;<br>            <span class="hljs-keyword">if</span>(i%prime[j])<span class="hljs-keyword">break</span>;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="约数求解"><a href="#约数求解" class="headerlink" title="约数求解"></a>约数求解</h2><h3 id="试除法求约数"><a href="#试除法求约数" class="headerlink" title="试除法求约数"></a>试除法求约数</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c++">vector&lt;<span class="hljs-type">int</span>&gt; v;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= a/i;i++)&#123;<br>    <span class="hljs-keyword">if</span>(a % i == <span class="hljs-number">0</span>)&#123;<br>        v.<span class="hljs-built_in">push_back</span>(i);<br>        <span class="hljs-keyword">if</span>(i != a/i)v.<span class="hljs-built_in">push_back</span>(a/i);<br>    &#125;<br>&#125;<br><span class="hljs-built_in">sort</span>(v.<span class="hljs-built_in">begin</span>(),v.<span class="hljs-built_in">end</span>());<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> t:v)&#123;<br>    cout &lt;&lt; t &lt;&lt; <span class="hljs-string">&quot; &quot;</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="求约数个数"><a href="#求约数个数" class="headerlink" title="求约数个数"></a>求约数个数</h3><p><img src="/post/48f084a8/image-20210825221658353.png" alt="image-20210825221658353"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c++"> <span class="hljs-keyword">while</span>(n--)&#123;<br>     <span class="hljs-type">int</span> a;<br>     cin &gt;&gt; a;<br>     <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">2</span>;i &lt;= a/i;i++)&#123;<br>         <span class="hljs-keyword">if</span>(a % i == <span class="hljs-number">0</span>)&#123;<br>        <span class="hljs-comment">//注意这里是取模判断，素数一定会在质数判断的时候被除完</span><br>             <span class="hljs-keyword">while</span>(a % i == <span class="hljs-number">0</span>)&#123;<br>                 m[i]++;<br>                 a/=i;<br>             &#125;<br>         &#125;<br>     &#125;<br>     <span class="hljs-comment">//注意这里</span><br>     <span class="hljs-keyword">if</span>(a &gt; <span class="hljs-number">1</span>)m[a]++;<br> &#125;<br><span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> a:m)&#123;<br>    <span class="hljs-comment">//注意这里是对每个乘数取模，这样才能存到longlong里，其中需要对res*j取模，</span><br>    <span class="hljs-comment">//res *= (a.second + 1) % N;  这样写等于没取模</span><br>    res = (res*(a.second+<span class="hljs-number">1</span>)) % N;<br>&#125;<br>cout &lt;&lt; res;<br></code></pre></td></tr></table></figure><h3 id="约数和：秦九韶算法"><a href="#约数和：秦九韶算法" class="headerlink" title="约数和：秦九韶算法"></a>约数和：秦九韶算法</h3><p><img src="/post/48f084a8/image-20210825223455741.png" alt="image-20210825223455741"></p><h3 id="最大公约数：辗转相除法"><a href="#最大公约数：辗转相除法" class="headerlink" title="最大公约数：辗转相除法"></a>最大公约数：辗转相除法</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">gcd</span><span class="hljs-params">(<span class="hljs-type">int</span> a,<span class="hljs-type">int</span> b)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> b==<span class="hljs-number">0</span>?a:<span class="hljs-built_in">gcd</span>(b,a%b);<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="欧拉函数"><a href="#欧拉函数" class="headerlink" title="欧拉函数"></a>欧拉函数</h2><img src="/post/image-20210825230159042.png" alt="image-20210825230159042" style="zoom:50%;"><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">while</span>(n--)&#123;<br>    <span class="hljs-type">int</span> a;<br>    cin &gt;&gt; a;<br>    <span class="hljs-comment">//注意这里使用long long,防止越界</span><br>    <span class="hljs-type">long</span> <span class="hljs-type">long</span> r = a;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">2</span>;i &lt;= a/i;i++)&#123;<br>        <span class="hljs-keyword">if</span>(a % i == <span class="hljs-number">0</span>)&#123;<br>            r = r*(i<span class="hljs-number">-1</span>)/i;<br>            <span class="hljs-keyword">while</span>(a % i == <span class="hljs-number">0</span>) a/=i;<br>        &#125; <br>    &#125;<br>    <span class="hljs-keyword">if</span>(a &gt; <span class="hljs-number">1</span>)r = r*(a<span class="hljs-number">-1</span>)/a;<br>    cout &lt;&lt; r &lt;&lt; endl;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="筛法求欧拉和"><a href="#筛法求欧拉和" class="headerlink" title="筛法求欧拉和"></a>筛法求欧拉和</h3><p><img src="/post/48f084a8/image-20210826115712665.png" alt="image-20210826115712665"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">ruler</span><span class="hljs-params">(<span class="hljs-type">int</span> n)</span></span>&#123;<br>    <span class="hljs-comment">// 首先背线筛的板子</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">2</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-keyword">if</span>(!st[i])&#123;<br>            p[cnt++] = i;<br>            <span class="hljs-comment">//如果是质数,则1~i-1都是互质的数</span><br>            phi[i] = i<span class="hljs-number">-1</span>;<br>            st[i] = <span class="hljs-number">1</span>;<br>        &#125;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;p[j] &lt;= n/i;j++)&#123;<br>            st[p[j]*i] = <span class="hljs-number">1</span>;<br>            <span class="hljs-comment">//分情况讨论,这里处理的是p[j]*i的欧拉函数</span><br>            <span class="hljs-comment">//如果是i%p[j] == 0,则p[j]*i和i的质因子相同,只需补N值</span><br>            <span class="hljs-keyword">if</span>(i % p[j] == <span class="hljs-number">0</span>)&#123;<br>                phi[p[j]*i] = phi[i]*p[j]; <br>                <span class="hljs-keyword">break</span>;<br>            &#125;<br>            <span class="hljs-comment">//如果是i%p[j] != 0,则需要在补p[j]的基础上再乘以(p[j]-1/p[j])</span><br>            <span class="hljs-keyword">else</span> phi[p[j]*i] = phi[i]*(p[j]<span class="hljs-number">-1</span>);<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">2</span>;i &lt;= n;i++) r+=phi[i];<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="快速幂"><a href="#快速幂" class="headerlink" title="快速幂"></a>快速幂</h2><h3 id="板子"><a href="#板子" class="headerlink" title="板子"></a>板子</h3><p>快速幂需要注意%p的位置</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">typedef</span> <span class="hljs-type">long</span> <span class="hljs-type">long</span> LL;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">qmi</span><span class="hljs-params">(LL a,<span class="hljs-type">int</span> b,<span class="hljs-type">int</span> p)</span></span>&#123;<br>    LL r = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">while</span>(b)&#123;<br>        <span class="hljs-comment">//如果b的二进制表示的第0位为1,则乘上当前的a</span><br>        <span class="hljs-keyword">if</span>(b&amp;<span class="hljs-number">1</span>) r = (LL)((r*a)%p);<br>        <span class="hljs-comment">//右移一位</span><br>        b &gt;&gt;= <span class="hljs-number">1</span>;<br>        <span class="hljs-comment">//更新a,a依次为a^&#123;2^0&#125;,a^&#123;2^1&#125;,a^&#123;2^2&#125;,....,a^&#123;2^logb&#125;</span><br>        a = a*a%p;<br>    &#125;<br>    <span class="hljs-keyword">return</span> r;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="快速幂求逆元"><a href="#快速幂求逆元" class="headerlink" title="快速幂求逆元"></a>快速幂求逆元</h3><p><img src="/post/48f084a8/image-20210826131138176.png" alt="image-20210826131138176"></p><p><img src="/post/48f084a8/image-20210826131645258.png" alt="image-20210826131645258"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//a = a * b * b^-1 mod p</span><br><span class="hljs-comment">//∴ b *b^-1mod p==1</span><br><span class="hljs-comment">//而由小费马定理得 b^p-1 mod p == 1;</span><br><span class="hljs-comment">//b^-1 == b^p-2;</span><br><span class="hljs-comment">//即求b^p-2 mod p的值，一个快速幂可以解决</span><br><span class="hljs-keyword">else</span> cout &lt;&lt; <span class="hljs-built_in">qmi</span>(a,p<span class="hljs-number">-2</span>,p) &lt;&lt; endl;<br></code></pre></td></tr></table></figure><h2 id="扩展欧几里得算法"><a href="#扩展欧几里得算法" class="headerlink" title="扩展欧几里得算法"></a>扩展欧几里得算法</h2><p><img src="/post/48f084a8/image-20210826133840327.png" alt="image-20210826133840327"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//扩展欧几里得算法在求得gcd的基础上还可以求得一组x,y解</span><br><span class="hljs-comment">//注意这里必须使用地址符</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">exgcd</span><span class="hljs-params">(<span class="hljs-type">int</span> a,<span class="hljs-type">int</span> b,<span class="hljs-type">int</span> &amp;x,<span class="hljs-type">int</span> &amp;y)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(!b)&#123;<br>        x=<span class="hljs-number">1</span>;y=<span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">return</span> a;<br>    &#125;<br>    <span class="hljs-type">int</span> d = <span class="hljs-built_in">exgcd</span>(b,a%b,y,x);<br>    <span class="hljs-comment">/*</span><br><span class="hljs-comment">        ax + by = by+(a%b)x = d;</span><br><span class="hljs-comment">        ∴ax+by = by+(a-(a/b)*b)x</span><br><span class="hljs-comment">        ax+by = by+ax-(a/b)*bx</span><br><span class="hljs-comment">        ax+by = b(y-(a/b)x)+ax;</span><br><span class="hljs-comment">        ∴x不变,y=y-(a/b)*x;</span><br><span class="hljs-comment">    */</span><br>    y = y - (a/b)*x;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="求线性同余方程"><a href="#求线性同余方程" class="headerlink" title="求线性同余方程"></a>求线性同余方程</h3><p><img src="/post/48f084a8/image-20210826143242611.png" alt="image-20210826143242611"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> a,b,m,x,y;<br>cin &gt;&gt; a &gt;&gt; b &gt;&gt; m;<br><span class="hljs-comment">//ax 同余 b mod m的对应条件是存在一组x,y使得ax = my+b</span><br><span class="hljs-comment">//转一下得ax+my&#x27; = b，只要b是gcd(a,m)的倍数，就一定存在一组x,y&#x27;=-y使其成立</span><br><span class="hljs-type">int</span> d = <span class="hljs-built_in">exgcd</span>(a,m,x,y);<br><span class="hljs-keyword">if</span>(b % d != <span class="hljs-number">0</span>) cout &lt;&lt; <span class="hljs-string">&quot;impossible&quot;</span> &lt;&lt; endl;<br><span class="hljs-comment">//题目说答案是int，所以我们%d输出，但是在mod m之前的乘法运算中，就可能暴int了，所以我们这里拿ll接一下</span><br><br><span class="hljs-comment">//这里真的是把人差点卡死在这儿</span><br><span class="hljs-comment">//因为x的结果是ax+my==d的解，不是等于b的解，所以首先要x *(b/d)将数放回去,因为这里可能爆栈所以拿LL先接一下，方便计算</span><br><span class="hljs-comment">//然后保险起见，用m mod一下即可，根据(a*x) % m = (a * (x % m)) % m，所以保险起见输出为x % m</span><br><span class="hljs-keyword">else</span> cout &lt;&lt; (LL)x *(b/d) % m &lt;&lt; endl;<br></code></pre></td></tr></table></figure><h2 id="中国剩余定理"><a href="#中国剩余定理" class="headerlink" title="中国剩余定理"></a>中国剩余定理</h2><h3 id="表达整数的奇怪方式"><a href="#表达整数的奇怪方式" class="headerlink" title="表达整数的奇怪方式"></a>表达整数的奇怪方式</h3><p><img src="/post/48f084a8/image-20210829182612966.png" alt="image-20210829182612966"><img src="/post/48f084a8/image-20210829182619920.png" alt="image-20210829182619920"></p><p><img src="/post/48f084a8/image-20210829182632294.png" alt="image-20210829182632294"></p><p><img src="/post/48f084a8/image-20210829182641612.png" alt="image-20210829182641612"></p><p><img src="/post/48f084a8/image-20210829182647238.png" alt="image-20210829182647238"></p><p><strong>推导过程：</strong></p><p><img src="/post/48f084a8/image-20210829184136757.png" alt="image-20210829184136757"></p><h2 id="组合数问题"><a href="#组合数问题" class="headerlink" title="组合数问题"></a>组合数问题</h2><h3 id="第一种解法：公式法递推"><a href="#第一种解法：公式法递推" class="headerlink" title="第一种解法：公式法递推"></a>第一种解法：公式法递推</h3><p>运用公式：Cb&#x2F;a &#x3D; Cb&#x2F;a-1+Cb-1&#x2F;a-1;</p><p>首先将所有的组合数情况用类似DP的方法给计算出来，然后直接哈希对应</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-comment">//从0开始</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt;= <span class="hljs-number">2000</span>;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt;= i;j++)&#123;<br>            <span class="hljs-comment">//j==0时的情况只有什么都不选这一种情况</span><br>            <span class="hljs-keyword">if</span>(!j) f[i][j] = <span class="hljs-number">1</span>;<br>            <span class="hljs-keyword">else</span> f[i][j] = (f[i<span class="hljs-number">-1</span>][j<span class="hljs-number">-1</span>]+f[i<span class="hljs-number">-1</span>][j])%mod;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="第二种解法：初始化阶乘值"><a href="#第二种解法：初始化阶乘值" class="headerlink" title="第二种解法：初始化阶乘值"></a>第二种解法：初始化阶乘值</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//快速幂的板子</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">qmi</span><span class="hljs-params">(<span class="hljs-type">int</span> a,<span class="hljs-type">int</span> k,<span class="hljs-type">int</span> q)</span></span>&#123;<br>    <span class="hljs-type">int</span> res = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">while</span>(k)&#123;<br>        <span class="hljs-comment">//注意LL</span><br>        <span class="hljs-keyword">if</span>(k&amp;<span class="hljs-number">1</span>) res = (LL)res*a%q;<br>        a = (LL)a*a%q;<br>        k &gt;&gt;= <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n;<br>    cin &gt;&gt; n;<br>    <span class="hljs-comment">//初始化</span><br>    f[<span class="hljs-number">0</span>] = inf[<span class="hljs-number">0</span> ] =<span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt; N;i++)&#123;<br>        <span class="hljs-comment">//注意LL的问题</span><br>        f[i] = (LL)f[i<span class="hljs-number">-1</span>]*i% mod;<br>        <span class="hljs-comment">//在这里使用到了逆元值，inf[i]代表i % mod的逆元，i %mod的逆元为i^m-2;</span><br>        inf[i] = (LL)inf[i<span class="hljs-number">-1</span>]*<span class="hljs-built_in">qmi</span>(i,mod<span class="hljs-number">-2</span>,mod) % mod;<br>    &#125;<br>    <span class="hljs-keyword">while</span>(n--)&#123;<br>        <span class="hljs-type">int</span> a,b;<br>        cin &gt;&gt; a &gt;&gt; b;<br>        <span class="hljs-comment">//注意LL的问题以及连乘时的两次mod</span><br>        cout &lt;&lt; (LL)f[a] * inf[a-b]%mod * inf[b]%mod &lt;&lt; endl;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p><img src="/post/48f084a8/image-20210829225806186.png" alt="image-20210829225806186"></p><h3 id="第三种解法：卢卡斯定理"><a href="#第三种解法：卢卡斯定理" class="headerlink" title="第三种解法：卢卡斯定理"></a>第三种解法：卢卡斯定理</h3><img src="/post/image-20210829234753028.png" alt="image-20210829234753028" style="zoom:50%;"><p>如何应用：直接应用公式</p><p>首先：注意LL</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-keyword">typedef</span> <span class="hljs-type">long</span> <span class="hljs-type">long</span> LL;<br><span class="hljs-type">int</span> p;<br><span class="hljs-comment">//快速幂</span><br><span class="hljs-function">LL <span class="hljs-title">qmi</span><span class="hljs-params">(<span class="hljs-type">int</span> a,<span class="hljs-type">int</span> k,<span class="hljs-type">int</span> p)</span></span>&#123;<br>    LL res = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">while</span>(k)&#123;<br>        <span class="hljs-keyword">if</span>(k&amp;<span class="hljs-number">1</span>) res = (LL)res*a%p;<br>        a = (LL)a*a%p;<br>        k &gt;&gt;= <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br><span class="hljs-comment">//正常求C的值</span><br><span class="hljs-function">LL <span class="hljs-title">C</span><span class="hljs-params">(LL a,LL b)</span></span>&#123;<br>    LL res = <span class="hljs-number">1</span>;<br>    <span class="hljs-comment">//上边从a开始，下边从1开始</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>,j = a;i &lt;= b;i++,j--)&#123;<br>        res = (LL)res * j;<br>        <span class="hljs-comment">//注意这里：逆元的成立条件中有一个%p，需要确定好p的取值</span><br>        res = (LL)res * <span class="hljs-built_in">qmi</span>(i,p<span class="hljs-number">-2</span>,p) % p;<br>    &#125;<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br><span class="hljs-comment">//卢卡斯定理</span><br><span class="hljs-function">LL <span class="hljs-title">lucas</span><span class="hljs-params">(LL a,LL b)</span></span>&#123;<br>    <span class="hljs-comment">//特例判断</span><br>    <span class="hljs-keyword">if</span>(a &lt; p &amp;&amp; b &lt; p)<span class="hljs-keyword">return</span> <span class="hljs-built_in">C</span>(a,b);<br>    <span class="hljs-comment">//否则使用lucas定理开始降维</span><br>    <span class="hljs-comment">//注意公式的适用条件中有一个同余</span><br>    <span class="hljs-keyword">else</span> <span class="hljs-built_in">return</span> (LL)<span class="hljs-built_in">C</span>(a%p,b%p) * <span class="hljs-built_in">lucas</span>(a/p,b/p) % p;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">while</span>(n--)&#123;<br>        LL a,b;<br>        cin &gt;&gt; a &gt;&gt; b &gt;&gt; p;<br>        cout &lt;&lt; <span class="hljs-built_in">lucas</span>(a,b) &lt;&lt; endl;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="第四种解法：高精度乘法-质数分解抵消"><a href="#第四种解法：高精度乘法-质数分解抵消" class="headerlink" title="第四种解法：高精度乘法+质数分解抵消"></a>第四种解法：高精度乘法+质数分解抵消</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">5010</span>;<br><span class="hljs-type">int</span> prime[N],cnt;<br><span class="hljs-type">int</span> sum[N],st[N];<br><span class="hljs-comment">//线性筛法求质数</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">get_prime</span><span class="hljs-params">(<span class="hljs-type">int</span> a)</span></span>&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">2</span>;i &lt;= a;i++)&#123;<br>        <span class="hljs-keyword">if</span>(!st[i])prime[cnt++] = i;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;prime[j] &lt;= a/i;j++)&#123;<br>            st[prime[j]*i] = <span class="hljs-literal">true</span>;<br>            <span class="hljs-keyword">if</span>(i % prime[j]==<span class="hljs-number">0</span>)<span class="hljs-keyword">break</span>;<br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-comment">//得到a!中的p质因子的个数</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">get</span><span class="hljs-params">(<span class="hljs-type">int</span> a,<span class="hljs-type">int</span> p)</span></span>&#123;<br>    <span class="hljs-type">int</span> res = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">while</span>(a)&#123;<br>        res+=a/p;<br>        a/=p;<br>    &#125;<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br><span class="hljs-comment">//高精度乘法</span><br><span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">mul</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt; a,<span class="hljs-type">int</span> b)</span></span>&#123;<br>    <span class="hljs-type">int</span> t = <span class="hljs-number">0</span>;<br>    vector&lt;<span class="hljs-type">int</span>&gt; res;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; a.<span class="hljs-built_in">size</span>();i++)&#123;<br>        t += a[i]*b;<br>        res.<span class="hljs-built_in">push_back</span>(t%<span class="hljs-number">10</span>);<br>        t/=<span class="hljs-number">10</span>;<br>    &#125;<br>    <span class="hljs-keyword">while</span>(t)&#123;<br>        res.<span class="hljs-built_in">push_back</span>(t%<span class="hljs-number">10</span>);<br>        t/=<span class="hljs-number">10</span>;<br>    &#125;<br>    <span class="hljs-comment">// if(t) res.push_back(t);</span><br>    <span class="hljs-keyword">while</span>(res.<span class="hljs-built_in">size</span>() &amp;&amp; res.<span class="hljs-built_in">back</span>()==<span class="hljs-number">0</span>) res.<span class="hljs-built_in">pop_back</span>();<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> a,b;<br>    cin &gt;&gt; a &gt;&gt; b;<br>    <span class="hljs-comment">//对最大值求质数</span><br>    <span class="hljs-built_in">get_prime</span>(a);<br>    <span class="hljs-comment">//对所有质数求结果中应该有的数量,其中大部分质数的个数都被抵消掉，这里是本算法的核心优化</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; cnt;i++)&#123;<br>        sum[i] = <span class="hljs-built_in">get</span>(a,prime[i])-<span class="hljs-built_in">get</span>(a-b,prime[i])-<span class="hljs-built_in">get</span>(b,prime[i]);<br>    &#125;<br>    vector&lt;<span class="hljs-type">int</span>&gt; ans;<br>    <span class="hljs-comment">//初始化</span><br>    ans.<span class="hljs-built_in">push_back</span>(<span class="hljs-number">1</span>);<br>    <span class="hljs-comment">//对所有质数求结果</span><br>    <span class="hljs-comment">//迭代所有质数</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; cnt;i++)&#123;<br>        <span class="hljs-comment">//迭代所有个数</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt; sum[i];j++)&#123;<br>            ans = <span class="hljs-built_in">mul</span>(ans,prime[i]);<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">//输出</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = ans.<span class="hljs-built_in">size</span>()<span class="hljs-number">-1</span>;i &gt;= <span class="hljs-number">0</span>;i--)&#123;<br>        cout &lt;&lt; ans[i];<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="满足条件的01序列"><a href="#满足条件的01序列" class="headerlink" title="满足条件的01序列"></a>满足条件的01序列</h3><p><img src="/post/48f084a8/image-20210831151233385.png" alt="image-20210831151233385"></p><p><img src="/post/48f084a8/image-20210831151327608.png" alt="image-20210831151327608"></p><p>红色线是高压红线。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-keyword">typedef</span> <span class="hljs-type">long</span> <span class="hljs-type">long</span> LL;<br><span class="hljs-comment">//注意这里开大！！！</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">200010</span>,mod = <span class="hljs-number">1e9</span>+<span class="hljs-number">7</span>;<br>LL f[N],inf[N];<br><span class="hljs-function">LL <span class="hljs-title">qmi</span><span class="hljs-params">(<span class="hljs-type">int</span> a,<span class="hljs-type">int</span> k,<span class="hljs-type">int</span> q)</span></span>&#123;<br>    <span class="hljs-type">int</span> res = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">while</span>(k)&#123;<br>        <span class="hljs-keyword">if</span>(k&amp;<span class="hljs-number">1</span>) res = (LL)res*a%q;<br>        a = (LL)a*a%q;<br>        k &gt;&gt;= <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-keyword">return</span> res;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span>&#123;<br>    f[<span class="hljs-number">0</span>] = inf[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt; N; i++)&#123;<br>        f[i] = (LL)f[i<span class="hljs-number">-1</span>]*i%mod;<br>        inf[i] = (LL)inf[i<span class="hljs-number">-1</span>]*<span class="hljs-built_in">qmi</span>(i,mod<span class="hljs-number">-2</span>,mod)%mod;<br>    &#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n;cin &gt;&gt; n;<span class="hljs-built_in">init</span>();<br>    <span class="hljs-type">int</span> res = (LL)f[<span class="hljs-number">2</span> * n]*inf[n] % mod * inf[n] % mod * <span class="hljs-built_in">qmi</span>(n + <span class="hljs-number">1</span>, mod - <span class="hljs-number">2</span>,mod) % mod;<br>        cout &lt;&lt; res &lt;&lt; endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="容斥原理"><a href="#容斥原理" class="headerlink" title="容斥原理"></a>容斥原理</h2><p><img src="/post/48f084a8/image-20210831202929031.png" alt="image-20210831202929031"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//用位运算表示选那几个数字，如果是奇数个则加，如果是偶数个即减。</span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-keyword">typedef</span> <span class="hljs-type">long</span> <span class="hljs-type">long</span> LL;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">1e9</span>+<span class="hljs-number">7</span>,M = <span class="hljs-number">20</span>;<br><span class="hljs-type">int</span> n,m,f[M];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n &gt;&gt; m;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; m;i++)&#123;<br>        cin &gt;&gt; f[i];<br>    &#125;<br>    <span class="hljs-type">int</span> res=<span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//位运算迭代</span><br>    <span class="hljs-comment">//枚举从1 到 1111...(m个1)的每一个集合状态, (至少选中一个集合)</span><br>    <span class="hljs-comment">//从1开始枚举， 枚举到 2^m-1</span><br>    <span class="hljs-comment">//一共有2^m-1项（除了一个都不选之外</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt; <span class="hljs-number">1</span> &lt;&lt; m;i++)&#123;<br>        <span class="hljs-comment">//质数乘积</span><br>        LL t = <span class="hljs-number">1</span>;<br>        <span class="hljs-comment">//位的数量</span><br>        <span class="hljs-type">int</span> s = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt; m;j++)&#123;<br>            <span class="hljs-keyword">if</span>(i&gt;&gt;j&amp;<span class="hljs-number">1</span>)&#123;<br><span class="hljs-comment">//乘积大于n, 则n/t = 0, 跳出这轮循环，可加可不加</span><br>                <span class="hljs-comment">//当前质数的乘积就已经&gt;n了， 直接跳出即可 </span><br>                <span class="hljs-comment">//注意这里需要放到LL</span><br>                <span class="hljs-keyword">if</span>((LL)t*f[j] &gt; n)&#123;<br>                    t = <span class="hljs-number">-1</span>;<span class="hljs-keyword">break</span>;<br>                &#125;<br>                s++;t*= f[j];<br>            &#125;<br>        &#125;<br>        <span class="hljs-comment">//1~n中能被p整除的个数：n/p下取整</span><br>        <span class="hljs-keyword">if</span>(t != <span class="hljs-number">-1</span>)&#123;<br>            <span class="hljs-keyword">if</span>(s%<span class="hljs-number">2</span>==<span class="hljs-number">1</span>) res += n/t;<br>            <span class="hljs-keyword">else</span> res -= n/t; <br>        &#125;<br>    &#125;<br>    cout &lt;&lt; res;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="博弈论"><a href="#博弈论" class="headerlink" title="博弈论"></a>博弈论</h2><h3 id="公平组合游戏"><a href="#公平组合游戏" class="headerlink" title="公平组合游戏"></a>公平组合游戏</h3><p><img src="/post/48f084a8/image-20210831213559881.png" alt="image-20210831213559881"></p><p><img src="/post/48f084a8/image-20210831222636790.png" alt="image-20210831222636790"></p><p><img src="/post/48f084a8/image-20210831221029922.png" alt="image-20210831221029922"></p><p><img src="/post/48f084a8/image-20210831221147080.png" alt="image-20210831221147080"></p><p><img src="/post/48f084a8/image-20210831221131911.png" alt="image-20210831221131911"></p><h3 id="台阶NIM问题"><a href="#台阶NIM问题" class="headerlink" title="台阶NIM问题"></a>台阶NIM问题</h3><p><img src="/post/48f084a8/image-20210831230715516.png" alt="image-20210831230715516"></p><p><strong>解：保证奇数点的数量保持一致不变，这样我们一定可以及时走下去</strong></p><p><img src="/post/48f084a8/image-20210831230647515.png" alt="image-20210831230647515"></p><img src="/post/image-20210831230625539.png" alt="image-20210831230625539" style="zoom: 50%;"><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> cnt = <span class="hljs-number">1</span>;<br><span class="hljs-type">int</span> res = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">while</span>(n--)&#123;<br>    <span class="hljs-type">int</span> a;cin &gt;&gt; a;<br>    <span class="hljs-keyword">if</span>(cnt&amp;<span class="hljs-number">1</span>)res ^= a;<br>    cnt++;<br>&#125;<br><span class="hljs-keyword">if</span>(res) cout &lt;&lt; <span class="hljs-string">&quot;Yes&quot;</span>;<br><span class="hljs-keyword">else</span> cout &lt;&lt; <span class="hljs-string">&quot;No&quot;</span>;<br></code></pre></td></tr></table></figure><h3 id="集合NIM问题"><a href="#集合NIM问题" class="headerlink" title="集合NIM问题"></a>集合NIM问题</h3><p><img src="/post/48f084a8/image-20210831234405143.png" alt="image-20210831234405143"></p><p><strong>SG函数：到不了的自然数最小值+1</strong></p><img src="/post/image-20210901000015149.png" alt="image-20210901000015149" style="zoom: 33%;"><p>SG(x) &#x3D; 0的话，其对应点无法走到终点，因为没有一条数字下降的路。</p><p>SG(x) !&#x3D; 0的话，一定能找到一条路，其路可以指向终点SG(0)</p><p>多个图的处理：SG(x1)^SG(x2)^…^SG(xn) &#x3D; 0必败，!&#x3D;0必胜，其中x1,x2,…,xn是每个图的起点</p><p><strong>1.Mex运算:</strong><br>设S表示一个非负整数集合.定义mex(S)为求出不属于集合S的最小非负整数运算,即:<br>mes(S)&#x3D;min{x};<br>例如:S&#x3D;{0,1,2,4},那么mes(S)&#x3D;3;</p><p><strong>2.SG函数</strong><br>在有向图游戏中,对于每个节点x,设从x出发共有k条有向边,分别到达节点y1,y2,····yk,定义SG(x)的后记节点y1,y2,····<br>yk的SG函数值构成的集合在执行mex运算的结果,即:<br>SG(x)&#x3D;mex({SG(y1),SG(y2)····SG(yk)})<br>特别地,整个有向图游戏G的SG函数值被定义为有向图游戏起点s的SG函数值,即 SG(G)&#x3D;SG(s).</p><p><strong>3.有向图游戏的和</strong><br>设G1，G2,····,Gm是m个有向图游戏.定义有向图游戏G,他的行动规则是任选某个有向图游戏Gi,并在Gi上行动一步.G被称为有向图游戏G1,G2,·····,Gm的和.<br>有向图游戏的和的SG函数值等于它包含的各个子游戏SG函数的异或和,即:<br>SG(G)&#x3D;SG(G1)xorSG(G2)xor···xor SG(Gm)</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">sg</span><span class="hljs-params">(<span class="hljs-type">int</span> x)</span></span>&#123;<br>    <span class="hljs-comment">//记忆化搜索</span><br>    <span class="hljs-keyword">if</span>(f[x] != <span class="hljs-number">-1</span>)<span class="hljs-keyword">return</span> f[x];<br>    set&lt;<span class="hljs-type">int</span>&gt; S;<br>    <span class="hljs-comment">//对所有可能连接到的情况进行迭代，如果x&gt;s[i]，则存入x-s[i]对应的点，加上该分支</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-keyword">if</span>(x&gt;=s[i])S.<span class="hljs-built_in">insert</span>(<span class="hljs-built_in">sg</span>(x-s[i]));<br>    &#125;<br>    <span class="hljs-comment">//判断所有分支的最大值</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;;i++)&#123;<br>        <span class="hljs-comment">//找到最大值赋值</span><br>        <span class="hljs-keyword">if</span>(!S.<span class="hljs-built_in">count</span>(i))<span class="hljs-keyword">return</span> f[x] = i;<br>    &#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">sg</span><span class="hljs-params">(<span class="hljs-type">int</span> x)</span></span>&#123;<br><span class="hljs-keyword">if</span>(f[x]!=<span class="hljs-number">-1</span>)<span class="hljs-keyword">return</span> f[x];<br>    set&lt;<span class="hljs-type">int</span>&gt; S;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-keyword">if</span>(x&gt;=s[i])S.<span class="hljs-built_in">insert</span>(<span class="hljs-built_in">sg</span>(x-s[i]));<br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;;i++)&#123;<br>        <span class="hljs-keyword">if</span>(!s.<span class="hljs-built_in">count</span>(i))<span class="hljs-keyword">return</span> f[x]=i;<br>    &#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)cin &gt;&gt; s[i];<br>    cin &gt;&gt; k;<br>    <span class="hljs-type">int</span> res = <span class="hljs-number">0</span>;<br>    <span class="hljs-built_in">memset</span>(f,<span class="hljs-number">-1</span>,<span class="hljs-keyword">sizeof</span> f);<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; k;i++)&#123;<br>        <span class="hljs-type">int</span> m;cin &gt;&gt; m;<br>        res^=<span class="hljs-built_in">sg</span>(m);<br>    &#125;<br>    <br>    <span class="hljs-keyword">if</span>(res) cout &lt;&lt; <span class="hljs-string">&quot;Yes&quot;</span>;<br>    <span class="hljs-keyword">else</span> cout &lt;&lt; <span class="hljs-string">&quot;No&quot;</span>;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="拆分NIM游戏"><a href="#拆分NIM游戏" class="headerlink" title="拆分NIM游戏"></a>拆分NIM游戏</h3><p><img src="/post/48f084a8/image-20210901135552976.png" alt="image-20210901135552976"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">sg</span><span class="hljs-params">(<span class="hljs-type">int</span> x)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(f[x]!=<span class="hljs-number">-1</span>)<span class="hljs-keyword">return</span> f[x];<br>    set&lt;<span class="hljs-type">int</span>&gt; S;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; x;i++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt;= i;j++)&#123;<br>            S.<span class="hljs-built_in">insert</span>(<span class="hljs-built_in">sg</span>(i)^<span class="hljs-built_in">sg</span>(j));<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;;i++)&#123;<br>        <span class="hljs-keyword">if</span>(!S.<span class="hljs-built_in">count</span>(i)) <span class="hljs-keyword">return</span> f[x] = i;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="背包问题"><a href="#背包问题" class="headerlink" title="背包问题"></a>背包问题</h2><p><img src="/post/48f084a8/image-20210826144543958.png" alt="image-20210826144543958"></p><h3 id="01背包：每个物品只有一件"><a href="#01背包：每个物品只有一件" class="headerlink" title="01背包：每个物品只有一件"></a>01背包：每个物品只有一件</h3><p><img src="/post/48f084a8/image-20210826144432994.png" alt="image-20210826144432994"></p><h4 id="二维解法"><a href="#二维解法" class="headerlink" title="二维解法"></a>二维解法</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> f[N][N];<br><span class="hljs-type">int</span> w[N],v[N];<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt;= m;j++)&#123;<br>        <span class="hljs-comment">//放物品,如果当前的容量够放得下当前物品</span><br>        <span class="hljs-keyword">if</span>(j &gt;= v[i])&#123;<br>            f[i][j] = <span class="hljs-built_in">max</span>(f[i<span class="hljs-number">-1</span>][j],f[i<span class="hljs-number">-1</span>][j-v[i]]+w[i]);<br>        &#125;<br>    &#125;<br>&#125;<br>cout &lt;&lt; f[n][m] &lt;&lt; endl;<br></code></pre></td></tr></table></figure><h4 id="一维解法"><a href="#一维解法" class="headerlink" title="一维解法"></a>一维解法</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> f[N][N];<br><span class="hljs-type">int</span> w[N],v[N];<br><span class="hljs-comment">//降到一维之后，原有的不选第i个物品就直接随着i的增加而自动迭代到了下一层</span><br><span class="hljs-comment">//但是，如果还是从小到大进行迭代，可能f[i][j]需要f[i-1][j-v[i]]的值，但是f[i-1][j-v[i]]在前面迭代成了f[i][j-v[i]]</span><br><span class="hljs-comment">//所以有数据污染的危险，因此对j需要从大到小处理</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-comment">//这里直接处理到v[i]，即将原有的if条件等价到这里</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = m;j &gt;= v[i];j--)&#123;<br>        f[j] = <span class="hljs-built_in">max</span>(f[j],f[j-v[i]]+w[i]);<br>    &#125;<br>&#125;<br><span class="hljs-comment">//因为我们一直算的是max值，所以推到m的时候的结果一定是全局最优解</span><br>cout &lt;&lt; f[m] &lt;&lt; endl;<br></code></pre></td></tr></table></figure><h3 id="完全背包：不规定物品件数"><a href="#完全背包：不规定物品件数" class="headerlink" title="完全背包：不规定物品件数"></a>完全背包：不规定物品件数</h3><p><img src="/post/48f084a8/image-20210826150626209.png" alt="image-20210826150626209"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">/*</span><br><span class="hljs-comment">    01背包的转移方程 f[i][j] = max(f[i-1][j],f[i-1][j-v[i]]+w[i])</span><br><span class="hljs-comment">    完全背包中选择第i件物品的推导</span><br><span class="hljs-comment">    f[i][j] = max(f[i-1][j],f[i-1][j-v[i]]+w[i],f[i-1][j-2v[i]]+2w[i]+...)</span><br><span class="hljs-comment">    减一个v</span><br><span class="hljs-comment">    f[i][j-v[i]] = max(f[i-1][j-v[i]],f[i-1][j-2v[i]]+w[i],f[i-1][j-3v[i]]+2w[i]+...)</span><br><span class="hljs-comment">    所以</span><br><span class="hljs-comment">    完全背包转移方程 f[i][j] = max(f[i-1][j],f[i][j-v[i]]+w[i])</span><br><span class="hljs-comment">    </span><br><span class="hljs-comment">    所以01背包和完全背包的区别在于选择i时的更新是从i-1更新还是从i更新</span><br><span class="hljs-comment">    </span><br><span class="hljs-comment">    所以01背包从后往前遍历，防止i-1 --&gt; i</span><br><span class="hljs-comment">    而完全背包正需要将i-1 --&gt; i，所以从前向后遍历</span><br><span class="hljs-comment">    </span><br><span class="hljs-comment">*/</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-comment">//从v[i]开始</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = v[i];j &lt;= m;j++)&#123;<br>        f[j] = <span class="hljs-built_in">max</span>(f[j],f[j-v[i]]+w[i]);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>两个代码其实只有一句不同（注意下标）</p><p><code>f[i][j] = max(f[i][j],f[i-1][j-v[i]]+w[i]);//01背包</code></p><p><code>f[i][j] = max(f[i][j],f[i][j-v[i]]+w[i]);//完全背包问题</code></p><h3 id="多重背包：规定每个物品的件数"><a href="#多重背包：规定每个物品的件数" class="headerlink" title="多重背包：规定每个物品的件数"></a>多重背包：规定每个物品的件数</h3><p><img src="/post/48f084a8/image-20210826150820966.png" alt="image-20210826150820966"></p><p>在01背包的基础上加入一套循环，每次加入k件物品，进行判断（O(N3))</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-type">int</span> v,w,s;<br>    cin &gt;&gt; v &gt;&gt; w &gt;&gt; s;<br>    <span class="hljs-comment">//注意这里需要从大到小进行处理</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = m;j &gt;= v;j--)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> k = <span class="hljs-number">1</span>;k &lt;= s;k++)&#123;<br>            <span class="hljs-keyword">if</span>(j &gt;= k*v) f[j] = <span class="hljs-built_in">max</span>(f[j],f[j-k*v]+k*w);<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>优化：<strong>使用快速幂的思路，重新创造一个物品列表</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Good</span>&#123;<br>    <span class="hljs-type">int</span> v,w;<br>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> n,m;<br>    cin &gt;&gt; n &gt;&gt; m;<br>    vector&lt;Good&gt; g;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-type">int</span> v,w,s;<br>        cin &gt;&gt; v &gt;&gt; w &gt;&gt; s;<br>        <span class="hljs-comment">//注意每次对g进行清理</span><br>        g.<span class="hljs-built_in">clear</span>();<br>        <span class="hljs-comment">//重新创建一个序列，按照二进制的规则存储</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;s &gt;= j;j*=<span class="hljs-number">2</span>)&#123;<br>            g.<span class="hljs-built_in">push_back</span>(&#123;j*v,j*w&#125;);<br>            s-=j;<br>        &#125;<br>        <span class="hljs-comment">//如果s有剩余，也存进去，防止漏掉物品</span><br>        <span class="hljs-keyword">if</span>(s)g.<span class="hljs-built_in">push_back</span>(&#123;s*v,s*w&#125;);<br>        <span class="hljs-comment">//对当前的新物品列表进行01背包</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> a:g)&#123;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = m;j &gt;= a.v;j--)&#123;<br>                f[j] = <span class="hljs-built_in">max</span>(f[j],f[j-a.v]+a.w);<br>            &#125;<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; f[m] &lt;&lt; endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="分组背包：每个组内01背包"><a href="#分组背包：每个组内01背包" class="headerlink" title="分组背包：每个组内01背包"></a>分组背包：每个组内01背包</h3><p><img src="/post/48f084a8/image-20210826160014477.png" alt="image-20210826160014477"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;=n;i++)&#123;<br>    cin&gt;&gt;s[i];<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j=<span class="hljs-number">0</span>;j&lt;s[i];j++)<br>        cin&gt;&gt;v[i][j]&gt;&gt;w[i][j];<br>&#125;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;=n;i++)&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j=m;j&gt;=<span class="hljs-number">0</span>;j--)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> k=<span class="hljs-number">0</span>;k&lt;s[i];k++)<br>            <span class="hljs-keyword">if</span>(v[i][k]&lt;=j) f[j]=<span class="hljs-built_in">max</span>(f[j],f[j-v[i][k]]+w[i][k]);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="线性DP问题"><a href="#线性DP问题" class="headerlink" title="线性DP问题"></a>线性DP问题</h2><p>线性DP问题一般都涉及到位置计算。</p><h3 id="数字三角形"><a href="#数字三角形" class="headerlink" title="数字三角形"></a>数字三角形</h3><p><img src="/post/48f084a8/image-20210826165855069.png" alt="image-20210826165855069"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//从下向上可以解决分支问题</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = n<span class="hljs-number">-1</span>;i &gt; <span class="hljs-number">0</span>;i--)&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= i;j++)&#123;<br>        f[i][j] = <span class="hljs-built_in">max</span>(f[i+<span class="hljs-number">1</span>][j],f[i+<span class="hljs-number">1</span>][j+<span class="hljs-number">1</span>])+f[i][j];<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="最长上升子序列：使用二分进行优化"><a href="#最长上升子序列：使用二分进行优化" class="headerlink" title="最长上升子序列：使用二分进行优化"></a>最长上升子序列：使用二分进行优化</h3><img src="/post/image-20210826172152769.png" alt="image-20210826172152769" style="zoom:50%;"><p>原理：</p><p>题解中最难理解的地方在于栈中序列虽然递增，但是每个元素在原串中对应的位置其实可能是乱的，那为什么这个栈还能用于计算最长子序列长度？<br>实际上这个栈【不用于记录最终的最长子序列】，<strong>而是【以stk[i]结尾的子串长度最长为i】</strong>或者说<strong>【长度为i的递增子串中，末尾元素最小的是stk[i]】</strong>。理解了这个问题以后就知道为什么新进来的元素要不就在末尾增加，要不就替代第一个大于等于它元素的位置。<br>这里的【替换】就蕴含了一个贪心的思想，对于同样长度的子串，我当然希望它的末端越小越好，这样以后我也有更多机会拓展。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-type">int</span> f[N],n;<br><span class="hljs-type">int</span> g[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)cin &gt;&gt; f[i];<br>    <span class="hljs-comment">//替换思想体现了贪心，我们希望序列每个位置的数都更小</span><br>    g[<span class="hljs-number">1</span>] = f[<span class="hljs-number">1</span>];<br>    <span class="hljs-type">int</span> t = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">2</span>;i &lt;= n;i++)&#123;<br>        <span class="hljs-comment">//如果当前序列的最大值小于f[n]，则加入数组</span><br>        <span class="hljs-keyword">if</span>(g[t] &lt; f[i])&#123;<br>            g[t+<span class="hljs-number">1</span>] = f[i];<br>            t++;<br>        &#125;<br>        <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-comment">//找到第一个大于等于f[n]的值</span><br>            <span class="hljs-type">int</span> l = <span class="hljs-number">1</span>,r = t;<br>            <span class="hljs-keyword">while</span>(l &lt; r)&#123;<br>                <span class="hljs-type">int</span> m = (l+r)/<span class="hljs-number">2</span>;<br>                <span class="hljs-keyword">if</span>(g[m] &gt;= f[i]) r = m;<br>                <span class="hljs-keyword">else</span> l = m+<span class="hljs-number">1</span>;<br>            &#125;<br>            g[l] = f[i];<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; t &lt;&lt; endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="最长公共子序列"><a href="#最长公共子序列" class="headerlink" title="最长公共子序列"></a>最长公共子序列</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">/*</span><br><span class="hljs-comment">    分析</span><br><span class="hljs-comment">    状态表示 f[i][j]</span><br><span class="hljs-comment">            定义：表示a的前i个字符和b的前j个字符所组成的所有子序列</span><br><span class="hljs-comment">            属性：长度最大值</span><br><span class="hljs-comment">    状态计算：看成集合</span><br><span class="hljs-comment">            这个状态下产生不同的原因是是否有a[i]或者b[j]</span><br><span class="hljs-comment">            f[i][j]的计算针对于a[i],b[j]可以由以下几个部分组成：</span><br><span class="hljs-comment">                1、有a[i]和b[j]     if(a[i] == b[j]) f[i][j] = f[i-1][j-1]+1;</span><br><span class="hljs-comment">                2、没有a[i]或b[j]    f[i][j] = f[i-1][j-1]</span><br><span class="hljs-comment">                3、有a[i]没b[j]   f[i][j-1]表示的是a的前i个字符和b的前j-1个字符的子序列最大值，虽然不一定包含b[j],但包含了当前情况。而且f[i][j-1]是不超过集合f[i][j]的，即f[i][j-1]集合的最大值包含当前情况且没有出界。所以可以用f[i][j-1]代替本情况</span><br><span class="hljs-comment">                4、有b[i]没a[i]   同上,用f[i-1][j]代替本情况</span><br><span class="hljs-comment">                </span><br><span class="hljs-comment">*/</span> <br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= m;j++)&#123;<br>        <span class="hljs-comment">//分情况讨论</span><br>        <span class="hljs-keyword">if</span>(a[i]==b[j]) f[i][j] = f[i<span class="hljs-number">-1</span>][j<span class="hljs-number">-1</span>]+<span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">else</span>&#123;<br>            f[i][j] = <span class="hljs-built_in">max</span>(f[i<span class="hljs-number">-1</span>][j],f[i][j<span class="hljs-number">-1</span>]);<br>        &#125;<br>    &#125;<br>&#125;<br>cout &lt;&lt; f[n][m] &lt;&lt; endl;<br></code></pre></td></tr></table></figure><h3 id="最短编辑距离"><a href="#最短编辑距离" class="headerlink" title="最短编辑距离"></a>最短编辑距离</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">/*</span><br><span class="hljs-comment">    1、状态表示：f[i][j]</span><br><span class="hljs-comment">        定义：从a[1..i]到b[1..j]的操作</span><br><span class="hljs-comment">        属性：求操作次数最小值</span><br><span class="hljs-comment">    2、状态计算：（从最后的操作出发）</span><br><span class="hljs-comment">        对于一个f[i][j]，求解的情况有三种（注意这里，对a的删除理论上和对b的增加一致，所以只考虑对a的操作）</span><br><span class="hljs-comment">        a.删除   f[i][j] = f[i-1][j]+1 （从a的后面删除一个字符）</span><br><span class="hljs-comment">        b.增加   f[i][j] = f[i][j-1]+1 （在a的后面增加一个字符）</span><br><span class="hljs-comment">            最后一步是增加：若最后一步是增加，为了使a[1 ~ i] = b[1 ~ j]那最后一步增加的一定是b[j]。</span><br><span class="hljs-comment">            那么f[i][j]就可以看成是，首先将a[1 ~ i]转化成b[1 ~ j - 1]，这时候所操作的步骤数为f[i][j - 1]，下一步在b[j - 1]后面添加一个b[j]，那么f[i][j] = f[i][j - 1] + 1。</span><br><span class="hljs-comment">            也就是说，我们的转化过程为：a[1 ~ i] -&gt; b[1 ~ j - 1] -&gt; b[1 ~ j]，</span><br><span class="hljs-comment">            f[i][j]的意义是将a[1 ~ i]转化为b[1 ~ j]的步骤数，不能理解为直接在a[i]后面添加一个数，这是一个动态的变化的过程。</span><br><span class="hljs-comment">        c.修改   f[i][j] = if(a[i] == b[j])f[i-1][j-1]（不用改） else f[i-1][j-1]+1（加上修改操作）</span><br><span class="hljs-comment">*/</span><br><br><span class="hljs-comment">//边界条件</span><br><span class="hljs-comment">//判断一下是否需要边界条件</span><br><span class="hljs-comment">//只进行增加操作</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt;= m;i++)f[<span class="hljs-number">0</span>][i] = i;<br><span class="hljs-comment">//只进行删除操作</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt;= n;i++)f[i][<span class="hljs-number">0</span>] = i;<br><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)&#123;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>;j &lt;= m;j++)&#123;<br>        <span class="hljs-comment">//四种情况</span><br>        <span class="hljs-comment">//增加与删除</span><br>        f[i][j] = <span class="hljs-built_in">min</span>(f[i<span class="hljs-number">-1</span>][j]+<span class="hljs-number">1</span>,f[i][j<span class="hljs-number">-1</span>]+<span class="hljs-number">1</span>);<br>        <span class="hljs-comment">//不动与替换</span><br>        f[i][j] = <span class="hljs-built_in">min</span>(f[i][j],f[i<span class="hljs-number">-1</span>][j<span class="hljs-number">-1</span>]+(a[i]!=b[j]));<br>    &#125;<br>&#125;<br>cout &lt;&lt; f[n][m];<br></code></pre></td></tr></table></figure><h2 id="区间DP"><a href="#区间DP" class="headerlink" title="区间DP"></a>区间DP</h2><h3 id="石子合并"><a href="#石子合并" class="headerlink" title="石子合并"></a>石子合并</h3><p><img src="/post/48f084a8/image-20210827130138656.png" alt="image-20210827130138656"></p><p><img src="/post/48f084a8/image-20210827121556326.png" alt="image-20210827121556326"></p><p><img src="/post/48f084a8/image-20210827123510128.png" alt="image-20210827123510128"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">310</span>;<br><span class="hljs-comment">/*</span><br><span class="hljs-comment">    1、集合表示f[i][j]</span><br><span class="hljs-comment">        定义：从第i个石子到第j个石子的所有合并情况</span><br><span class="hljs-comment">        属性：代价最小值</span><br><span class="hljs-comment">    2、集合计算</span><br><span class="hljs-comment">        对于一个从i到j的集合，可以从[i,i],[i+1,j] / [i,i+1],[i+2,j] / ... / [i,k],[k+1,j] / ... / [i,j-1],[j,j]这样来枚举</span><br><span class="hljs-comment">        即对每一个可能的合并情况进行处理</span><br><span class="hljs-comment">*/</span><br><br><span class="hljs-type">int</span> n;<br><span class="hljs-type">int</span> s[N],a[N],f[N][N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)cin &gt;&gt; a[i];<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)s[i] = a[i]+s[i<span class="hljs-number">-1</span>];<br><br>    <span class="hljs-comment">//长度为1时不需要合并，为0</span><br>    <span class="hljs-comment">// for(int i = 1;i &lt;= n;i++)f[i][i] = a[i];</span><br>    <span class="hljs-comment">//区间dp的第一步一般为对区间长度进行枚举</span><br>    <span class="hljs-comment">//O(n^3)的时间复杂度</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> len = <span class="hljs-number">2</span>;len &lt;= n;len++)&#123;<span class="hljs-comment">//区间枚举</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i + len<span class="hljs-number">-1</span> &lt;= n;i++)&#123;<span class="hljs-comment">//起点迭代</span><br>            <span class="hljs-type">int</span> j = i+len<span class="hljs-number">-1</span>;<span class="hljs-comment">//计算终点</span><br>            f[i][j] = <span class="hljs-number">1e7</span>;<br>            <span class="hljs-comment">//转移方程</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> k = i;k &lt;= j<span class="hljs-number">-1</span>;k++)&#123;<span class="hljs-comment">//状态转移</span><br>                f[i][j] = <span class="hljs-built_in">min</span>(f[i][j],f[i][k]+f[k+<span class="hljs-number">1</span>][j]);<br>            &#125;<br>            f[i][j] += s[j]-s[i<span class="hljs-number">-1</span>];<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; f[<span class="hljs-number">1</span>][n];<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="计数DP"><a href="#计数DP" class="headerlink" title="计数DP"></a>计数DP</h2><p><img src="/post/48f084a8/image-20210827130855450.png" alt="image-20210827130855450"></p><img src="/post/image-20210827150150578.png" alt="image-20210827150150578" style="zoom: 50%;"><p><img src="/post/48f084a8/image-20210827150207137.png" alt="image-20210827150207137"></p><h2 id="数位统计DP"><a href="#数位统计DP" class="headerlink" title="数位统计DP"></a>数位统计DP</h2><h3 id="计数问题"><a href="#计数问题" class="headerlink" title="计数问题"></a>计数问题</h3><h2 id="状态压缩DP"><a href="#状态压缩DP" class="headerlink" title="状态压缩DP"></a>状态压缩DP</h2><h3 id="蒙特卡罗的梦想"><a href="#蒙特卡罗的梦想" class="headerlink" title="蒙特卡罗的梦想"></a>蒙特卡罗的梦想</h3><p><img src="/post/48f084a8/image-20210827151843247.png" alt="image-20210827151843247"></p><p><img src="/post/48f084a8/image-20210827164455255.png" alt="image-20210827164455255"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;bits/stdc++.h&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N=<span class="hljs-number">12</span>, M = <span class="hljs-number">1</span>&lt;&lt; N;  <br><span class="hljs-type">long</span> <span class="hljs-type">long</span> f[N][M] ;<span class="hljs-comment">// 第一维表示列， 第二维表示所有可能的状态</span><br><span class="hljs-type">bool</span> st[M];  <span class="hljs-comment">//存储每种状态是否有奇数个连续的0，如果奇数个0是无效状态，如果是偶数个零置为true。</span><br><span class="hljs-comment">//vector&lt;int &gt; state[M];  //二维数组记录合法的状态</span><br>vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; <span class="hljs-built_in">state</span>(M);  <span class="hljs-comment">//两种写法等价:二维数组</span><br><span class="hljs-type">int</span> m , n;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br><br>    <span class="hljs-keyword">while</span>(cin&gt;&gt;n&gt;&gt;m, n||m)&#123; <span class="hljs-comment">//读入n和m，并且不是两个0即合法输入就继续读入</span><br>        <span class="hljs-comment">//第一部分：预处理1</span><br>        <span class="hljs-comment">//对于每种状态，先预处理每列不能有奇数个连续的0</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt; <span class="hljs-number">1</span>&lt;&lt;n; i++)&#123;<br>            <span class="hljs-type">int</span> cnt =<span class="hljs-number">0</span> ;<span class="hljs-comment">//记录连续的0的个数</span><br>            <span class="hljs-type">bool</span> isValid = <span class="hljs-literal">true</span>; <span class="hljs-comment">// 某种状态没有奇数个连续的0则标记为true</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j=<span class="hljs-number">0</span>;j&lt;n;j++)&#123; <span class="hljs-comment">//遍历这一列，从上到下</span><br>                 <span class="hljs-keyword">if</span>( i&gt;&gt;j &amp;<span class="hljs-number">1</span>)&#123;  <span class="hljs-comment">//i&gt;&gt;j位运算，表示i（i在此处是一种状态）的二进制数的第j位； &amp;1为判断该位是否为1，如果为1进入if</span><br>                    <span class="hljs-keyword">if</span>(cnt &amp;<span class="hljs-number">1</span>) &#123; <span class="hljs-comment">//这一位为1，看前面连续的0的个数，如果是奇数（cnt &amp;1为真）则该状态不合法</span><br>                        isValid =<span class="hljs-literal">false</span>;<span class="hljs-keyword">break</span>;<br>                    &#125; <br>                    cnt=<span class="hljs-number">0</span>; <span class="hljs-comment">// 既然该位是1，并且前面不是奇数个0（经过上面的if判断），计数器清零。//其实清不清零没有影响</span><br>                 &#125;<br>                 <span class="hljs-keyword">else</span> cnt++; <span class="hljs-comment">//否则的话该位还是0，则统计连续0的计数器++。</span><br>            &#125;<br>            <span class="hljs-keyword">if</span>(cnt &amp;<span class="hljs-number">1</span>)  isValid =<span class="hljs-literal">false</span>; <span class="hljs-comment">//最下面的那一段判断一下连续的0的个数</span><br>            st[i]  = isValid; <span class="hljs-comment">//状态i是否有奇数个连续的0的情况,输入到数组st中</span><br>        &#125;<br><br>        <span class="hljs-comment">//第二部分：预处理2</span><br>        <span class="hljs-comment">// 经过上面每种状态 连续0的判断，已经筛掉一些状态。</span><br>        <span class="hljs-comment">//下面来看进一步的判断：看第i-2列伸出来的和第i-1列伸出去的是否冲突</span><br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j=<span class="hljs-number">0</span>;j&lt; <span class="hljs-number">1</span>&lt;&lt;n;j++)&#123; <span class="hljs-comment">//对于第i列的所有状态</span><br>            state[j].<span class="hljs-built_in">clear</span>(); <span class="hljs-comment">//清空上次操作遗留的状态，防止影响本次状态。</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> k=<span class="hljs-number">0</span>;k&lt; <span class="hljs-number">1</span>&lt;&lt;n;k++)&#123; <span class="hljs-comment">//对于第i-1列所有状态</span><br>                <span class="hljs-keyword">if</span>((j&amp;k )==<span class="hljs-number">0</span> &amp;&amp; st[ j| k] ) <span class="hljs-comment">// 第i-2列伸出来的 和第i-1列伸出来的不冲突(不在同一行) </span><br>                <span class="hljs-comment">//解释一下st[j | k] </span><br>                <span class="hljs-comment">//已经知道st[]数组表示的是这一列没有连续奇数个0的情况，</span><br>                <span class="hljs-comment">//我们要考虑的是第i-1列（第i-1列是这里的主体）中从第i-2列横插过来的，还要考虑自己这一列（i-1列）横插到第i列的</span><br>                <span class="hljs-comment">//比如 第i-2列插过来的是k=10101，第i-1列插出去到第i列的是 j =01000，</span><br>                <span class="hljs-comment">//那么合在第i-1列，到底有多少个1呢？自然想到的就是这两个操作共同的结果：两个状态或。 j | k = 01000 | 10101 = 11101</span><br>                <span class="hljs-comment">//这个 j|k 就是当前 第i-1列的到底有几个1，即哪几行是横着放格子的</span><br>                    state[j].<span class="hljs-built_in">push_back</span>(k);  <span class="hljs-comment">//二维数组state[j]表示第j行， </span><br>                    <span class="hljs-comment">//j表示 第i列“真正”可行的状态，如果第i-1列的状态k和j不冲突则压入state数组中的第j行。</span><br>                    <span class="hljs-comment">//“真正”可行是指：既没有前后两列伸进伸出的冲突；又没有连续奇数个0。</span><br>            &#125;<br><br>        &#125;<br><br>        <span class="hljs-comment">//第三部分：dp开始</span><br>        <span class="hljs-built_in">memset</span>(f,<span class="hljs-number">0</span>,<span class="hljs-keyword">sizeof</span> f);  <span class="hljs-comment">//全部初始化为0，因为是连续读入，这里是一个清空操作。类似上面的state[j].clear()</span><br>        f[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]=<span class="hljs-number">1</span> ;<span class="hljs-comment">// 这里需要回忆状态表示的定义，按定义这里是：前第-1列都摆好，且从-1列到第0列伸出来的状态为0的方案数。</span><br>        <span class="hljs-comment">//首先，这里没有-1列，最少也是0列。其次，没有伸出来，即没有横着摆的。即这里第0列只有竖着摆这1种状态。</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>;i&lt;= m;i++)&#123; <span class="hljs-comment">//遍历每一列:第i列合法范围是(0~m-1列)</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j=<span class="hljs-number">0</span>; j&lt; <span class="hljs-number">1</span>&lt;&lt;n; j++)&#123;  <span class="hljs-comment">//遍历当前列（第i列）所有状态j</span><br>                <span class="hljs-keyword">for</span>( <span class="hljs-keyword">auto</span> k : state[j])    <span class="hljs-comment">// 遍历第i-1列的状态k，如果“真正”可行，就转移</span><br>                    f[i][j] += f[i<span class="hljs-number">-1</span>][k];    <span class="hljs-comment">// 当前列的方案数就等于之前的第i-1列所有状态k的累加。</span><br>            &#125;<br>        &#125;<br>        <span class="hljs-comment">//最后答案是什么呢？</span><br>        <span class="hljs-comment">//f[m][0]表示 前m-1列都处理完，并且第m-1列没有伸出来的所有方案数。</span><br>        <span class="hljs-comment">//即整个棋盘处理完的方案数</span><br>        cout&lt;&lt; f[m][<span class="hljs-number">0</span>]&lt;&lt;endl;<br><br>    &#125;<br>&#125;  <br></code></pre></td></tr></table></figure><h3 id="最短哈密顿路径（旅行商问题）"><a href="#最短哈密顿路径（旅行商问题）" class="headerlink" title="最短哈密顿路径（旅行商问题）"></a>最短哈密顿路径（旅行商问题）</h3><p><img src="/post/48f084a8/image-20210828111300203.png" alt="image-20210828111300203"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">/*</span><br><span class="hljs-comment">    分析：</span><br><span class="hljs-comment">    状态表示：f[i][j]</span><br><span class="hljs-comment">        定义：在完成i的状态路径之后（i表示走过的点）到达j点的所有路径集合</span><br><span class="hljs-comment">        计算：权重最小值</span><br><span class="hljs-comment">    状态计算：</span><br><span class="hljs-comment">        因为到了j点，所以j点是确定的，不确定的是从哪个点到达的k点</span><br><span class="hljs-comment">        所以转移方程是</span><br><span class="hljs-comment">        f[state][j] =  min(f[state][j],f[state ^ 1&lt;&lt;j][k] + w[k][j])</span><br><span class="hljs-comment">        即我们只需要注意走过哪些点、走到哪个点，共两种情况</span><br><span class="hljs-comment">        因为如果走的是相同的点，所形成的所有路径中一定有一个最小值，而且其他路径都可以用其代替</span><br><span class="hljs-comment">        注意：初始化是从第0个点开始，state用二进制表示当前所在的点，所以f[1][0]=0;代表从0开始</span><br><span class="hljs-comment">*/</span><br><br><br></code></pre></td></tr></table></figure><p><img src="/post/48f084a8/image-20210828104558147.png" alt="image-20210828104558147"></p><p><img src="/image-20210915004217354.png" alt="image-20210915004217354"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//DP</span><br><span class="hljs-comment">//初始化每个点都没有连</span><br><span class="hljs-built_in">memset</span>(g,<span class="hljs-number">0x3f</span>,<span class="hljs-keyword">sizeof</span> g);<br><span class="hljs-comment">//从1点开始</span><br>g[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>;<br><br><span class="hljs-comment">//确定当前状态，即哪些点被用过</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; <span class="hljs-number">1</span>&lt;&lt;n;i++)&#123;<br>    <span class="hljs-comment">//当前停在了哪个点上</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j &lt; n;j++)&#123;<span class="hljs-comment">//确定了所用点和所停点，即可确定序列</span><br>        <span class="hljs-comment">//如果当前状态包含j，即j有效</span><br>        <span class="hljs-keyword">if</span>(i&gt;&gt;j &amp; <span class="hljs-number">1</span>)&#123;<br>            <span class="hljs-comment">//j确定，迭代之前一个的k状态，判断更新最小距离</span><br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> k = <span class="hljs-number">0</span>;k &lt; n;k++)&#123;<br>                <span class="hljs-comment">//如果k状态也包含</span><br>                <span class="hljs-keyword">if</span>(i&gt;&gt;k &amp; <span class="hljs-number">1</span>)&#123;<br>                    <span class="hljs-comment">//更新最小值g[i-(1&lt;&lt;j)][k]，将j去除之后的k结果</span><br>                    g[i][j] = <span class="hljs-built_in">min</span>(g[i][j],g[i-(<span class="hljs-number">1</span>&lt;&lt;j)][k]+f[k][j]);<br>                &#125;<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-comment">//输出</span><br>cout &lt;&lt; g[(<span class="hljs-number">1</span>&lt;&lt;n)<span class="hljs-number">-1</span>][n<span class="hljs-number">-1</span>];<br></code></pre></td></tr></table></figure><h2 id="树形DP"><a href="#树形DP" class="headerlink" title="树形DP"></a>树形DP</h2><h3 id="没有上司的酒会"><a href="#没有上司的酒会" class="headerlink" title="没有上司的酒会"></a>没有上司的酒会</h3><p><img src="/post/48f084a8/image-20210828111946980.png" alt="image-20210828111946980"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">dfs</span><span class="hljs-params">(<span class="hljs-type">int</span> n)</span></span>&#123;<br>    <span class="hljs-comment">//如果选择n点，加上happy值</span><br>    f[n][<span class="hljs-number">1</span>] = happy[n];<br>    <span class="hljs-comment">//对其上司进行处理</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = h[n];i != <span class="hljs-number">-1</span>;i = ne[i])&#123;<br>        <span class="hljs-type">int</span> down = e[i];<br>        <span class="hljs-built_in">dfs</span>(down);<br>        <span class="hljs-comment">//如果不选n点，down可能选可能不选</span><br>        f[n][<span class="hljs-number">0</span>] += <span class="hljs-built_in">max</span>(f[down][<span class="hljs-number">1</span>],f[down][<span class="hljs-number">0</span>]);<br>        <span class="hljs-comment">//如果选n点，down点一定不会被选</span><br>        f[n][<span class="hljs-number">1</span>] += f[down][<span class="hljs-number">0</span>];<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-built_in">memset</span>(h,<span class="hljs-number">-1</span>,<span class="hljs-keyword">sizeof</span> h);<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= n;i++)cin &gt;&gt; happy[i];<br>    <span class="hljs-comment">//建树</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n<span class="hljs-number">-1</span>;i++)&#123;<br>        <span class="hljs-type">int</span> a,b;<br>        cin &gt;&gt; a &gt;&gt; b;<br>        <span class="hljs-built_in">add</span>(b,a);<br>        upper[a] = <span class="hljs-number">1</span>;<br>    &#125;<br>    <span class="hljs-comment">//找根节点</span><br>    <span class="hljs-type">int</span> root = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">while</span>(upper[root])root++;<br>    <span class="hljs-comment">//从根节点进行迭代</span><br>    <span class="hljs-built_in">dfs</span>(root);<br>    cout &lt;&lt; <span class="hljs-built_in">max</span>(f[root][<span class="hljs-number">0</span>],f[root][<span class="hljs-number">1</span>]) &lt;&lt; endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="记忆化搜索"><a href="#记忆化搜索" class="headerlink" title="记忆化搜索"></a>记忆化搜索</h2><p>记忆化搜索一般跟着dfs一起使用</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">dp</span><span class="hljs-params">(<span class="hljs-type">int</span> x,<span class="hljs-type">int</span> y)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(f[x][y]) <span class="hljs-keyword">return</span> f[x][y];<span class="hljs-comment">//如果已经记录过了，就不用再算了 </span><br>    f[x][y]=<span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;<span class="hljs-number">4</span>;i++)<br>    &#123;<br>        <span class="hljs-type">int</span> xx=x+dx[i];<span class="hljs-comment">//到下一个点 </span><br>        <span class="hljs-type">int</span> yy=y+dy[i];<br>        <span class="hljs-keyword">if</span>(xx&gt;=<span class="hljs-number">1</span>&amp;&amp;xx&lt;=n&amp;&amp;yy&gt;=<span class="hljs-number">1</span>&amp;&amp;yy&lt;=m&amp;&amp;h[x][y]&gt;h[xx][yy])<span class="hljs-comment">//点在范围内，且此点高度，比刚才的点矮就滑过去 </span><br>        &#123;<br>            f[x][y]=<span class="hljs-built_in">max</span>(f[x][y],<span class="hljs-built_in">dp</span>(xx,yy)+<span class="hljs-number">1</span>);<span class="hljs-comment">//比较大小，取出最大的 </span><br>        &#125;                                       <span class="hljs-comment">//+1，是因为如果到从两个点的滑雪距离一样的话，再滑一步，就滑到x，y这个点了，所以+1 </span><br>    &#125;<br>    <span class="hljs-keyword">return</span> f[x][y];<span class="hljs-comment">//返回最终值 </span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="划分数"><a href="#划分数" class="headerlink" title="划分数"></a>划分数</h2><p><img src="/image-20210914152204058.png" alt="image-20210914152204058"></p><img src="/post/image-20210914152628226.png" alt="image-20210914152628226" style="zoom: 67%;"><p><img src="/image-20210914152609245.png" alt="image-20210914152609245"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-keyword">typedef</span> <span class="hljs-type">long</span> <span class="hljs-type">long</span> LL;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">12</span>;<br>LL f[N][N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    LL a,b;<br>    <span class="hljs-keyword">while</span>(cin &gt;&gt; a &gt;&gt; b)&#123;<br><span class="hljs-comment">//         cout &lt;&lt; a &lt;&lt; &quot; &quot; &lt;&lt; b &lt;&lt;endl;</span><br>        <span class="hljs-built_in">memset</span>(f,<span class="hljs-number">0</span>,<span class="hljs-keyword">sizeof</span> f);<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= a; ++i)<br>            f[i][<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>; j &lt;= b; ++j)<br>            f[<span class="hljs-number">0</span>][j] = <span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>;i &lt;= a;i++)&#123;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">1</span>; j &lt;= b;j++)&#123;<br>                <span class="hljs-keyword">if</span>(i &gt;= j)f[i][j] = f[i][j<span class="hljs-number">-1</span>]+f[i-j][j];<br>                <span class="hljs-keyword">else</span> f[i][j] = f[i][i];<br>            &#125;<br>        &#125;<br>        cout &lt;&lt; f[a][b] &lt;&lt;endl;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="区间问题"><a href="#区间问题" class="headerlink" title="区间问题"></a>区间问题</h2><h3 id="区间选点（最大不相交区间数）"><a href="#区间选点（最大不相交区间数）" class="headerlink" title="区间选点（最大不相交区间数）"></a>区间选点（最大不相交区间数）</h3><p><img src="/post/48f084a8/image-20210828221114654.png" alt="image-20210828221114654"></p><p><strong>右端点排序。</strong>每次优先找最右端点，如果当前最右点小于下一个区间的最左点，则前面所有区间的最右边和下一区间的最左边没有相交，即多一个新的区间。·</p><p><img src="/post/48f084a8/image-20210828220535066.png" alt="image-20210828220535066"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs c++"> <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> l,r;<br>        cin &gt;&gt; l &gt;&gt; r;<br>        e[i]=&#123;l,r&#125;;<br>    &#125;<br><span class="hljs-comment">//按右端点排序</span><br>    <span class="hljs-built_in">sort</span>(e,e+n,cmp);<br>    <span class="hljs-type">int</span> r = <span class="hljs-number">-2e9</span>;<br>    <span class="hljs-type">int</span> sum = <span class="hljs-number">0</span>;<br><span class="hljs-comment">//更新端点</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        <span class="hljs-type">int</span> tl = e[i].l;<br>        <span class="hljs-keyword">if</span>(r&lt;tl)&#123;<br>            sum++;<br>            r = e[i].r;<br>        &#125;<br>    &#125;<br>    cout &lt;&lt; sum &lt;&lt; endl;<br></code></pre></td></tr></table></figure><h3 id="区间分组（教室安排问题）"><a href="#区间分组（教室安排问题）" class="headerlink" title="区间分组（教室安排问题）"></a>区间分组（教室安排问题）</h3><p><strong>左端点排序。</strong>找到最靠左的右端点，如果最靠左的max_r &lt; l，代表存在一个组可以放入这个区间。</p><p><img src="/post/48f084a8/image-20210829133130091.png" alt="image-20210829133130091"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-built_in">sort</span>(p,p+n,cmp);<br><span class="hljs-comment">//我们的小根堆始终保证所有组中的最小的右端点为根节点</span><br>priority_queue&lt;<span class="hljs-type">int</span>,vector&lt;<span class="hljs-type">int</span>&gt;,greater&lt;<span class="hljs-type">int</span>&gt;&gt; q;<br><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>    <span class="hljs-type">int</span> l = p[i].l;<br>    <span class="hljs-comment">//如果还没有分组，或者当前最小右端点还要大于区间左端点</span><br>    <span class="hljs-keyword">if</span>(q.<span class="hljs-built_in">empty</span>() || l &lt;= q.<span class="hljs-built_in">top</span>())&#123;<br>        <span class="hljs-comment">//所以需要新开一个分组</span><br>        q.<span class="hljs-built_in">push</span>(p[i].r);r++;<br>    &#125;<br>    <span class="hljs-keyword">else</span>&#123;<br>        <span class="hljs-comment">//否则存在一个组可以放入</span><br>        <span class="hljs-comment">//弹出之前的最小右端点并放入当前的右端点</span><br>        q.<span class="hljs-built_in">pop</span>();q.<span class="hljs-built_in">push</span>(p[i].r);<br>    &#125;<br>&#125;<br><br><br><br><span class="hljs-comment">//左端点排序</span><br><span class="hljs-built_in">sort</span>(p,p+n,cmp);<br>pri q;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>    <span class="hljs-comment">//如果满足条件</span><br>    <span class="hljs-keyword">if</span>(q.<span class="hljs-built_in">empty</span>()||p[i].l &lt;= q.<span class="hljs-built_in">top</span>())&#123;<br>        q.<span class="hljs-built_in">push</span>(p[i].r);<br>    &#125;<br>    <span class="hljs-keyword">else</span>&#123;<br>        q.<span class="hljs-built_in">pop</span>();q.<span class="hljs-built_in">push</span>(p[i].r);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="区间覆盖"><a href="#区间覆盖" class="headerlink" title="区间覆盖"></a>区间覆盖</h3><p><img src="/post/48f084a8/image-20210829135504450.png" alt="image-20210829135504450"></p><p><strong>左端点排序</strong>，在左端点小于s的情况下优先找右端点最大的区间。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>    <span class="hljs-comment">//注意初始化</span><br>    <span class="hljs-type">int</span> j = i,r = <span class="hljs-number">-2e9</span>;<br>    <span class="hljs-comment">//判断当前的l小于st的每个区间，找到右端最远的区间</span><br>    <span class="hljs-keyword">while</span>(j &lt; n &amp;&amp; p[j].l &lt;= s)&#123;<br>        <span class="hljs-comment">//如果小于，则将最右端更新成最远的那个点</span><br>        r = <span class="hljs-built_in">max</span>(r,p[j].r);<br>        j++;<br>    &#125;<br>    <span class="hljs-comment">//首先判断是否有断点</span><br>    <span class="hljs-keyword">if</span>(r &lt; s) <span class="hljs-keyword">break</span>;<br>    <span class="hljs-comment">//否则当前满足区域，将该区域加到res中</span><br>    res++;<br>    <span class="hljs-comment">//其次判断是否完成搜索</span><br>    <span class="hljs-keyword">if</span>(r &gt;= t)&#123;<br>        sc = <span class="hljs-literal">true</span>;<span class="hljs-keyword">break</span>;<br>    &#125;<br>    <span class="hljs-comment">//如果没完成，则将s更新，继续遍历</span><br>    s = r;<br>    <span class="hljs-comment">//注意倒退操作</span><br>    i = j<span class="hljs-number">-1</span>;<br>&#125;<br><span class="hljs-keyword">if</span>(sc) cout &lt;&lt; res;<br><span class="hljs-keyword">else</span> cout &lt;&lt; <span class="hljs-number">-1</span>;<br></code></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>区间选点问题：<strong>右排序</strong>，优先选择最右点</p><p>区间分组问题：<strong>左排序</strong>，如果最小分组的max_r都大于当前区间的l，则代表区间与所有分组有冲突，新开一组</p><p>区间覆盖问题：<strong>左排序</strong>，如果l&lt;s，则找到其符合条件区间的最右端点并更新成s，注意判断断点和结束点</p><h2 id="哈夫曼树"><a href="#哈夫曼树" class="headerlink" title="哈夫曼树"></a>哈夫曼树</h2><p><img src="/post/48f084a8/image-20210829160917625.png" alt="image-20210829160917625"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//小根堆</span><br>priority_queue&lt;<span class="hljs-type">int</span>,vector&lt;<span class="hljs-type">int</span>&gt;,greater&lt;<span class="hljs-type">int</span>&gt;&gt; q;<br><span class="hljs-keyword">while</span>(n--)&#123;<br>    <span class="hljs-type">int</span> t;<br>    cin &gt;&gt; t;<br>    q.<span class="hljs-built_in">push</span>(t);<br>&#125;<br><span class="hljs-type">int</span> r = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">while</span>(q.<span class="hljs-built_in">size</span>()&gt;<span class="hljs-number">1</span>)&#123;<br>    <span class="hljs-type">int</span> a = q.<span class="hljs-built_in">top</span>();q.<span class="hljs-built_in">pop</span>();<br>    <span class="hljs-type">int</span> b = q.<span class="hljs-built_in">top</span>();q.<span class="hljs-built_in">pop</span>();<br>    <span class="hljs-type">int</span> c = a+b;r+=c;<br>    q.<span class="hljs-built_in">push</span>(c);<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="排序不等式"><a href="#排序不等式" class="headerlink" title="排序不等式"></a>排序不等式</h2><img src="/post/image-20210829163822491.png" alt="image-20210829163822491" style="zoom: 50%;"><p><img src="/post/48f084a8/image-20210829163918358.png" alt="image-20210829163918358"></p><h2 id="绝对值不等式"><a href="#绝对值不等式" class="headerlink" title="绝对值不等式"></a>绝对值不等式</h2><h3 id="货仓选址"><a href="#货仓选址" class="headerlink" title="货仓选址"></a>货仓选址</h3><p><img src="/post/48f084a8/image-20210829164000459.png" alt="image-20210829164000459"></p><p>​<strong>优先向中间建货仓</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">int</span> n;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-type">int</span> f[N];<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin &gt;&gt; n;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)cin &gt;&gt; f[i];<br>    <span class="hljs-built_in">sort</span>(f,f+n);<br>    <span class="hljs-type">int</span> r = <span class="hljs-number">0</span>;<br>    <span class="hljs-type">int</span> m = n/<span class="hljs-number">2</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i &lt; n;i++)&#123;<br>        r += <span class="hljs-built_in">abs</span>(f[i]-f[m]);<br>    &#125;<br>    cout &lt;&lt; r;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><img src="/post/image-20210829165354551.png" alt="image-20210829165354551" style="zoom: 50%;"><h2 id="日期类"><a href="#日期类" class="headerlink" title="日期类"></a>日期类</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> months[<span class="hljs-number">2</span>][<span class="hljs-number">13</span>] = &#123;<br>        &#123;<span class="hljs-number">0</span>,<span class="hljs-number">31</span>,<span class="hljs-number">28</span>,<span class="hljs-number">31</span>,<span class="hljs-number">30</span>,<span class="hljs-number">31</span>,<span class="hljs-number">30</span>,<span class="hljs-number">31</span>,<span class="hljs-number">31</span>,<span class="hljs-number">30</span>,<span class="hljs-number">31</span>,<span class="hljs-number">30</span>,<span class="hljs-number">31</span>&#125;,<br>        &#123;<span class="hljs-number">0</span>,<span class="hljs-number">31</span>,<span class="hljs-number">29</span>,<span class="hljs-number">31</span>,<span class="hljs-number">30</span>,<span class="hljs-number">31</span>,<span class="hljs-number">30</span>,<span class="hljs-number">31</span>,<span class="hljs-number">31</span>,<span class="hljs-number">30</span>,<span class="hljs-number">31</span>,<span class="hljs-number">30</span>,<span class="hljs-number">31</span>&#125;,<br>&#125;;<br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">isLeap</span><span class="hljs-params">(<span class="hljs-type">int</span> y)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> (y%<span class="hljs-number">4</span>==<span class="hljs-number">0</span>&amp;&amp;y%<span class="hljs-number">100</span>!=<span class="hljs-number">0</span>)||y%<span class="hljs-number">400</span>==<span class="hljs-number">0</span>;<br>&#125;<br><span class="hljs-type">int</span> years[<span class="hljs-number">2</span>] = &#123;<span class="hljs-number">365</span>,<span class="hljs-number">366</span>&#125;;<br>string s[<span class="hljs-number">10</span>]=&#123;<span class="hljs-string">&quot;Friday&quot;</span>,<span class="hljs-string">&quot;Saturday&quot;</span>,<span class="hljs-string">&quot;Sunday&quot;</span>,<span class="hljs-string">&quot;Monday&quot;</span>,<span class="hljs-string">&quot;Tuesday&quot;</span>,<span class="hljs-string">&quot;Wednesday&quot;</span>,<span class="hljs-string">&quot;Thursday&quot;</span>&#125;;<br><br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>保研经验贴</title>
      <link href="/post/95c4b274.html"/>
      <url>/post/95c4b274.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h6 id="今天的分享原则：尽量干货，介绍经验，不保证客观但是我自己想说的"><a href="#今天的分享原则：尽量干货，介绍经验，不保证客观但是我自己想说的" class="headerlink" title="今天的分享原则：尽量干货，介绍经验，不保证客观但是我自己想说的"></a>今天的分享原则：尽量干货，介绍经验，不保证客观但是我自己想说的</h6><h2 id="个人信息"><a href="#个人信息" class="headerlink" title="个人信息"></a>个人信息</h2><p><strong>姓名：</strong>姚文进</p><p><strong>班级：</strong>软件工程1805班</p><p><strong>保研院校：</strong>北京大学软件与微电子学院</p><p><strong>保研专业：</strong>电子信息专硕-软件工程方向</p><p><strong>研究方向：</strong>大致为自然语言处理与程序语言理解</p><h2 id="某些定义"><a href="#某些定义" class="headerlink" title="某些定义"></a>某些定义</h2><p>摘自百度百科：</p><p>保研（全称：推荐优秀应届本科毕业生免试攻读硕士学位研究生），顾名思义，就是被保送者不经过笔试等初试一些程序，通过一个考评形式鉴定学生的学习成绩、综合素质等，在一个允许的范围内（拥有保研资格），直接由学校保送读研究生。</p><p><strong>简而言之：保研就是各个高校为了招生所设置的面试考评。</strong></p><p><strong>保研只会给保研资格，保内保外是自己的选择。</strong></p><p>学硕、专硕、直博</p><h2 id="保研流程"><a href="#保研流程" class="headerlink" title="保研流程"></a>保研流程</h2><table><thead><tr><th></th><th>准备阶段</th><th>夏令营阶段</th><th>预推免阶段</th><th>九推（填报系统）</th></tr></thead><tbody><tr><td>时间</td><td>从现在开始-6月中旬；8月份（其实是持续输入）</td><td>6月中旬-8月上旬</td><td>8月下旬-9月28日前</td><td>9月28日当天已经之后的两天左右</td></tr><tr><td>竞争难度</td><td>无</td><td>很大，超级大</td><td>正常</td><td>正常</td></tr><tr><td>作用</td><td>准备面试的技能点，预先了解各个院校与实验室</td><td>show学校的办学水平与现有导师与实验室 <strong>保研面试</strong></td><td><strong>保研面试</strong></td><td><strong>填报推免系统，</strong>流程上接收保研学生；部分院校在前期未招满或被鸽穿之后会补录（凭运气）</td></tr><tr><td>开放院校</td><td>无</td><td>60%-70%左右的院校</td><td>80%-90%左右的高校，其中没举办的基本上是夏令营招满了</td><td>100%高校，其中补录高校大约会在50%，院校大致排名越高补录概率越低</td></tr></tbody></table><h2 id="备考流程"><a href="#备考流程" class="headerlink" title="备考流程"></a>备考流程</h2><h3 id="成绩准备"><a href="#成绩准备" class="headerlink" title="成绩准备"></a>成绩准备</h3><h4 id="推免成绩"><a href="#推免成绩" class="headerlink" title="推免成绩"></a>推免成绩</h4><p>前五学期成绩（夏令营要求）：3&#x2F;193（约1.5%）</p><p>前六学期成绩（预推免要求）：3&#x2F;193（约1.5%)</p><p>推免成绩：有名额就是胜利</p><p>英语成绩：四级530&#x2F;710，六级490+&#x2F;710</p><h4 id="对成绩的一些tips"><a href="#对成绩的一些tips" class="headerlink" title="对成绩的一些tips"></a>对成绩的一些tips</h4><ul><li>在软件，<strong>据我个人经验</strong>，保研时一般按照前1%，1%-5%，5%+大致分档，其中1%报营基本会过，1%-5%概率性不过，5%+概率性过。</li><li>英语分数重要，<strong>但属于锦上添花</strong>，英语好会给老师一个很好的印象</li></ul><h3 id="文书准备"><a href="#文书准备" class="headerlink" title="文书准备"></a>文书准备</h3><h4 id="绝对必要的支撑材料"><a href="#绝对必要的支撑材料" class="headerlink" title="绝对必要的支撑材料"></a>绝对必要的支撑材料</h4><p><strong>个人简历、个人陈述、成绩单、成绩排名证明（由教务部分开具）、获奖证书、身份信息（身份证学生证）</strong></p><h4 id="见机行事的文书"><a href="#见机行事的文书" class="headerlink" title="见机行事的文书"></a>见机行事的文书</h4><p>套磁信、导师推荐信（一般为两封，副教授及以上）、各个高校要求的报名表（可能需要盖章）</p><h3 id="机试准备"><a href="#机试准备" class="headerlink" title="机试准备"></a>机试准备</h3><p>注：由于疫情影响，近两年各高校组织保研考核中，机试比例在不断降低，但仍然有如清华大学计算机系、上海交通大学计算机系等院校大概率如期举办机试。</p><p>再注：我的保研流程在机试上吃了很大一亏，我参营的考核有一半都是因为机试不好而被刷，所以虽然机试考核院校不多，但该准备还是得准备</p><h5 id="由于我的机试不太好（一塌糊涂），所以单纯是推荐学习路线（参考了我的学长学姐谢谢你们），不做路线好坏的评价"><a href="#由于我的机试不太好（一塌糊涂），所以单纯是推荐学习路线（参考了我的学长学姐谢谢你们），不做路线好坏的评价" class="headerlink" title="由于我的机试不太好（一塌糊涂），所以单纯是推荐学习路线（参考了我的学长学姐谢谢你们），不做路线好坏的评价"></a>由于我的机试不太好（一塌糊涂），所以单纯是推荐学习路线（参考了我的学长学姐谢谢你们），不做路线好坏的评价</h5><h4 id="准备流程"><a href="#准备流程" class="headerlink" title="准备流程"></a>准备流程</h4><table><thead><tr><th></th><th>算法不太好的同学（超级小白）建议从这里开始</th><th>算法有底子但很久没动的同学建议从这儿开始</th><th>算法很厉害的同学</th><th>其他资料</th></tr></thead><tbody><tr><td>推荐路线</td><td>PTA实验辅助平台+《算法笔记》</td><td>LeetCode 热题 HOT100+<strong>Acwing在线评测平台（付费）</strong></td><td>LeetCode 新题难题+北大ACM-POJ试题+其余ACM赛事题目</td><td>哔哩哔哩、牛客网、CSDN、知乎（分享你刚编的故事）、简书、其他个人博客，有需要则查</td></tr><tr><td>推荐原因</td><td>简单易上手，<strong>夯实基础</strong>，<strong>增强信心</strong>（😀）</td><td>涉及面广，包括近年来的常考题型，讲述有针对性</td><td>题目新颖，难度较高，准备在机试中做出难题向高分冲刺</td><td></td></tr><tr><td>难度</td><td>简单</td><td>中等偏难</td><td>难，很难</td><td></td></tr><tr><td>学习内容</td><td>回顾经典数据结构，回顾经典算法</td><td>深入理解经典数据结构与算法，初步了解机试中的部分常考算法</td><td>算法竞赛中的部分高效解算法，数论图论等数学难题</td><td></td></tr></tbody></table><p><strong>有机试的院校一般机试都占很高比例（理论在10%-30%，高的在50%，但面试时老师有你的机试分，一看便知），复习时需要有针对性的参考院校考核偏向，如：去年中山大学更趋向考察数据结构使用、上交大软件更趋向考察系统逻辑、清华大学更趋向考察算法理解深度</strong>、</p><h3 id="笔试准备"><a href="#笔试准备" class="headerlink" title="笔试准备"></a>笔试准备</h3><p>注：本身有笔试的招生院校就不算多，由于疫情影响，笔试比例还在不断降低，所以现有保留笔试的院校不算太多，去年的话有南京大学人工智能学院、南京大学计算机科学与技术系、哈尔滨工业大学（深圳校区）计算机科学与技术系，中国科学院软件技术研究所等招生单位有笔试。</p><table><thead><tr><th></th><th>数学（高数、线代、概率论）</th><th>408（数据结构与算法，操作系统，计算机网络，计算机组成原理）</th><th>程序语言基础</th><th>离散数学、数据库、编译原理等其他科目</th><th>深度学习</th></tr></thead><tbody><tr><td>推荐材料</td><td>概念理解：看高数书，或者哔哩哔哩大法好<br>重点：线代的特征工程、矩阵与秩等，概率论中的定理等<br>深入做题：李永乐必刷N题，参考考研初试材料</td><td>王道考研四本书（其实软件相关是前三本书）或其他408考研指导书<br>其中数据结构与算法可以和机试补充学习</td><td>之前课内的笔记</td><td>之前课内的笔记</td><td>github中的知识梳理与技术点</td></tr><tr><td>推荐原因</td><td>数学只能一点点夯实，没办法，如果必须想要速成，则google一些面试要求的概念</td><td>重点在于概念的理解，了解范例，活学活用</td><td>理解一下<strong>基础</strong>二字</td><td>复习以前看过的会效率高一些</td><td>部分高校会稍微考一些深度学习，有时间的话可以自行学习梳理</td></tr></tbody></table><h3 id="面试准备"><a href="#面试准备" class="headerlink" title="面试准备"></a>面试准备</h3><h4 id="自我介绍"><a href="#自我介绍" class="headerlink" title="自我介绍"></a>自我介绍</h4><p>包括简历中的生源、成绩、奖项、科研经历等，一般需要准备1分钟、3分钟、5分钟等版本，英文自我介绍也是。</p><p>部分高校要求自我介绍时附带个人简介ppt，这里的ppt宜简约典雅而不宜像在课设pre里面的那种花里胡哨（我吃了这个亏—,—）。</p><h4 id="简历面"><a href="#简历面" class="headerlink" title="简历面"></a>简历面</h4><table><thead><tr><th></th><th>科研经历（必要）</th><th>比赛经历</th><th>项目经历</th><th>基础知识</th><th>英语</th></tr></thead><tbody><tr><td>面试重点</td><td>1、科研中的技术点；<br>2、科研中的idea和难题解决<br>3、对自己科研领域的了解程度<br>4、其他与面试实验室相联系的部分</td><td>1、比赛经历中的技术部分<br>2、比赛经历中的idea部分</td><td>1、比赛经历中的技术部分<br>2、比赛经历中的idea部分</td><td>见”笔试“</td><td>简单问答+结合简历科研项目问答</td></tr></tbody></table><h4 id="其他考核形式"><a href="#其他考核形式" class="headerlink" title="其他考核形式"></a>其他考核形式</h4><p>心理考核、英语单项考核、<strong>手撕代码、文献阅读理解翻译</strong>等</p><h2 id="院校填报"><a href="#院校填报" class="headerlink" title="院校填报"></a>院校填报</h2><p>保研的一个最大的优势是可以填报多个院校与多个专业，因此可以的话多利用这点</p><h3 id="报考建议"><a href="#报考建议" class="headerlink" title="报考建议"></a>报考建议</h3><h4 id="确定报考导向"><a href="#确定报考导向" class="headerlink" title="确定报考导向"></a>确定报考导向</h4><p>1、是否坚定科研（决定是否报考直博，是否坚定学硕）</p><p>2、是否就业导向（决定是否选择专硕）</p><h4 id="学校与实验室选择"><a href="#学校与实验室选择" class="headerlink" title="学校与实验室选择"></a>学校与实验室选择</h4><h5 id="遵循：多层次、一个层次中多个选择、选择中划出重点"><a href="#遵循：多层次、一个层次中多个选择、选择中划出重点" class="headerlink" title="遵循：多层次、一个层次中多个选择、选择中划出重点"></a>遵循：多层次、一个层次中多个选择、选择中划出重点</h5><p>例如：清华、北大、上交；南大、中科大、华科；中南、中山。</p><h4 id="导师选择（又名套磁）"><a href="#导师选择（又名套磁）" class="headerlink" title="导师选择（又名套磁）"></a>导师选择（又名套磁）</h4><p><strong>保研本质上是双选过程，</strong>套磁的过程也给了选择导师的机会，让老师能够通过单独面试来了解你的机会。</p><h5 id="套磁需要考虑：人品，科研方向，科研能力，院校等"><a href="#套磁需要考虑：人品，科研方向，科研能力，院校等" class="headerlink" title="套磁需要考虑：人品，科研方向，科研能力，院校等"></a>套磁需要考虑：人品，科研方向，科研能力，院校等</h5><h5 id="套磁过程中可能会问到的问题："><a href="#套磁过程中可能会问到的问题：" class="headerlink" title="套磁过程中可能会问到的问题："></a>套磁过程中可能会问到的问题：</h5><ul><li>对方向的了解程度</li><li>对实验室的了解程度，是否掌握实验室所需的部分技术</li><li>是否单个人完成一些项目或科研工作，有独立解决问题的能力</li><li>个人的性格，家境等</li></ul><h2 id="个人建议"><a href="#个人建议" class="headerlink" title="个人建议"></a>个人建议</h2><h3 id="针对于保研阶段的建议"><a href="#针对于保研阶段的建议" class="headerlink" title="针对于保研阶段的建议"></a>针对于保研阶段的建议</h3><ul><li><h4 id="保研之前，请问自己一个问题：我读研的目的是什么"><a href="#保研之前，请问自己一个问题：我读研的目的是什么" class="headerlink" title="保研之前，请问自己一个问题：我读研的目的是什么"></a><strong>保研之前，请问自己一个问题：我读研的目的是什么</strong></h4></li></ul><p><strong>这个问题十分重要，</strong>我认为重要性高于你之后的简历准备和报考。不同的读研目的对应着不同的报考风格，也对应着你是选择学硕专硕还是直博。不只有保研一条出路，多问问自己适合什么，而不是百度建议干什么。<strong>认清自己更重要！！！</strong></p><ul><li><h5 id="保研优先级（个人理解）"><a href="#保研优先级（个人理解）" class="headerlink" title="保研优先级（个人理解）"></a>保研优先级（个人理解）</h5></li></ul><p>rk（CET&#x2F;GPA等）&gt; 科研（有产出）&gt; 科研（无产出）&gt;&#x3D; ACM数模等硬通货竞赛 &gt;&gt;&gt;&gt; 创新创业类竞赛 &gt; 其他</p><p>排名是报考硬通货，英语在我理解中是锦上添花，科研经历有必要，不然老师去哪儿了解你的科研潜力</p><p>保研确实会看成绩，但我认为决定性最大的还是个人实力。</p><ul><li><h5 id="知识优先级（个人理解）"><a href="#知识优先级（个人理解）" class="headerlink" title="知识优先级（个人理解）"></a>知识优先级（个人理解）</h5></li></ul><p>数学（线代&gt;概率论&gt;&#x3D;离散&gt;高数）&#x3D;&#x3D;数据结构&gt;操作系统&gt;计算机网络&gt;程序设计语言(C++,JAVA,PYTHON)&gt;数据库&gt;计组&gt;&gt;思政&gt;&gt;&gt;&gt;其他</p><h5 id="人工智能单列，想要研究方向与AI相关的同学切记准备"><a href="#人工智能单列，想要研究方向与AI相关的同学切记准备" class="headerlink" title="人工智能单列，想要研究方向与AI相关的同学切记准备"></a>人工智能单列，想要研究方向与AI相关的同学切记准备</h5><ul><li><h5 id="写到简历上的东西一定一定要会，切勿报侥幸心理"><a href="#写到简历上的东西一定一定要会，切勿报侥幸心理" class="headerlink" title="写到简历上的东西一定一定要会，切勿报侥幸心理"></a>写到简历上的东西一定一定要会，切勿报侥幸心理</h5></li><li><h5 id="近期网上冲浪的时候多看一点保研经验贴，大数据建议保研交流群住在里面，大佬们真的长得又好看说话又好听"><a href="#近期网上冲浪的时候多看一点保研经验贴，大数据建议保研交流群住在里面，大佬们真的长得又好看说话又好听" class="headerlink" title="近期网上冲浪的时候多看一点保研经验贴，大数据建议保研交流群住在里面，大佬们真的长得又好看说话又好听"></a>近期网上冲浪的时候多看一点保研经验贴，大数据建议保研交流群住在里面，大佬们真的长得又好看说话又好听</h5></li><li><h5 id="多与老师、学长学姐交流，他们走过你可能要走的路，会告诉你可能需要规避的点（这里我真的非常非常感谢我的辅导员和很多给予我具体建议的学长学姐）"><a href="#多与老师、学长学姐交流，他们走过你可能要走的路，会告诉你可能需要规避的点（这里我真的非常非常感谢我的辅导员和很多给予我具体建议的学长学姐）" class="headerlink" title="多与老师、学长学姐交流，他们走过你可能要走的路，会告诉你可能需要规避的点（这里我真的非常非常感谢我的辅导员和很多给予我具体建议的学长学姐）"></a>多与老师、学长学姐交流，他们走过你可能要走的路，会告诉你可能需要规避的点（这里我真的非常非常感谢我的辅导员和很多给予我具体建议的学长学姐）</h5></li><li><h5 id="放轻松放轻松，保研流程时间很长，从现在算起到九月底我们都算是保研阶段，大家需要做到的是认真准备，不留遗憾，其他的就看老天的恩赐了。"><a href="#放轻松放轻松，保研流程时间很长，从现在算起到九月底我们都算是保研阶段，大家需要做到的是认真准备，不留遗憾，其他的就看老天的恩赐了。" class="headerlink" title="放轻松放轻松，保研流程时间很长，从现在算起到九月底我们都算是保研阶段，大家需要做到的是认真准备，不留遗憾，其他的就看老天的恩赐了。"></a>放轻松放轻松，保研流程时间很长，从现在算起到九月底我们都算是保研阶段，大家需要做到的是认真准备，不留遗憾，其他的就看老天的恩赐了。</h5></li><li><h5 id="套磁，信息战"><a href="#套磁，信息战" class="headerlink" title="套磁，信息战"></a>套磁，信息战</h5></li></ul><h3 id="针对于平衡竞赛与生活的建议"><a href="#针对于平衡竞赛与生活的建议" class="headerlink" title="针对于平衡竞赛与生活的建议"></a>针对于平衡竞赛与生活的建议</h3><h5 id="转换重点，制定短期计划，提高效率"><a href="#转换重点，制定短期计划，提高效率" class="headerlink" title="转换重点，制定短期计划，提高效率"></a>转换重点，制定短期计划，提高效率</h5>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 保研 </tag>
            
            <tag> 经验贴 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
